{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS613 Final Project: Image Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='bytes')\n",
    "    \n",
    "def load_cifar10():\n",
    "    train_data, train_labels = [] , []\n",
    "    for i in range(1,6):\n",
    "        batch = unpickle(f\"data/cifar-10-python/cifar-10-batches-py/data_batch_{i}\")\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    # Load test batch\n",
    "    test_batch = unpickle(f\"data/cifar-10-python/cifar-10-batches-py/test_batch\")\n",
    "    test_data = np.array(test_batch[b'data'])\n",
    "    test_labels = np.array(test_batch[b'labels'])\n",
    "\n",
    "    # Reshape the data to (N, 32, 32, 3)\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, labels, file_path):\n",
    "    # Combine labels and data\n",
    "    combined = np.column_stack((labels, data))\n",
    "    # Save as a CSV file\n",
    "    np.savetxt(file_path, combined, delimiter=\",\", fmt=\"%f\")\n",
    "    print(f\"Saved {file_path} successfully!\")\n",
    "    \n",
    "# Prepare data\n",
    "def normalize_images(data):\n",
    "    return data / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot = np.zeros((labels.size, num_classes))\n",
    "    one_hot[np.arange(labels.size),labels] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10 dataset...\n",
      "Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "x_train, y_train, x_test, y_test = load_cifar10()\n",
    "\n",
    "# Preprocess data\n",
    "print(\"Preprocessing data...\")\n",
    "x_train = normalize_images(x_train).reshape(x_train.shape[0], -1)\n",
    "x_test = normalize_images(x_test).reshape(x_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to CSV...\n",
      "Saved train.csv successfully!\n",
      "Saved test.csv successfully!\n",
      "CSV files created.\n"
     ]
    }
   ],
   "source": [
    "# Save csv files if needed\n",
    "print(\"Saving to CSV...\")\n",
    "save_to_csv(x_train, y_train, \"train.csv\")\n",
    "save_to_csv(x_test, y_test, \"test.csv\")\n",
    "print(\"CSV files created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    num_classes = y_true.shape[1]\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    y_true_indices = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    for t, p in zip(y_true_indices, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true_label = np.argmax(y_true, axis=1)  #Converting one-hot encoded back to label encoded\n",
    "    return np.sum(y_true_label == y_pred) / len(y_true_label)\n",
    "\n",
    "\n",
    "def precision(cm):\n",
    "    \n",
    "    precisions = []\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        if tp + fp > 0:\n",
    "            precisions.append(tp / (tp + fp))\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "def recall(cm):\n",
    "    \n",
    "    recalls = []\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        if tp + fn > 0:\n",
    "            recalls.append(tp / (tp + fn))\n",
    "        else:\n",
    "            recalls.append(0)\n",
    "    return np.mean(recalls)\n",
    "\n",
    "def f1_score(cm):\n",
    "    \n",
    "    precisions = precision(cm)\n",
    "    recalls = recall(cm)\n",
    "    \n",
    "    if (precisions + recalls) > 0:\n",
    "        return 2 * (precisions * recalls) / (precisions + recalls) \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        #Weight Initalization\n",
    "        self.num_features = None\n",
    "        self.num_classes = None\n",
    "        self.weights = None\n",
    "\n",
    "        # Initialize lists for tracking losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    # Logistic Regression Functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predictProb(self, X):\n",
    "        y_pred = self.softmax(np.dot(X, self.weights))\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, y_pred):\n",
    "        return np.argmax(self.predictProb(y_pred), axis=1)\n",
    "\n",
    "    def log_loss(self, y, y_pred, epsilon=1e-15):\n",
    "        return -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
    "    \n",
    "    def categorical_crossentropy(self, y, y_pred, epsilon=1e-15):\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon) \n",
    "        return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  \n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def gradient_descent(self, X, y, weights, lr):\n",
    "        y_pred = self.softmax(np.dot(X, weights))\n",
    "        error = y_pred - y\n",
    "        gradient = np.dot(X.T, error) / len(y)\n",
    "        return weights - lr * gradient\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test=None, y_test=None):\n",
    "        self.num_features = X_train.shape[1]\n",
    "        self.num_classes = y_train.shape[1]\n",
    "        self.weights = np.random.randn(self.num_features, self.num_classes)\n",
    "    \n",
    "        for epoch in range(self.epochs):\n",
    "            # Update Weights\n",
    "            self.weights = self.gradient_descent(X_train, y_train, self.weights, self.learning_rate)\n",
    "\n",
    "            train_pred = self.softmax(np.dot(X_train, self.weights))\n",
    "            train_loss = self.categorical_crossentropy(y_train, train_pred)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_pred = self.softmax(np.dot(X_test, self.weights))\n",
    "                test_loss = self.categorical_crossentropy(y_test, test_pred)\n",
    "                self.test_losses.append(test_loss)\n",
    "            else:\n",
    "                test_loss = None\n",
    "\n",
    "            # if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = one_hot_encode(y_train, num_classes)\n",
    "y_test = one_hot_encode(y_test, num_classes)\n",
    "\n",
    "# Add biases to X\n",
    "X_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "X_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Train Loss = 20.9015, Test Loss = 20.9528\n",
      "Epoch 2/1000: Train Loss = 18.2786, Test Loss = 18.3053\n",
      "Epoch 3/1000: Train Loss = 17.0166, Test Loss = 17.0375\n",
      "Epoch 4/1000: Train Loss = 16.4364, Test Loss = 16.4140\n",
      "Epoch 5/1000: Train Loss = 16.1070, Test Loss = 16.0786\n",
      "Epoch 6/1000: Train Loss = 15.9089, Test Loss = 15.8649\n",
      "Epoch 7/1000: Train Loss = 15.7491, Test Loss = 15.7009\n",
      "Epoch 8/1000: Train Loss = 15.6075, Test Loss = 15.5561\n",
      "Epoch 9/1000: Train Loss = 15.4649, Test Loss = 15.4124\n",
      "Epoch 10/1000: Train Loss = 15.3284, Test Loss = 15.2742\n",
      "Epoch 11/1000: Train Loss = 15.1931, Test Loss = 15.1377\n",
      "Epoch 12/1000: Train Loss = 15.0621, Test Loss = 15.0051\n",
      "Epoch 13/1000: Train Loss = 14.9334, Test Loss = 14.8749\n",
      "Epoch 14/1000: Train Loss = 14.8083, Test Loss = 14.7480\n",
      "Epoch 15/1000: Train Loss = 14.6859, Test Loss = 14.6237\n",
      "Epoch 16/1000: Train Loss = 14.5666, Test Loss = 14.5023\n",
      "Epoch 17/1000: Train Loss = 14.4501, Test Loss = 14.3837\n",
      "Epoch 18/1000: Train Loss = 14.3366, Test Loss = 14.2679\n",
      "Epoch 19/1000: Train Loss = 14.2262, Test Loss = 14.1551\n",
      "Epoch 20/1000: Train Loss = 14.1186, Test Loss = 14.0451\n",
      "Epoch 21/1000: Train Loss = 14.0138, Test Loss = 13.9377\n",
      "Epoch 22/1000: Train Loss = 13.9118, Test Loss = 13.8331\n",
      "Epoch 23/1000: Train Loss = 13.8124, Test Loss = 13.7311\n",
      "Epoch 24/1000: Train Loss = 13.7157, Test Loss = 13.6318\n",
      "Epoch 25/1000: Train Loss = 13.6215, Test Loss = 13.5352\n",
      "Epoch 26/1000: Train Loss = 13.5297, Test Loss = 13.4410\n",
      "Epoch 27/1000: Train Loss = 13.4404, Test Loss = 13.3494\n",
      "Epoch 28/1000: Train Loss = 13.3534, Test Loss = 13.2601\n",
      "Epoch 29/1000: Train Loss = 13.2687, Test Loss = 13.1728\n",
      "Epoch 30/1000: Train Loss = 13.1861, Test Loss = 13.0877\n",
      "Epoch 31/1000: Train Loss = 13.1057, Test Loss = 13.0047\n",
      "Epoch 32/1000: Train Loss = 13.0273, Test Loss = 12.9239\n",
      "Epoch 33/1000: Train Loss = 12.9508, Test Loss = 12.8450\n",
      "Epoch 34/1000: Train Loss = 12.8762, Test Loss = 12.7681\n",
      "Epoch 35/1000: Train Loss = 12.8036, Test Loss = 12.6930\n",
      "Epoch 36/1000: Train Loss = 12.7326, Test Loss = 12.6197\n",
      "Epoch 37/1000: Train Loss = 12.6634, Test Loss = 12.5482\n",
      "Epoch 38/1000: Train Loss = 12.5959, Test Loss = 12.4784\n",
      "Epoch 39/1000: Train Loss = 12.5300, Test Loss = 12.4104\n",
      "Epoch 40/1000: Train Loss = 12.4657, Test Loss = 12.3441\n",
      "Epoch 41/1000: Train Loss = 12.4029, Test Loss = 12.2793\n",
      "Epoch 42/1000: Train Loss = 12.3416, Test Loss = 12.2160\n",
      "Epoch 43/1000: Train Loss = 12.2817, Test Loss = 12.1542\n",
      "Epoch 44/1000: Train Loss = 12.2231, Test Loss = 12.0938\n",
      "Epoch 45/1000: Train Loss = 12.1659, Test Loss = 12.0349\n",
      "Epoch 46/1000: Train Loss = 12.1099, Test Loss = 11.9772\n",
      "Epoch 47/1000: Train Loss = 12.0552, Test Loss = 11.9208\n",
      "Epoch 48/1000: Train Loss = 12.0016, Test Loss = 11.8657\n",
      "Epoch 49/1000: Train Loss = 11.9492, Test Loss = 11.8119\n",
      "Epoch 50/1000: Train Loss = 11.8979, Test Loss = 11.7592\n",
      "Epoch 51/1000: Train Loss = 11.8477, Test Loss = 11.7076\n",
      "Epoch 52/1000: Train Loss = 11.7985, Test Loss = 11.6571\n",
      "Epoch 53/1000: Train Loss = 11.7503, Test Loss = 11.6076\n",
      "Epoch 54/1000: Train Loss = 11.7031, Test Loss = 11.5591\n",
      "Epoch 55/1000: Train Loss = 11.6568, Test Loss = 11.5115\n",
      "Epoch 56/1000: Train Loss = 11.6114, Test Loss = 11.4649\n",
      "Epoch 57/1000: Train Loss = 11.5669, Test Loss = 11.4193\n",
      "Epoch 58/1000: Train Loss = 11.5231, Test Loss = 11.3744\n",
      "Epoch 59/1000: Train Loss = 11.4802, Test Loss = 11.3304\n",
      "Epoch 60/1000: Train Loss = 11.4380, Test Loss = 11.2873\n",
      "Epoch 61/1000: Train Loss = 11.3967, Test Loss = 11.2448\n",
      "Epoch 62/1000: Train Loss = 11.3560, Test Loss = 11.2032\n",
      "Epoch 63/1000: Train Loss = 11.3161, Test Loss = 11.1623\n",
      "Epoch 64/1000: Train Loss = 11.2768, Test Loss = 11.1222\n",
      "Epoch 65/1000: Train Loss = 11.2382, Test Loss = 11.0827\n",
      "Epoch 66/1000: Train Loss = 11.2003, Test Loss = 11.0439\n",
      "Epoch 67/1000: Train Loss = 11.1629, Test Loss = 11.0056\n",
      "Epoch 68/1000: Train Loss = 11.1261, Test Loss = 10.9681\n",
      "Epoch 69/1000: Train Loss = 11.0899, Test Loss = 10.9311\n",
      "Epoch 70/1000: Train Loss = 11.0542, Test Loss = 10.8947\n",
      "Epoch 71/1000: Train Loss = 11.0191, Test Loss = 10.8588\n",
      "Epoch 72/1000: Train Loss = 10.9846, Test Loss = 10.8236\n",
      "Epoch 73/1000: Train Loss = 10.9505, Test Loss = 10.7888\n",
      "Epoch 74/1000: Train Loss = 10.9169, Test Loss = 10.7546\n",
      "Epoch 75/1000: Train Loss = 10.8839, Test Loss = 10.7208\n",
      "Epoch 76/1000: Train Loss = 10.8513, Test Loss = 10.6876\n",
      "Epoch 77/1000: Train Loss = 10.8191, Test Loss = 10.6547\n",
      "Epoch 78/1000: Train Loss = 10.7874, Test Loss = 10.6225\n",
      "Epoch 79/1000: Train Loss = 10.7561, Test Loss = 10.5905\n",
      "Epoch 80/1000: Train Loss = 10.7253, Test Loss = 10.5593\n",
      "Epoch 81/1000: Train Loss = 10.6948, Test Loss = 10.5281\n",
      "Epoch 82/1000: Train Loss = 10.6647, Test Loss = 10.4977\n",
      "Epoch 83/1000: Train Loss = 10.6350, Test Loss = 10.4673\n",
      "Epoch 84/1000: Train Loss = 10.6057, Test Loss = 10.4378\n",
      "Epoch 85/1000: Train Loss = 10.5768, Test Loss = 10.4082\n",
      "Epoch 86/1000: Train Loss = 10.5482, Test Loss = 10.3795\n",
      "Epoch 87/1000: Train Loss = 10.5200, Test Loss = 10.3505\n",
      "Epoch 88/1000: Train Loss = 10.4921, Test Loss = 10.3226\n",
      "Epoch 89/1000: Train Loss = 10.4646, Test Loss = 10.2941\n",
      "Epoch 90/1000: Train Loss = 10.4374, Test Loss = 10.2670\n",
      "Epoch 91/1000: Train Loss = 10.4105, Test Loss = 10.2391\n",
      "Epoch 92/1000: Train Loss = 10.3840, Test Loss = 10.2128\n",
      "Epoch 93/1000: Train Loss = 10.3577, Test Loss = 10.1854\n",
      "Epoch 94/1000: Train Loss = 10.3318, Test Loss = 10.1600\n",
      "Epoch 95/1000: Train Loss = 10.3061, Test Loss = 10.1330\n",
      "Epoch 96/1000: Train Loss = 10.2808, Test Loss = 10.1086\n",
      "Epoch 97/1000: Train Loss = 10.2556, Test Loss = 10.0818\n",
      "Epoch 98/1000: Train Loss = 10.2309, Test Loss = 10.0584\n",
      "Epoch 99/1000: Train Loss = 10.2064, Test Loss = 10.0317\n",
      "Epoch 100/1000: Train Loss = 10.1823, Test Loss = 10.0097\n",
      "Epoch 101/1000: Train Loss = 10.1583, Test Loss = 9.9829\n",
      "Epoch 102/1000: Train Loss = 10.1349, Test Loss = 9.9623\n",
      "Epoch 103/1000: Train Loss = 10.1115, Test Loss = 9.9352\n",
      "Epoch 104/1000: Train Loss = 10.0888, Test Loss = 9.9163\n",
      "Epoch 105/1000: Train Loss = 10.0660, Test Loss = 9.8888\n",
      "Epoch 106/1000: Train Loss = 10.0442, Test Loss = 9.8722\n",
      "Epoch 107/1000: Train Loss = 10.0221, Test Loss = 9.8439\n",
      "Epoch 108/1000: Train Loss = 10.0015, Test Loss = 9.8303\n",
      "Epoch 109/1000: Train Loss = 9.9803, Test Loss = 9.8009\n",
      "Epoch 110/1000: Train Loss = 9.9615, Test Loss = 9.7914\n",
      "Epoch 111/1000: Train Loss = 9.9410, Test Loss = 9.7603\n",
      "Epoch 112/1000: Train Loss = 9.9251, Test Loss = 9.7565\n",
      "Epoch 113/1000: Train Loss = 9.9051, Test Loss = 9.7229\n",
      "Epoch 114/1000: Train Loss = 9.8935, Test Loss = 9.7268\n",
      "Epoch 115/1000: Train Loss = 9.8726, Test Loss = 9.6890\n",
      "Epoch 116/1000: Train Loss = 9.8671, Test Loss = 9.7025\n",
      "Epoch 117/1000: Train Loss = 9.8429, Test Loss = 9.6578\n",
      "Epoch 118/1000: Train Loss = 9.8446, Test Loss = 9.6820\n",
      "Epoch 119/1000: Train Loss = 9.8138, Test Loss = 9.6276\n",
      "Epoch 120/1000: Train Loss = 9.8223, Test Loss = 9.6615\n",
      "Epoch 121/1000: Train Loss = 9.7829, Test Loss = 9.5960\n",
      "Epoch 122/1000: Train Loss = 9.7965, Test Loss = 9.6370\n",
      "Epoch 123/1000: Train Loss = 9.7496, Test Loss = 9.5623\n",
      "Epoch 124/1000: Train Loss = 9.7661, Test Loss = 9.6075\n",
      "Epoch 125/1000: Train Loss = 9.7145, Test Loss = 9.5270\n",
      "Epoch 126/1000: Train Loss = 9.7325, Test Loss = 9.5745\n",
      "Epoch 127/1000: Train Loss = 9.6786, Test Loss = 9.4912\n",
      "Epoch 128/1000: Train Loss = 9.6974, Test Loss = 9.5398\n",
      "Epoch 129/1000: Train Loss = 9.6428, Test Loss = 9.4553\n",
      "Epoch 130/1000: Train Loss = 9.6620, Test Loss = 9.5047\n",
      "Epoch 131/1000: Train Loss = 9.6072, Test Loss = 9.4199\n",
      "Epoch 132/1000: Train Loss = 9.6267, Test Loss = 9.4698\n",
      "Epoch 133/1000: Train Loss = 9.5720, Test Loss = 9.3849\n",
      "Epoch 134/1000: Train Loss = 9.5919, Test Loss = 9.4354\n",
      "Epoch 135/1000: Train Loss = 9.5374, Test Loss = 9.3506\n",
      "Epoch 136/1000: Train Loss = 9.5576, Test Loss = 9.4016\n",
      "Epoch 137/1000: Train Loss = 9.5034, Test Loss = 9.3169\n",
      "Epoch 138/1000: Train Loss = 9.5239, Test Loss = 9.3684\n",
      "Epoch 139/1000: Train Loss = 9.4699, Test Loss = 9.2838\n",
      "Epoch 140/1000: Train Loss = 9.4907, Test Loss = 9.3358\n",
      "Epoch 141/1000: Train Loss = 9.4370, Test Loss = 9.2513\n",
      "Epoch 142/1000: Train Loss = 9.4582, Test Loss = 9.3038\n",
      "Epoch 143/1000: Train Loss = 9.4046, Test Loss = 9.2194\n",
      "Epoch 144/1000: Train Loss = 9.4262, Test Loss = 9.2725\n",
      "Epoch 145/1000: Train Loss = 9.3727, Test Loss = 9.1880\n",
      "Epoch 146/1000: Train Loss = 9.3947, Test Loss = 9.2417\n",
      "Epoch 147/1000: Train Loss = 9.3414, Test Loss = 9.1573\n",
      "Epoch 148/1000: Train Loss = 9.3638, Test Loss = 9.2115\n",
      "Epoch 149/1000: Train Loss = 9.3106, Test Loss = 9.1270\n",
      "Epoch 150/1000: Train Loss = 9.3334, Test Loss = 9.1819\n",
      "Epoch 151/1000: Train Loss = 9.2803, Test Loss = 9.0973\n",
      "Epoch 152/1000: Train Loss = 9.3035, Test Loss = 9.1528\n",
      "Epoch 153/1000: Train Loss = 9.2504, Test Loss = 9.0681\n",
      "Epoch 154/1000: Train Loss = 9.2742, Test Loss = 9.1242\n",
      "Epoch 155/1000: Train Loss = 9.2211, Test Loss = 9.0394\n",
      "Epoch 156/1000: Train Loss = 9.2453, Test Loss = 9.0962\n",
      "Epoch 157/1000: Train Loss = 9.1922, Test Loss = 9.0112\n",
      "Epoch 158/1000: Train Loss = 9.2169, Test Loss = 9.0687\n",
      "Epoch 159/1000: Train Loss = 9.1638, Test Loss = 8.9834\n",
      "Epoch 160/1000: Train Loss = 9.1889, Test Loss = 9.0416\n",
      "Epoch 161/1000: Train Loss = 9.1358, Test Loss = 8.9561\n",
      "Epoch 162/1000: Train Loss = 9.1614, Test Loss = 9.0150\n",
      "Epoch 163/1000: Train Loss = 9.1082, Test Loss = 8.9292\n",
      "Epoch 164/1000: Train Loss = 9.1344, Test Loss = 8.9889\n",
      "Epoch 165/1000: Train Loss = 9.0811, Test Loss = 8.9028\n",
      "Epoch 166/1000: Train Loss = 9.1077, Test Loss = 8.9632\n",
      "Epoch 167/1000: Train Loss = 9.0544, Test Loss = 8.8767\n",
      "Epoch 168/1000: Train Loss = 9.0815, Test Loss = 8.9379\n",
      "Epoch 169/1000: Train Loss = 9.0281, Test Loss = 8.8510\n",
      "Epoch 170/1000: Train Loss = 9.0558, Test Loss = 8.9130\n",
      "Epoch 171/1000: Train Loss = 9.0021, Test Loss = 8.8257\n",
      "Epoch 172/1000: Train Loss = 9.0304, Test Loss = 8.8886\n",
      "Epoch 173/1000: Train Loss = 8.9766, Test Loss = 8.8008\n",
      "Epoch 174/1000: Train Loss = 9.0054, Test Loss = 8.8645\n",
      "Epoch 175/1000: Train Loss = 8.9514, Test Loss = 8.7763\n",
      "Epoch 176/1000: Train Loss = 8.9808, Test Loss = 8.8408\n",
      "Epoch 177/1000: Train Loss = 8.9266, Test Loss = 8.7522\n",
      "Epoch 178/1000: Train Loss = 8.9565, Test Loss = 8.8175\n",
      "Epoch 179/1000: Train Loss = 8.9022, Test Loss = 8.7284\n",
      "Epoch 180/1000: Train Loss = 8.9327, Test Loss = 8.7945\n",
      "Epoch 181/1000: Train Loss = 8.8781, Test Loss = 8.7050\n",
      "Epoch 182/1000: Train Loss = 8.9092, Test Loss = 8.7719\n",
      "Epoch 183/1000: Train Loss = 8.8543, Test Loss = 8.6819\n",
      "Epoch 184/1000: Train Loss = 8.8860, Test Loss = 8.7497\n",
      "Epoch 185/1000: Train Loss = 8.8308, Test Loss = 8.6591\n",
      "Epoch 186/1000: Train Loss = 8.8631, Test Loss = 8.7277\n",
      "Epoch 187/1000: Train Loss = 8.8077, Test Loss = 8.6367\n",
      "Epoch 188/1000: Train Loss = 8.8406, Test Loss = 8.7062\n",
      "Epoch 189/1000: Train Loss = 8.7849, Test Loss = 8.6146\n",
      "Epoch 190/1000: Train Loss = 8.8184, Test Loss = 8.6849\n",
      "Epoch 191/1000: Train Loss = 8.7624, Test Loss = 8.5927\n",
      "Epoch 192/1000: Train Loss = 8.7965, Test Loss = 8.6639\n",
      "Epoch 193/1000: Train Loss = 8.7402, Test Loss = 8.5712\n",
      "Epoch 194/1000: Train Loss = 8.7749, Test Loss = 8.6432\n",
      "Epoch 195/1000: Train Loss = 8.7183, Test Loss = 8.5500\n",
      "Epoch 196/1000: Train Loss = 8.7536, Test Loss = 8.6228\n",
      "Epoch 197/1000: Train Loss = 8.6967, Test Loss = 8.5291\n",
      "Epoch 198/1000: Train Loss = 8.7326, Test Loss = 8.6027\n",
      "Epoch 199/1000: Train Loss = 8.6753, Test Loss = 8.5084\n",
      "Epoch 200/1000: Train Loss = 8.7118, Test Loss = 8.5829\n",
      "Epoch 201/1000: Train Loss = 8.6543, Test Loss = 8.4880\n",
      "Epoch 202/1000: Train Loss = 8.6913, Test Loss = 8.5633\n",
      "Epoch 203/1000: Train Loss = 8.6335, Test Loss = 8.4679\n",
      "Epoch 204/1000: Train Loss = 8.6711, Test Loss = 8.5440\n",
      "Epoch 205/1000: Train Loss = 8.6130, Test Loss = 8.4480\n",
      "Epoch 206/1000: Train Loss = 8.6512, Test Loss = 8.5250\n",
      "Epoch 207/1000: Train Loss = 8.5927, Test Loss = 8.4284\n",
      "Epoch 208/1000: Train Loss = 8.6315, Test Loss = 8.5062\n",
      "Epoch 209/1000: Train Loss = 8.5727, Test Loss = 8.4091\n",
      "Epoch 210/1000: Train Loss = 8.6121, Test Loss = 8.4876\n",
      "Epoch 211/1000: Train Loss = 8.5529, Test Loss = 8.3899\n",
      "Epoch 212/1000: Train Loss = 8.5929, Test Loss = 8.4693\n",
      "Epoch 213/1000: Train Loss = 8.5334, Test Loss = 8.3711\n",
      "Epoch 214/1000: Train Loss = 8.5739, Test Loss = 8.4512\n",
      "Epoch 215/1000: Train Loss = 8.5141, Test Loss = 8.3525\n",
      "Epoch 216/1000: Train Loss = 8.5552, Test Loss = 8.4333\n",
      "Epoch 217/1000: Train Loss = 8.4950, Test Loss = 8.3341\n",
      "Epoch 218/1000: Train Loss = 8.5367, Test Loss = 8.4157\n",
      "Epoch 219/1000: Train Loss = 8.4762, Test Loss = 8.3159\n",
      "Epoch 220/1000: Train Loss = 8.5184, Test Loss = 8.3983\n",
      "Epoch 221/1000: Train Loss = 8.4576, Test Loss = 8.2980\n",
      "Epoch 222/1000: Train Loss = 8.5004, Test Loss = 8.3811\n",
      "Epoch 223/1000: Train Loss = 8.4393, Test Loss = 8.2802\n",
      "Epoch 224/1000: Train Loss = 8.4826, Test Loss = 8.3641\n",
      "Epoch 225/1000: Train Loss = 8.4212, Test Loss = 8.2627\n",
      "Epoch 226/1000: Train Loss = 8.4650, Test Loss = 8.3474\n",
      "Epoch 227/1000: Train Loss = 8.4032, Test Loss = 8.2454\n",
      "Epoch 228/1000: Train Loss = 8.4476, Test Loss = 8.3308\n",
      "Epoch 229/1000: Train Loss = 8.3856, Test Loss = 8.2284\n",
      "Epoch 230/1000: Train Loss = 8.4304, Test Loss = 8.3145\n",
      "Epoch 231/1000: Train Loss = 8.3681, Test Loss = 8.2115\n",
      "Epoch 232/1000: Train Loss = 8.4135, Test Loss = 8.2983\n",
      "Epoch 233/1000: Train Loss = 8.3508, Test Loss = 8.1948\n",
      "Epoch 234/1000: Train Loss = 8.3968, Test Loss = 8.2824\n",
      "Epoch 235/1000: Train Loss = 8.3338, Test Loss = 8.1784\n",
      "Epoch 236/1000: Train Loss = 8.3802, Test Loss = 8.2667\n",
      "Epoch 237/1000: Train Loss = 8.3169, Test Loss = 8.1621\n",
      "Epoch 238/1000: Train Loss = 8.3639, Test Loss = 8.2511\n",
      "Epoch 239/1000: Train Loss = 8.3002, Test Loss = 8.1461\n",
      "Epoch 240/1000: Train Loss = 8.3478, Test Loss = 8.2358\n",
      "Epoch 241/1000: Train Loss = 8.2838, Test Loss = 8.1302\n",
      "Epoch 242/1000: Train Loss = 8.3319, Test Loss = 8.2207\n",
      "Epoch 243/1000: Train Loss = 8.2676, Test Loss = 8.1146\n",
      "Epoch 244/1000: Train Loss = 8.3162, Test Loss = 8.2057\n",
      "Epoch 245/1000: Train Loss = 8.2515, Test Loss = 8.0991\n",
      "Epoch 246/1000: Train Loss = 8.3007, Test Loss = 8.1910\n",
      "Epoch 247/1000: Train Loss = 8.2357, Test Loss = 8.0838\n",
      "Epoch 248/1000: Train Loss = 8.2853, Test Loss = 8.1765\n",
      "Epoch 249/1000: Train Loss = 8.2200, Test Loss = 8.0687\n",
      "Epoch 250/1000: Train Loss = 8.2702, Test Loss = 8.1621\n",
      "Epoch 251/1000: Train Loss = 8.2046, Test Loss = 8.0538\n",
      "Epoch 252/1000: Train Loss = 8.2553, Test Loss = 8.1480\n",
      "Epoch 253/1000: Train Loss = 8.1893, Test Loss = 8.0391\n",
      "Epoch 254/1000: Train Loss = 8.2406, Test Loss = 8.1340\n",
      "Epoch 255/1000: Train Loss = 8.1743, Test Loss = 8.0246\n",
      "Epoch 256/1000: Train Loss = 8.2260, Test Loss = 8.1203\n",
      "Epoch 257/1000: Train Loss = 8.1594, Test Loss = 8.0103\n",
      "Epoch 258/1000: Train Loss = 8.2117, Test Loss = 8.1067\n",
      "Epoch 259/1000: Train Loss = 8.1448, Test Loss = 7.9962\n",
      "Epoch 260/1000: Train Loss = 8.1976, Test Loss = 8.0933\n",
      "Epoch 261/1000: Train Loss = 8.1303, Test Loss = 7.9823\n",
      "Epoch 262/1000: Train Loss = 8.1836, Test Loss = 8.0801\n",
      "Epoch 263/1000: Train Loss = 8.1160, Test Loss = 7.9685\n",
      "Epoch 264/1000: Train Loss = 8.1698, Test Loss = 8.0671\n",
      "Epoch 265/1000: Train Loss = 8.1019, Test Loss = 7.9549\n",
      "Epoch 266/1000: Train Loss = 8.1563, Test Loss = 8.0543\n",
      "Epoch 267/1000: Train Loss = 8.0880, Test Loss = 7.9415\n",
      "Epoch 268/1000: Train Loss = 8.1429, Test Loss = 8.0416\n",
      "Epoch 269/1000: Train Loss = 8.0743, Test Loss = 7.9283\n",
      "Epoch 270/1000: Train Loss = 8.1297, Test Loss = 8.0291\n",
      "Epoch 271/1000: Train Loss = 8.0607, Test Loss = 7.9153\n",
      "Epoch 272/1000: Train Loss = 8.1166, Test Loss = 8.0168\n",
      "Epoch 273/1000: Train Loss = 8.0474, Test Loss = 7.9024\n",
      "Epoch 274/1000: Train Loss = 8.1037, Test Loss = 8.0047\n",
      "Epoch 275/1000: Train Loss = 8.0342, Test Loss = 7.8898\n",
      "Epoch 276/1000: Train Loss = 8.0910, Test Loss = 7.9927\n",
      "Epoch 277/1000: Train Loss = 8.0212, Test Loss = 7.8772\n",
      "Epoch 278/1000: Train Loss = 8.0785, Test Loss = 7.9808\n",
      "Epoch 279/1000: Train Loss = 8.0083, Test Loss = 7.8649\n",
      "Epoch 280/1000: Train Loss = 8.0661, Test Loss = 7.9691\n",
      "Epoch 281/1000: Train Loss = 7.9956, Test Loss = 7.8527\n",
      "Epoch 282/1000: Train Loss = 8.0539, Test Loss = 7.9576\n",
      "Epoch 283/1000: Train Loss = 7.9830, Test Loss = 7.8406\n",
      "Epoch 284/1000: Train Loss = 8.0418, Test Loss = 7.9462\n",
      "Epoch 285/1000: Train Loss = 7.9706, Test Loss = 7.8287\n",
      "Epoch 286/1000: Train Loss = 8.0298, Test Loss = 7.9348\n",
      "Epoch 287/1000: Train Loss = 7.9584, Test Loss = 7.8170\n",
      "Epoch 288/1000: Train Loss = 8.0179, Test Loss = 7.9236\n",
      "Epoch 289/1000: Train Loss = 7.9462, Test Loss = 7.8053\n",
      "Epoch 290/1000: Train Loss = 8.0062, Test Loss = 7.9125\n",
      "Epoch 291/1000: Train Loss = 7.9342, Test Loss = 7.7938\n",
      "Epoch 292/1000: Train Loss = 7.9946, Test Loss = 7.9015\n",
      "Epoch 293/1000: Train Loss = 7.9224, Test Loss = 7.7824\n",
      "Epoch 294/1000: Train Loss = 7.9830, Test Loss = 7.8906\n",
      "Epoch 295/1000: Train Loss = 7.9106, Test Loss = 7.7712\n",
      "Epoch 296/1000: Train Loss = 7.9716, Test Loss = 7.8798\n",
      "Epoch 297/1000: Train Loss = 7.8990, Test Loss = 7.7600\n",
      "Epoch 298/1000: Train Loss = 7.9603, Test Loss = 7.8691\n",
      "Epoch 299/1000: Train Loss = 7.8874, Test Loss = 7.7490\n",
      "Epoch 300/1000: Train Loss = 7.9490, Test Loss = 7.8584\n",
      "Epoch 301/1000: Train Loss = 7.8760, Test Loss = 7.7380\n",
      "Epoch 302/1000: Train Loss = 7.9378, Test Loss = 7.8478\n",
      "Epoch 303/1000: Train Loss = 7.8647, Test Loss = 7.7272\n",
      "Epoch 304/1000: Train Loss = 7.9267, Test Loss = 7.8373\n",
      "Epoch 305/1000: Train Loss = 7.8535, Test Loss = 7.7165\n",
      "Epoch 306/1000: Train Loss = 7.9157, Test Loss = 7.8269\n",
      "Epoch 307/1000: Train Loss = 7.8424, Test Loss = 7.7059\n",
      "Epoch 308/1000: Train Loss = 7.9048, Test Loss = 7.8165\n",
      "Epoch 309/1000: Train Loss = 7.8314, Test Loss = 7.6954\n",
      "Epoch 310/1000: Train Loss = 7.8939, Test Loss = 7.8061\n",
      "Epoch 311/1000: Train Loss = 7.8205, Test Loss = 7.6849\n",
      "Epoch 312/1000: Train Loss = 7.8831, Test Loss = 7.7959\n",
      "Epoch 313/1000: Train Loss = 7.8098, Test Loss = 7.6746\n",
      "Epoch 314/1000: Train Loss = 7.8724, Test Loss = 7.7857\n",
      "Epoch 315/1000: Train Loss = 7.7991, Test Loss = 7.6644\n",
      "Epoch 316/1000: Train Loss = 7.8617, Test Loss = 7.7756\n",
      "Epoch 317/1000: Train Loss = 7.7885, Test Loss = 7.6543\n",
      "Epoch 318/1000: Train Loss = 7.8512, Test Loss = 7.7655\n",
      "Epoch 319/1000: Train Loss = 7.7780, Test Loss = 7.6442\n",
      "Epoch 320/1000: Train Loss = 7.8407, Test Loss = 7.7555\n",
      "Epoch 321/1000: Train Loss = 7.7677, Test Loss = 7.6343\n",
      "Epoch 322/1000: Train Loss = 7.8303, Test Loss = 7.7456\n",
      "Epoch 323/1000: Train Loss = 7.7574, Test Loss = 7.6245\n",
      "Epoch 324/1000: Train Loss = 7.8199, Test Loss = 7.7357\n",
      "Epoch 325/1000: Train Loss = 7.7473, Test Loss = 7.6148\n",
      "Epoch 326/1000: Train Loss = 7.8097, Test Loss = 7.7259\n",
      "Epoch 327/1000: Train Loss = 7.7372, Test Loss = 7.6053\n",
      "Epoch 328/1000: Train Loss = 7.7995, Test Loss = 7.7162\n",
      "Epoch 329/1000: Train Loss = 7.7273, Test Loss = 7.5958\n",
      "Epoch 330/1000: Train Loss = 7.7894, Test Loss = 7.7065\n",
      "Epoch 331/1000: Train Loss = 7.7174, Test Loss = 7.5864\n",
      "Epoch 332/1000: Train Loss = 7.7793, Test Loss = 7.6970\n",
      "Epoch 333/1000: Train Loss = 7.7077, Test Loss = 7.5771\n",
      "Epoch 334/1000: Train Loss = 7.7694, Test Loss = 7.6874\n",
      "Epoch 335/1000: Train Loss = 7.6981, Test Loss = 7.5680\n",
      "Epoch 336/1000: Train Loss = 7.7595, Test Loss = 7.6780\n",
      "Epoch 337/1000: Train Loss = 7.6886, Test Loss = 7.5589\n",
      "Epoch 338/1000: Train Loss = 7.7497, Test Loss = 7.6686\n",
      "Epoch 339/1000: Train Loss = 7.6792, Test Loss = 7.5500\n",
      "Epoch 340/1000: Train Loss = 7.7399, Test Loss = 7.6593\n",
      "Epoch 341/1000: Train Loss = 7.6699, Test Loss = 7.5411\n",
      "Epoch 342/1000: Train Loss = 7.7303, Test Loss = 7.6501\n",
      "Epoch 343/1000: Train Loss = 7.6608, Test Loss = 7.5324\n",
      "Epoch 344/1000: Train Loss = 7.7207, Test Loss = 7.6409\n",
      "Epoch 345/1000: Train Loss = 7.6517, Test Loss = 7.5238\n",
      "Epoch 346/1000: Train Loss = 7.7112, Test Loss = 7.6318\n",
      "Epoch 347/1000: Train Loss = 7.6427, Test Loss = 7.5153\n",
      "Epoch 348/1000: Train Loss = 7.7018, Test Loss = 7.6228\n",
      "Epoch 349/1000: Train Loss = 7.6339, Test Loss = 7.5069\n",
      "Epoch 350/1000: Train Loss = 7.6924, Test Loss = 7.6138\n",
      "Epoch 351/1000: Train Loss = 7.6252, Test Loss = 7.4986\n",
      "Epoch 352/1000: Train Loss = 7.6832, Test Loss = 7.6050\n",
      "Epoch 353/1000: Train Loss = 7.6166, Test Loss = 7.4904\n",
      "Epoch 354/1000: Train Loss = 7.6740, Test Loss = 7.5962\n",
      "Epoch 355/1000: Train Loss = 7.6081, Test Loss = 7.4824\n",
      "Epoch 356/1000: Train Loss = 7.6649, Test Loss = 7.5875\n",
      "Epoch 357/1000: Train Loss = 7.5997, Test Loss = 7.4744\n",
      "Epoch 358/1000: Train Loss = 7.6558, Test Loss = 7.5788\n",
      "Epoch 359/1000: Train Loss = 7.5914, Test Loss = 7.4665\n",
      "Epoch 360/1000: Train Loss = 7.6469, Test Loss = 7.5703\n",
      "Epoch 361/1000: Train Loss = 7.5832, Test Loss = 7.4588\n",
      "Epoch 362/1000: Train Loss = 7.6380, Test Loss = 7.5618\n",
      "Epoch 363/1000: Train Loss = 7.5750, Test Loss = 7.4511\n",
      "Epoch 364/1000: Train Loss = 7.6292, Test Loss = 7.5534\n",
      "Epoch 365/1000: Train Loss = 7.5670, Test Loss = 7.4435\n",
      "Epoch 366/1000: Train Loss = 7.6205, Test Loss = 7.5451\n",
      "Epoch 367/1000: Train Loss = 7.5591, Test Loss = 7.4361\n",
      "Epoch 368/1000: Train Loss = 7.6119, Test Loss = 7.5368\n",
      "Epoch 369/1000: Train Loss = 7.5513, Test Loss = 7.4287\n",
      "Epoch 370/1000: Train Loss = 7.6034, Test Loss = 7.5287\n",
      "Epoch 371/1000: Train Loss = 7.5435, Test Loss = 7.4213\n",
      "Epoch 372/1000: Train Loss = 7.5949, Test Loss = 7.5206\n",
      "Epoch 373/1000: Train Loss = 7.5358, Test Loss = 7.4141\n",
      "Epoch 374/1000: Train Loss = 7.5865, Test Loss = 7.5126\n",
      "Epoch 375/1000: Train Loss = 7.5282, Test Loss = 7.4069\n",
      "Epoch 376/1000: Train Loss = 7.5782, Test Loss = 7.5046\n",
      "Epoch 377/1000: Train Loss = 7.5207, Test Loss = 7.3998\n",
      "Epoch 378/1000: Train Loss = 7.5700, Test Loss = 7.4968\n",
      "Epoch 379/1000: Train Loss = 7.5132, Test Loss = 7.3927\n",
      "Epoch 380/1000: Train Loss = 7.5618, Test Loss = 7.4890\n",
      "Epoch 381/1000: Train Loss = 7.5057, Test Loss = 7.3857\n",
      "Epoch 382/1000: Train Loss = 7.5537, Test Loss = 7.4812\n",
      "Epoch 383/1000: Train Loss = 7.4983, Test Loss = 7.3787\n",
      "Epoch 384/1000: Train Loss = 7.5456, Test Loss = 7.4736\n",
      "Epoch 385/1000: Train Loss = 7.4909, Test Loss = 7.3717\n",
      "Epoch 386/1000: Train Loss = 7.5376, Test Loss = 7.4660\n",
      "Epoch 387/1000: Train Loss = 7.4836, Test Loss = 7.3648\n",
      "Epoch 388/1000: Train Loss = 7.5297, Test Loss = 7.4584\n",
      "Epoch 389/1000: Train Loss = 7.4763, Test Loss = 7.3579\n",
      "Epoch 390/1000: Train Loss = 7.5218, Test Loss = 7.4510\n",
      "Epoch 391/1000: Train Loss = 7.4690, Test Loss = 7.3510\n",
      "Epoch 392/1000: Train Loss = 7.5140, Test Loss = 7.4435\n",
      "Epoch 393/1000: Train Loss = 7.4617, Test Loss = 7.3442\n",
      "Epoch 394/1000: Train Loss = 7.5062, Test Loss = 7.4362\n",
      "Epoch 395/1000: Train Loss = 7.4545, Test Loss = 7.3374\n",
      "Epoch 396/1000: Train Loss = 7.4985, Test Loss = 7.4288\n",
      "Epoch 397/1000: Train Loss = 7.4473, Test Loss = 7.3305\n",
      "Epoch 398/1000: Train Loss = 7.4908, Test Loss = 7.4215\n",
      "Epoch 399/1000: Train Loss = 7.4401, Test Loss = 7.3237\n",
      "Epoch 400/1000: Train Loss = 7.4831, Test Loss = 7.4143\n",
      "Epoch 401/1000: Train Loss = 7.4329, Test Loss = 7.3169\n",
      "Epoch 402/1000: Train Loss = 7.4755, Test Loss = 7.4071\n",
      "Epoch 403/1000: Train Loss = 7.4257, Test Loss = 7.3102\n",
      "Epoch 404/1000: Train Loss = 7.4680, Test Loss = 7.3999\n",
      "Epoch 405/1000: Train Loss = 7.4186, Test Loss = 7.3034\n",
      "Epoch 406/1000: Train Loss = 7.4604, Test Loss = 7.3928\n",
      "Epoch 407/1000: Train Loss = 7.4115, Test Loss = 7.2967\n",
      "Epoch 408/1000: Train Loss = 7.4530, Test Loss = 7.3858\n",
      "Epoch 409/1000: Train Loss = 7.4044, Test Loss = 7.2900\n",
      "Epoch 410/1000: Train Loss = 7.4455, Test Loss = 7.3787\n",
      "Epoch 411/1000: Train Loss = 7.3973, Test Loss = 7.2833\n",
      "Epoch 412/1000: Train Loss = 7.4381, Test Loss = 7.3717\n",
      "Epoch 413/1000: Train Loss = 7.3903, Test Loss = 7.2766\n",
      "Epoch 414/1000: Train Loss = 7.4307, Test Loss = 7.3648\n",
      "Epoch 415/1000: Train Loss = 7.3833, Test Loss = 7.2699\n",
      "Epoch 416/1000: Train Loss = 7.4234, Test Loss = 7.3579\n",
      "Epoch 417/1000: Train Loss = 7.3763, Test Loss = 7.2633\n",
      "Epoch 418/1000: Train Loss = 7.4161, Test Loss = 7.3510\n",
      "Epoch 419/1000: Train Loss = 7.3694, Test Loss = 7.2567\n",
      "Epoch 420/1000: Train Loss = 7.4089, Test Loss = 7.3442\n",
      "Epoch 421/1000: Train Loss = 7.3625, Test Loss = 7.2501\n",
      "Epoch 422/1000: Train Loss = 7.4017, Test Loss = 7.3374\n",
      "Epoch 423/1000: Train Loss = 7.3556, Test Loss = 7.2436\n",
      "Epoch 424/1000: Train Loss = 7.3945, Test Loss = 7.3306\n",
      "Epoch 425/1000: Train Loss = 7.3487, Test Loss = 7.2371\n",
      "Epoch 426/1000: Train Loss = 7.3874, Test Loss = 7.3239\n",
      "Epoch 427/1000: Train Loss = 7.3419, Test Loss = 7.2306\n",
      "Epoch 428/1000: Train Loss = 7.3803, Test Loss = 7.3172\n",
      "Epoch 429/1000: Train Loss = 7.3351, Test Loss = 7.2241\n",
      "Epoch 430/1000: Train Loss = 7.3732, Test Loss = 7.3105\n",
      "Epoch 431/1000: Train Loss = 7.3284, Test Loss = 7.2177\n",
      "Epoch 432/1000: Train Loss = 7.3662, Test Loss = 7.3039\n",
      "Epoch 433/1000: Train Loss = 7.3217, Test Loss = 7.2113\n",
      "Epoch 434/1000: Train Loss = 7.3592, Test Loss = 7.2974\n",
      "Epoch 435/1000: Train Loss = 7.3150, Test Loss = 7.2049\n",
      "Epoch 436/1000: Train Loss = 7.3523, Test Loss = 7.2908\n",
      "Epoch 437/1000: Train Loss = 7.3083, Test Loss = 7.1986\n",
      "Epoch 438/1000: Train Loss = 7.3454, Test Loss = 7.2843\n",
      "Epoch 439/1000: Train Loss = 7.3017, Test Loss = 7.1923\n",
      "Epoch 440/1000: Train Loss = 7.3386, Test Loss = 7.2779\n",
      "Epoch 441/1000: Train Loss = 7.2951, Test Loss = 7.1860\n",
      "Epoch 442/1000: Train Loss = 7.3317, Test Loss = 7.2714\n",
      "Epoch 443/1000: Train Loss = 7.2886, Test Loss = 7.1798\n",
      "Epoch 444/1000: Train Loss = 7.3250, Test Loss = 7.2650\n",
      "Epoch 445/1000: Train Loss = 7.2821, Test Loss = 7.1736\n",
      "Epoch 446/1000: Train Loss = 7.3182, Test Loss = 7.2587\n",
      "Epoch 447/1000: Train Loss = 7.2756, Test Loss = 7.1674\n",
      "Epoch 448/1000: Train Loss = 7.3115, Test Loss = 7.2524\n",
      "Epoch 449/1000: Train Loss = 7.2691, Test Loss = 7.1612\n",
      "Epoch 450/1000: Train Loss = 7.3049, Test Loss = 7.2461\n",
      "Epoch 451/1000: Train Loss = 7.2627, Test Loss = 7.1551\n",
      "Epoch 452/1000: Train Loss = 7.2983, Test Loss = 7.2399\n",
      "Epoch 453/1000: Train Loss = 7.2563, Test Loss = 7.1490\n",
      "Epoch 454/1000: Train Loss = 7.2917, Test Loss = 7.2337\n",
      "Epoch 455/1000: Train Loss = 7.2500, Test Loss = 7.1430\n",
      "Epoch 456/1000: Train Loss = 7.2851, Test Loss = 7.2275\n",
      "Epoch 457/1000: Train Loss = 7.2437, Test Loss = 7.1369\n",
      "Epoch 458/1000: Train Loss = 7.2786, Test Loss = 7.2213\n",
      "Epoch 459/1000: Train Loss = 7.2374, Test Loss = 7.1310\n",
      "Epoch 460/1000: Train Loss = 7.2721, Test Loss = 7.2153\n",
      "Epoch 461/1000: Train Loss = 7.2312, Test Loss = 7.1250\n",
      "Epoch 462/1000: Train Loss = 7.2657, Test Loss = 7.2092\n",
      "Epoch 463/1000: Train Loss = 7.2250, Test Loss = 7.1191\n",
      "Epoch 464/1000: Train Loss = 7.2593, Test Loss = 7.2032\n",
      "Epoch 465/1000: Train Loss = 7.2188, Test Loss = 7.1132\n",
      "Epoch 466/1000: Train Loss = 7.2529, Test Loss = 7.1972\n",
      "Epoch 467/1000: Train Loss = 7.2126, Test Loss = 7.1073\n",
      "Epoch 468/1000: Train Loss = 7.2466, Test Loss = 7.1912\n",
      "Epoch 469/1000: Train Loss = 7.2065, Test Loss = 7.1014\n",
      "Epoch 470/1000: Train Loss = 7.2403, Test Loss = 7.1853\n",
      "Epoch 471/1000: Train Loss = 7.2004, Test Loss = 7.0956\n",
      "Epoch 472/1000: Train Loss = 7.2340, Test Loss = 7.1794\n",
      "Epoch 473/1000: Train Loss = 7.1944, Test Loss = 7.0898\n",
      "Epoch 474/1000: Train Loss = 7.2278, Test Loss = 7.1735\n",
      "Epoch 475/1000: Train Loss = 7.1883, Test Loss = 7.0841\n",
      "Epoch 476/1000: Train Loss = 7.2216, Test Loss = 7.1677\n",
      "Epoch 477/1000: Train Loss = 7.1823, Test Loss = 7.0784\n",
      "Epoch 478/1000: Train Loss = 7.2154, Test Loss = 7.1619\n",
      "Epoch 479/1000: Train Loss = 7.1764, Test Loss = 7.0727\n",
      "Epoch 480/1000: Train Loss = 7.2093, Test Loss = 7.1561\n",
      "Epoch 481/1000: Train Loss = 7.1704, Test Loss = 7.0670\n",
      "Epoch 482/1000: Train Loss = 7.2032, Test Loss = 7.1504\n",
      "Epoch 483/1000: Train Loss = 7.1645, Test Loss = 7.0614\n",
      "Epoch 484/1000: Train Loss = 7.1971, Test Loss = 7.1447\n",
      "Epoch 485/1000: Train Loss = 7.1587, Test Loss = 7.0557\n",
      "Epoch 486/1000: Train Loss = 7.1911, Test Loss = 7.1390\n",
      "Epoch 487/1000: Train Loss = 7.1528, Test Loss = 7.0502\n",
      "Epoch 488/1000: Train Loss = 7.1851, Test Loss = 7.1334\n",
      "Epoch 489/1000: Train Loss = 7.1470, Test Loss = 7.0446\n",
      "Epoch 490/1000: Train Loss = 7.1791, Test Loss = 7.1278\n",
      "Epoch 491/1000: Train Loss = 7.1412, Test Loss = 7.0391\n",
      "Epoch 492/1000: Train Loss = 7.1732, Test Loss = 7.1222\n",
      "Epoch 493/1000: Train Loss = 7.1355, Test Loss = 7.0336\n",
      "Epoch 494/1000: Train Loss = 7.1673, Test Loss = 7.1167\n",
      "Epoch 495/1000: Train Loss = 7.1297, Test Loss = 7.0281\n",
      "Epoch 496/1000: Train Loss = 7.1614, Test Loss = 7.1112\n",
      "Epoch 497/1000: Train Loss = 7.1240, Test Loss = 7.0226\n",
      "Epoch 498/1000: Train Loss = 7.1556, Test Loss = 7.1057\n",
      "Epoch 499/1000: Train Loss = 7.1184, Test Loss = 7.0172\n",
      "Epoch 500/1000: Train Loss = 7.1498, Test Loss = 7.1002\n",
      "Epoch 501/1000: Train Loss = 7.1127, Test Loss = 7.0118\n",
      "Epoch 502/1000: Train Loss = 7.1440, Test Loss = 7.0948\n",
      "Epoch 503/1000: Train Loss = 7.1071, Test Loss = 7.0064\n",
      "Epoch 504/1000: Train Loss = 7.1382, Test Loss = 7.0894\n",
      "Epoch 505/1000: Train Loss = 7.1015, Test Loss = 7.0011\n",
      "Epoch 506/1000: Train Loss = 7.1325, Test Loss = 7.0840\n",
      "Epoch 507/1000: Train Loss = 7.0959, Test Loss = 6.9957\n",
      "Epoch 508/1000: Train Loss = 7.1268, Test Loss = 7.0787\n",
      "Epoch 509/1000: Train Loss = 7.0904, Test Loss = 6.9904\n",
      "Epoch 510/1000: Train Loss = 7.1212, Test Loss = 7.0733\n",
      "Epoch 511/1000: Train Loss = 7.0849, Test Loss = 6.9852\n",
      "Epoch 512/1000: Train Loss = 7.1155, Test Loss = 7.0681\n",
      "Epoch 513/1000: Train Loss = 7.0794, Test Loss = 6.9799\n",
      "Epoch 514/1000: Train Loss = 7.1099, Test Loss = 7.0628\n",
      "Epoch 515/1000: Train Loss = 7.0739, Test Loss = 6.9747\n",
      "Epoch 516/1000: Train Loss = 7.1043, Test Loss = 7.0576\n",
      "Epoch 517/1000: Train Loss = 7.0685, Test Loss = 6.9695\n",
      "Epoch 518/1000: Train Loss = 7.0988, Test Loss = 7.0523\n",
      "Epoch 519/1000: Train Loss = 7.0631, Test Loss = 6.9643\n",
      "Epoch 520/1000: Train Loss = 7.0933, Test Loss = 7.0472\n",
      "Epoch 521/1000: Train Loss = 7.0577, Test Loss = 6.9591\n",
      "Epoch 522/1000: Train Loss = 7.0878, Test Loss = 7.0420\n",
      "Epoch 523/1000: Train Loss = 7.0524, Test Loss = 6.9540\n",
      "Epoch 524/1000: Train Loss = 7.0823, Test Loss = 7.0369\n",
      "Epoch 525/1000: Train Loss = 7.0470, Test Loss = 6.9489\n",
      "Epoch 526/1000: Train Loss = 7.0769, Test Loss = 7.0318\n",
      "Epoch 527/1000: Train Loss = 7.0417, Test Loss = 6.9438\n",
      "Epoch 528/1000: Train Loss = 7.0715, Test Loss = 7.0267\n",
      "Epoch 529/1000: Train Loss = 7.0364, Test Loss = 6.9387\n",
      "Epoch 530/1000: Train Loss = 7.0661, Test Loss = 7.0216\n",
      "Epoch 531/1000: Train Loss = 7.0312, Test Loss = 6.9337\n",
      "Epoch 532/1000: Train Loss = 7.0607, Test Loss = 7.0166\n",
      "Epoch 533/1000: Train Loss = 7.0259, Test Loss = 6.9287\n",
      "Epoch 534/1000: Train Loss = 7.0554, Test Loss = 7.0116\n",
      "Epoch 535/1000: Train Loss = 7.0207, Test Loss = 6.9237\n",
      "Epoch 536/1000: Train Loss = 7.0501, Test Loss = 7.0066\n",
      "Epoch 537/1000: Train Loss = 7.0155, Test Loss = 6.9187\n",
      "Epoch 538/1000: Train Loss = 7.0448, Test Loss = 7.0016\n",
      "Epoch 539/1000: Train Loss = 7.0104, Test Loss = 6.9138\n",
      "Epoch 540/1000: Train Loss = 7.0396, Test Loss = 6.9967\n",
      "Epoch 541/1000: Train Loss = 7.0052, Test Loss = 6.9088\n",
      "Epoch 542/1000: Train Loss = 7.0343, Test Loss = 6.9918\n",
      "Epoch 543/1000: Train Loss = 7.0001, Test Loss = 6.9039\n",
      "Epoch 544/1000: Train Loss = 7.0291, Test Loss = 6.9869\n",
      "Epoch 545/1000: Train Loss = 6.9950, Test Loss = 6.8990\n",
      "Epoch 546/1000: Train Loss = 7.0240, Test Loss = 6.9820\n",
      "Epoch 547/1000: Train Loss = 6.9900, Test Loss = 6.8942\n",
      "Epoch 548/1000: Train Loss = 7.0188, Test Loss = 6.9772\n",
      "Epoch 549/1000: Train Loss = 6.9849, Test Loss = 6.8893\n",
      "Epoch 550/1000: Train Loss = 7.0137, Test Loss = 6.9724\n",
      "Epoch 551/1000: Train Loss = 6.9799, Test Loss = 6.8845\n",
      "Epoch 552/1000: Train Loss = 7.0086, Test Loss = 6.9675\n",
      "Epoch 553/1000: Train Loss = 6.9749, Test Loss = 6.8797\n",
      "Epoch 554/1000: Train Loss = 7.0035, Test Loss = 6.9628\n",
      "Epoch 555/1000: Train Loss = 6.9699, Test Loss = 6.8749\n",
      "Epoch 556/1000: Train Loss = 6.9984, Test Loss = 6.9580\n",
      "Epoch 557/1000: Train Loss = 6.9650, Test Loss = 6.8702\n",
      "Epoch 558/1000: Train Loss = 6.9934, Test Loss = 6.9533\n",
      "Epoch 559/1000: Train Loss = 6.9600, Test Loss = 6.8654\n",
      "Epoch 560/1000: Train Loss = 6.9884, Test Loss = 6.9486\n",
      "Epoch 561/1000: Train Loss = 6.9551, Test Loss = 6.8607\n",
      "Epoch 562/1000: Train Loss = 6.9834, Test Loss = 6.9439\n",
      "Epoch 563/1000: Train Loss = 6.9502, Test Loss = 6.8560\n",
      "Epoch 564/1000: Train Loss = 6.9785, Test Loss = 6.9392\n",
      "Epoch 565/1000: Train Loss = 6.9454, Test Loss = 6.8513\n",
      "Epoch 566/1000: Train Loss = 6.9735, Test Loss = 6.9345\n",
      "Epoch 567/1000: Train Loss = 6.9405, Test Loss = 6.8467\n",
      "Epoch 568/1000: Train Loss = 6.9686, Test Loss = 6.9299\n",
      "Epoch 569/1000: Train Loss = 6.9357, Test Loss = 6.8420\n",
      "Epoch 570/1000: Train Loss = 6.9637, Test Loss = 6.9253\n",
      "Epoch 571/1000: Train Loss = 6.9309, Test Loss = 6.8374\n",
      "Epoch 572/1000: Train Loss = 6.9589, Test Loss = 6.9207\n",
      "Epoch 573/1000: Train Loss = 6.9261, Test Loss = 6.8328\n",
      "Epoch 574/1000: Train Loss = 6.9540, Test Loss = 6.9162\n",
      "Epoch 575/1000: Train Loss = 6.9214, Test Loss = 6.8283\n",
      "Epoch 576/1000: Train Loss = 6.9492, Test Loss = 6.9116\n",
      "Epoch 577/1000: Train Loss = 6.9166, Test Loss = 6.8237\n",
      "Epoch 578/1000: Train Loss = 6.9444, Test Loss = 6.9071\n",
      "Epoch 579/1000: Train Loss = 6.9119, Test Loss = 6.8192\n",
      "Epoch 580/1000: Train Loss = 6.9396, Test Loss = 6.9026\n",
      "Epoch 581/1000: Train Loss = 6.9072, Test Loss = 6.8147\n",
      "Epoch 582/1000: Train Loss = 6.9348, Test Loss = 6.8981\n",
      "Epoch 583/1000: Train Loss = 6.9026, Test Loss = 6.8102\n",
      "Epoch 584/1000: Train Loss = 6.9301, Test Loss = 6.8937\n",
      "Epoch 585/1000: Train Loss = 6.8979, Test Loss = 6.8058\n",
      "Epoch 586/1000: Train Loss = 6.9254, Test Loss = 6.8892\n",
      "Epoch 587/1000: Train Loss = 6.8933, Test Loss = 6.8013\n",
      "Epoch 588/1000: Train Loss = 6.9207, Test Loss = 6.8848\n",
      "Epoch 589/1000: Train Loss = 6.8887, Test Loss = 6.7969\n",
      "Epoch 590/1000: Train Loss = 6.9160, Test Loss = 6.8804\n",
      "Epoch 591/1000: Train Loss = 6.8841, Test Loss = 6.7925\n",
      "Epoch 592/1000: Train Loss = 6.9114, Test Loss = 6.8761\n",
      "Epoch 593/1000: Train Loss = 6.8796, Test Loss = 6.7881\n",
      "Epoch 594/1000: Train Loss = 6.9068, Test Loss = 6.8717\n",
      "Epoch 595/1000: Train Loss = 6.8750, Test Loss = 6.7838\n",
      "Epoch 596/1000: Train Loss = 6.9022, Test Loss = 6.8674\n",
      "Epoch 597/1000: Train Loss = 6.8705, Test Loss = 6.7794\n",
      "Epoch 598/1000: Train Loss = 6.8976, Test Loss = 6.8631\n",
      "Epoch 599/1000: Train Loss = 6.8660, Test Loss = 6.7751\n",
      "Epoch 600/1000: Train Loss = 6.8930, Test Loss = 6.8588\n",
      "Epoch 601/1000: Train Loss = 6.8615, Test Loss = 6.7708\n",
      "Epoch 602/1000: Train Loss = 6.8885, Test Loss = 6.8545\n",
      "Epoch 603/1000: Train Loss = 6.8571, Test Loss = 6.7665\n",
      "Epoch 604/1000: Train Loss = 6.8840, Test Loss = 6.8502\n",
      "Epoch 605/1000: Train Loss = 6.8526, Test Loss = 6.7623\n",
      "Epoch 606/1000: Train Loss = 6.8795, Test Loss = 6.8460\n",
      "Epoch 607/1000: Train Loss = 6.8482, Test Loss = 6.7580\n",
      "Epoch 608/1000: Train Loss = 6.8750, Test Loss = 6.8418\n",
      "Epoch 609/1000: Train Loss = 6.8438, Test Loss = 6.7538\n",
      "Epoch 610/1000: Train Loss = 6.8706, Test Loss = 6.8376\n",
      "Epoch 611/1000: Train Loss = 6.8395, Test Loss = 6.7496\n",
      "Epoch 612/1000: Train Loss = 6.8661, Test Loss = 6.8334\n",
      "Epoch 613/1000: Train Loss = 6.8351, Test Loss = 6.7454\n",
      "Epoch 614/1000: Train Loss = 6.8617, Test Loss = 6.8293\n",
      "Epoch 615/1000: Train Loss = 6.8308, Test Loss = 6.7413\n",
      "Epoch 616/1000: Train Loss = 6.8573, Test Loss = 6.8252\n",
      "Epoch 617/1000: Train Loss = 6.8265, Test Loss = 6.7371\n",
      "Epoch 618/1000: Train Loss = 6.8530, Test Loss = 6.8210\n",
      "Epoch 619/1000: Train Loss = 6.8222, Test Loss = 6.7330\n",
      "Epoch 620/1000: Train Loss = 6.8486, Test Loss = 6.8170\n",
      "Epoch 621/1000: Train Loss = 6.8179, Test Loss = 6.7289\n",
      "Epoch 622/1000: Train Loss = 6.8443, Test Loss = 6.8129\n",
      "Epoch 623/1000: Train Loss = 6.8137, Test Loss = 6.7248\n",
      "Epoch 624/1000: Train Loss = 6.8400, Test Loss = 6.8088\n",
      "Epoch 625/1000: Train Loss = 6.8094, Test Loss = 6.7208\n",
      "Epoch 626/1000: Train Loss = 6.8358, Test Loss = 6.8048\n",
      "Epoch 627/1000: Train Loss = 6.8052, Test Loss = 6.7167\n",
      "Epoch 628/1000: Train Loss = 6.8315, Test Loss = 6.8008\n",
      "Epoch 629/1000: Train Loss = 6.8010, Test Loss = 6.7127\n",
      "Epoch 630/1000: Train Loss = 6.8273, Test Loss = 6.7968\n",
      "Epoch 631/1000: Train Loss = 6.7968, Test Loss = 6.7087\n",
      "Epoch 632/1000: Train Loss = 6.8231, Test Loss = 6.7928\n",
      "Epoch 633/1000: Train Loss = 6.7927, Test Loss = 6.7047\n",
      "Epoch 634/1000: Train Loss = 6.8189, Test Loss = 6.7889\n",
      "Epoch 635/1000: Train Loss = 6.7886, Test Loss = 6.7008\n",
      "Epoch 636/1000: Train Loss = 6.8147, Test Loss = 6.7850\n",
      "Epoch 637/1000: Train Loss = 6.7844, Test Loss = 6.6968\n",
      "Epoch 638/1000: Train Loss = 6.8106, Test Loss = 6.7811\n",
      "Epoch 639/1000: Train Loss = 6.7803, Test Loss = 6.6929\n",
      "Epoch 640/1000: Train Loss = 6.8064, Test Loss = 6.7772\n",
      "Epoch 641/1000: Train Loss = 6.7763, Test Loss = 6.6890\n",
      "Epoch 642/1000: Train Loss = 6.8023, Test Loss = 6.7733\n",
      "Epoch 643/1000: Train Loss = 6.7722, Test Loss = 6.6851\n",
      "Epoch 644/1000: Train Loss = 6.7983, Test Loss = 6.7695\n",
      "Epoch 645/1000: Train Loss = 6.7682, Test Loss = 6.6812\n",
      "Epoch 646/1000: Train Loss = 6.7942, Test Loss = 6.7657\n",
      "Epoch 647/1000: Train Loss = 6.7641, Test Loss = 6.6774\n",
      "Epoch 648/1000: Train Loss = 6.7902, Test Loss = 6.7619\n",
      "Epoch 649/1000: Train Loss = 6.7601, Test Loss = 6.6735\n",
      "Epoch 650/1000: Train Loss = 6.7862, Test Loss = 6.7581\n",
      "Epoch 651/1000: Train Loss = 6.7562, Test Loss = 6.6697\n",
      "Epoch 652/1000: Train Loss = 6.7822, Test Loss = 6.7543\n",
      "Epoch 653/1000: Train Loss = 6.7522, Test Loss = 6.6659\n",
      "Epoch 654/1000: Train Loss = 6.7782, Test Loss = 6.7506\n",
      "Epoch 655/1000: Train Loss = 6.7482, Test Loss = 6.6622\n",
      "Epoch 656/1000: Train Loss = 6.7742, Test Loss = 6.7469\n",
      "Epoch 657/1000: Train Loss = 6.7443, Test Loss = 6.6584\n",
      "Epoch 658/1000: Train Loss = 6.7703, Test Loss = 6.7432\n",
      "Epoch 659/1000: Train Loss = 6.7404, Test Loss = 6.6546\n",
      "Epoch 660/1000: Train Loss = 6.7664, Test Loss = 6.7395\n",
      "Epoch 661/1000: Train Loss = 6.7365, Test Loss = 6.6509\n",
      "Epoch 662/1000: Train Loss = 6.7625, Test Loss = 6.7358\n",
      "Epoch 663/1000: Train Loss = 6.7326, Test Loss = 6.6472\n",
      "Epoch 664/1000: Train Loss = 6.7587, Test Loss = 6.7322\n",
      "Epoch 665/1000: Train Loss = 6.7288, Test Loss = 6.6435\n",
      "Epoch 666/1000: Train Loss = 6.7548, Test Loss = 6.7286\n",
      "Epoch 667/1000: Train Loss = 6.7249, Test Loss = 6.6398\n",
      "Epoch 668/1000: Train Loss = 6.7510, Test Loss = 6.7250\n",
      "Epoch 669/1000: Train Loss = 6.7211, Test Loss = 6.6362\n",
      "Epoch 670/1000: Train Loss = 6.7472, Test Loss = 6.7214\n",
      "Epoch 671/1000: Train Loss = 6.7173, Test Loss = 6.6325\n",
      "Epoch 672/1000: Train Loss = 6.7434, Test Loss = 6.7178\n",
      "Epoch 673/1000: Train Loss = 6.7135, Test Loss = 6.6289\n",
      "Epoch 674/1000: Train Loss = 6.7397, Test Loss = 6.7143\n",
      "Epoch 675/1000: Train Loss = 6.7097, Test Loss = 6.6253\n",
      "Epoch 676/1000: Train Loss = 6.7360, Test Loss = 6.7108\n",
      "Epoch 677/1000: Train Loss = 6.7059, Test Loss = 6.6217\n",
      "Epoch 678/1000: Train Loss = 6.7322, Test Loss = 6.7073\n",
      "Epoch 679/1000: Train Loss = 6.7022, Test Loss = 6.6181\n",
      "Epoch 680/1000: Train Loss = 6.7285, Test Loss = 6.7038\n",
      "Epoch 681/1000: Train Loss = 6.6985, Test Loss = 6.6146\n",
      "Epoch 682/1000: Train Loss = 6.7249, Test Loss = 6.7003\n",
      "Epoch 683/1000: Train Loss = 6.6948, Test Loss = 6.6110\n",
      "Epoch 684/1000: Train Loss = 6.7212, Test Loss = 6.6969\n",
      "Epoch 685/1000: Train Loss = 6.6911, Test Loss = 6.6075\n",
      "Epoch 686/1000: Train Loss = 6.7176, Test Loss = 6.6935\n",
      "Epoch 687/1000: Train Loss = 6.6874, Test Loss = 6.6040\n",
      "Epoch 688/1000: Train Loss = 6.7140, Test Loss = 6.6901\n",
      "Epoch 689/1000: Train Loss = 6.6837, Test Loss = 6.6005\n",
      "Epoch 690/1000: Train Loss = 6.7104, Test Loss = 6.6867\n",
      "Epoch 691/1000: Train Loss = 6.6801, Test Loss = 6.5970\n",
      "Epoch 692/1000: Train Loss = 6.7068, Test Loss = 6.6833\n",
      "Epoch 693/1000: Train Loss = 6.6764, Test Loss = 6.5935\n",
      "Epoch 694/1000: Train Loss = 6.7032, Test Loss = 6.6800\n",
      "Epoch 695/1000: Train Loss = 6.6728, Test Loss = 6.5900\n",
      "Epoch 696/1000: Train Loss = 6.6997, Test Loss = 6.6766\n",
      "Epoch 697/1000: Train Loss = 6.6692, Test Loss = 6.5866\n",
      "Epoch 698/1000: Train Loss = 6.6962, Test Loss = 6.6733\n",
      "Epoch 699/1000: Train Loss = 6.6656, Test Loss = 6.5832\n",
      "Epoch 700/1000: Train Loss = 6.6927, Test Loss = 6.6700\n",
      "Epoch 701/1000: Train Loss = 6.6620, Test Loss = 6.5797\n",
      "Epoch 702/1000: Train Loss = 6.6892, Test Loss = 6.6667\n",
      "Epoch 703/1000: Train Loss = 6.6584, Test Loss = 6.5763\n",
      "Epoch 704/1000: Train Loss = 6.6857, Test Loss = 6.6635\n",
      "Epoch 705/1000: Train Loss = 6.6549, Test Loss = 6.5729\n",
      "Epoch 706/1000: Train Loss = 6.6823, Test Loss = 6.6602\n",
      "Epoch 707/1000: Train Loss = 6.6513, Test Loss = 6.5695\n",
      "Epoch 708/1000: Train Loss = 6.6788, Test Loss = 6.6570\n",
      "Epoch 709/1000: Train Loss = 6.6478, Test Loss = 6.5662\n",
      "Epoch 710/1000: Train Loss = 6.6754, Test Loss = 6.6538\n",
      "Epoch 711/1000: Train Loss = 6.6443, Test Loss = 6.5628\n",
      "Epoch 712/1000: Train Loss = 6.6720, Test Loss = 6.6506\n",
      "Epoch 713/1000: Train Loss = 6.6408, Test Loss = 6.5595\n",
      "Epoch 714/1000: Train Loss = 6.6686, Test Loss = 6.6474\n",
      "Epoch 715/1000: Train Loss = 6.6373, Test Loss = 6.5561\n",
      "Epoch 716/1000: Train Loss = 6.6653, Test Loss = 6.6443\n",
      "Epoch 717/1000: Train Loss = 6.6338, Test Loss = 6.5528\n",
      "Epoch 718/1000: Train Loss = 6.6619, Test Loss = 6.6411\n",
      "Epoch 719/1000: Train Loss = 6.6304, Test Loss = 6.5495\n",
      "Epoch 720/1000: Train Loss = 6.6586, Test Loss = 6.6380\n",
      "Epoch 721/1000: Train Loss = 6.6269, Test Loss = 6.5462\n",
      "Epoch 722/1000: Train Loss = 6.6552, Test Loss = 6.6349\n",
      "Epoch 723/1000: Train Loss = 6.6235, Test Loss = 6.5429\n",
      "Epoch 724/1000: Train Loss = 6.6519, Test Loss = 6.6318\n",
      "Epoch 725/1000: Train Loss = 6.6200, Test Loss = 6.5396\n",
      "Epoch 726/1000: Train Loss = 6.6486, Test Loss = 6.6287\n",
      "Epoch 727/1000: Train Loss = 6.6166, Test Loss = 6.5364\n",
      "Epoch 728/1000: Train Loss = 6.6453, Test Loss = 6.6256\n",
      "Epoch 729/1000: Train Loss = 6.6132, Test Loss = 6.5331\n",
      "Epoch 730/1000: Train Loss = 6.6421, Test Loss = 6.6225\n",
      "Epoch 731/1000: Train Loss = 6.6098, Test Loss = 6.5299\n",
      "Epoch 732/1000: Train Loss = 6.6388, Test Loss = 6.6195\n",
      "Epoch 733/1000: Train Loss = 6.6064, Test Loss = 6.5266\n",
      "Epoch 734/1000: Train Loss = 6.6356, Test Loss = 6.6164\n",
      "Epoch 735/1000: Train Loss = 6.6031, Test Loss = 6.5234\n",
      "Epoch 736/1000: Train Loss = 6.6323, Test Loss = 6.6134\n",
      "Epoch 737/1000: Train Loss = 6.5997, Test Loss = 6.5202\n",
      "Epoch 738/1000: Train Loss = 6.6291, Test Loss = 6.6104\n",
      "Epoch 739/1000: Train Loss = 6.5964, Test Loss = 6.5170\n",
      "Epoch 740/1000: Train Loss = 6.6259, Test Loss = 6.6074\n",
      "Epoch 741/1000: Train Loss = 6.5930, Test Loss = 6.5138\n",
      "Epoch 742/1000: Train Loss = 6.6227, Test Loss = 6.6044\n",
      "Epoch 743/1000: Train Loss = 6.5897, Test Loss = 6.5106\n",
      "Epoch 744/1000: Train Loss = 6.6196, Test Loss = 6.6014\n",
      "Epoch 745/1000: Train Loss = 6.5864, Test Loss = 6.5075\n",
      "Epoch 746/1000: Train Loss = 6.6164, Test Loss = 6.5985\n",
      "Epoch 747/1000: Train Loss = 6.5831, Test Loss = 6.5043\n",
      "Epoch 748/1000: Train Loss = 6.6132, Test Loss = 6.5955\n",
      "Epoch 749/1000: Train Loss = 6.5798, Test Loss = 6.5012\n",
      "Epoch 750/1000: Train Loss = 6.6101, Test Loss = 6.5926\n",
      "Epoch 751/1000: Train Loss = 6.5765, Test Loss = 6.4980\n",
      "Epoch 752/1000: Train Loss = 6.6070, Test Loss = 6.5897\n",
      "Epoch 753/1000: Train Loss = 6.5733, Test Loss = 6.4949\n",
      "Epoch 754/1000: Train Loss = 6.6039, Test Loss = 6.5868\n",
      "Epoch 755/1000: Train Loss = 6.5700, Test Loss = 6.4918\n",
      "Epoch 756/1000: Train Loss = 6.6007, Test Loss = 6.5839\n",
      "Epoch 757/1000: Train Loss = 6.5668, Test Loss = 6.4887\n",
      "Epoch 758/1000: Train Loss = 6.5977, Test Loss = 6.5810\n",
      "Epoch 759/1000: Train Loss = 6.5635, Test Loss = 6.4856\n",
      "Epoch 760/1000: Train Loss = 6.5946, Test Loss = 6.5781\n",
      "Epoch 761/1000: Train Loss = 6.5603, Test Loss = 6.4825\n",
      "Epoch 762/1000: Train Loss = 6.5915, Test Loss = 6.5752\n",
      "Epoch 763/1000: Train Loss = 6.5571, Test Loss = 6.4794\n",
      "Epoch 764/1000: Train Loss = 6.5885, Test Loss = 6.5724\n",
      "Epoch 765/1000: Train Loss = 6.5539, Test Loss = 6.4764\n",
      "Epoch 766/1000: Train Loss = 6.5854, Test Loss = 6.5696\n",
      "Epoch 767/1000: Train Loss = 6.5507, Test Loss = 6.4733\n",
      "Epoch 768/1000: Train Loss = 6.5824, Test Loss = 6.5667\n",
      "Epoch 769/1000: Train Loss = 6.5475, Test Loss = 6.4703\n",
      "Epoch 770/1000: Train Loss = 6.5793, Test Loss = 6.5639\n",
      "Epoch 771/1000: Train Loss = 6.5443, Test Loss = 6.4673\n",
      "Epoch 772/1000: Train Loss = 6.5763, Test Loss = 6.5611\n",
      "Epoch 773/1000: Train Loss = 6.5411, Test Loss = 6.4642\n",
      "Epoch 774/1000: Train Loss = 6.5733, Test Loss = 6.5583\n",
      "Epoch 775/1000: Train Loss = 6.5380, Test Loss = 6.4612\n",
      "Epoch 776/1000: Train Loss = 6.5703, Test Loss = 6.5555\n",
      "Epoch 777/1000: Train Loss = 6.5348, Test Loss = 6.4582\n",
      "Epoch 778/1000: Train Loss = 6.5674, Test Loss = 6.5527\n",
      "Epoch 779/1000: Train Loss = 6.5317, Test Loss = 6.4552\n",
      "Epoch 780/1000: Train Loss = 6.5644, Test Loss = 6.5500\n",
      "Epoch 781/1000: Train Loss = 6.5286, Test Loss = 6.4522\n",
      "Epoch 782/1000: Train Loss = 6.5614, Test Loss = 6.5472\n",
      "Epoch 783/1000: Train Loss = 6.5255, Test Loss = 6.4493\n",
      "Epoch 784/1000: Train Loss = 6.5585, Test Loss = 6.5445\n",
      "Epoch 785/1000: Train Loss = 6.5224, Test Loss = 6.4463\n",
      "Epoch 786/1000: Train Loss = 6.5556, Test Loss = 6.5417\n",
      "Epoch 787/1000: Train Loss = 6.5193, Test Loss = 6.4434\n",
      "Epoch 788/1000: Train Loss = 6.5526, Test Loss = 6.5390\n",
      "Epoch 789/1000: Train Loss = 6.5162, Test Loss = 6.4404\n",
      "Epoch 790/1000: Train Loss = 6.5497, Test Loss = 6.5363\n",
      "Epoch 791/1000: Train Loss = 6.5131, Test Loss = 6.4375\n",
      "Epoch 792/1000: Train Loss = 6.5468, Test Loss = 6.5336\n",
      "Epoch 793/1000: Train Loss = 6.5100, Test Loss = 6.4346\n",
      "Epoch 794/1000: Train Loss = 6.5439, Test Loss = 6.5309\n",
      "Epoch 795/1000: Train Loss = 6.5070, Test Loss = 6.4316\n",
      "Epoch 796/1000: Train Loss = 6.5410, Test Loss = 6.5282\n",
      "Epoch 797/1000: Train Loss = 6.5039, Test Loss = 6.4287\n",
      "Epoch 798/1000: Train Loss = 6.5382, Test Loss = 6.5256\n",
      "Epoch 799/1000: Train Loss = 6.5009, Test Loss = 6.4258\n",
      "Epoch 800/1000: Train Loss = 6.5353, Test Loss = 6.5229\n",
      "Epoch 801/1000: Train Loss = 6.4979, Test Loss = 6.4230\n",
      "Epoch 802/1000: Train Loss = 6.5324, Test Loss = 6.5202\n",
      "Epoch 803/1000: Train Loss = 6.4949, Test Loss = 6.4201\n",
      "Epoch 804/1000: Train Loss = 6.5296, Test Loss = 6.5176\n",
      "Epoch 805/1000: Train Loss = 6.4919, Test Loss = 6.4172\n",
      "Epoch 806/1000: Train Loss = 6.5268, Test Loss = 6.5150\n",
      "Epoch 807/1000: Train Loss = 6.4889, Test Loss = 6.4144\n",
      "Epoch 808/1000: Train Loss = 6.5239, Test Loss = 6.5124\n",
      "Epoch 809/1000: Train Loss = 6.4859, Test Loss = 6.4115\n",
      "Epoch 810/1000: Train Loss = 6.5211, Test Loss = 6.5097\n",
      "Epoch 811/1000: Train Loss = 6.4829, Test Loss = 6.4087\n",
      "Epoch 812/1000: Train Loss = 6.5183, Test Loss = 6.5071\n",
      "Epoch 813/1000: Train Loss = 6.4799, Test Loss = 6.4058\n",
      "Epoch 814/1000: Train Loss = 6.5155, Test Loss = 6.5046\n",
      "Epoch 815/1000: Train Loss = 6.4770, Test Loss = 6.4030\n",
      "Epoch 816/1000: Train Loss = 6.5127, Test Loss = 6.5020\n",
      "Epoch 817/1000: Train Loss = 6.4740, Test Loss = 6.4002\n",
      "Epoch 818/1000: Train Loss = 6.5100, Test Loss = 6.4994\n",
      "Epoch 819/1000: Train Loss = 6.4711, Test Loss = 6.3974\n",
      "Epoch 820/1000: Train Loss = 6.5072, Test Loss = 6.4968\n",
      "Epoch 821/1000: Train Loss = 6.4681, Test Loss = 6.3946\n",
      "Epoch 822/1000: Train Loss = 6.5045, Test Loss = 6.4943\n",
      "Epoch 823/1000: Train Loss = 6.4652, Test Loss = 6.3918\n",
      "Epoch 824/1000: Train Loss = 6.5017, Test Loss = 6.4917\n",
      "Epoch 825/1000: Train Loss = 6.4623, Test Loss = 6.3891\n",
      "Epoch 826/1000: Train Loss = 6.4990, Test Loss = 6.4892\n",
      "Epoch 827/1000: Train Loss = 6.4594, Test Loss = 6.3863\n",
      "Epoch 828/1000: Train Loss = 6.4962, Test Loss = 6.4867\n",
      "Epoch 829/1000: Train Loss = 6.4565, Test Loss = 6.3835\n",
      "Epoch 830/1000: Train Loss = 6.4935, Test Loss = 6.4842\n",
      "Epoch 831/1000: Train Loss = 6.4536, Test Loss = 6.3808\n",
      "Epoch 832/1000: Train Loss = 6.4908, Test Loss = 6.4817\n",
      "Epoch 833/1000: Train Loss = 6.4507, Test Loss = 6.3781\n",
      "Epoch 834/1000: Train Loss = 6.4881, Test Loss = 6.4792\n",
      "Epoch 835/1000: Train Loss = 6.4479, Test Loss = 6.3753\n",
      "Epoch 836/1000: Train Loss = 6.4854, Test Loss = 6.4767\n",
      "Epoch 837/1000: Train Loss = 6.4450, Test Loss = 6.3726\n",
      "Epoch 838/1000: Train Loss = 6.4828, Test Loss = 6.4742\n",
      "Epoch 839/1000: Train Loss = 6.4422, Test Loss = 6.3699\n",
      "Epoch 840/1000: Train Loss = 6.4801, Test Loss = 6.4717\n",
      "Epoch 841/1000: Train Loss = 6.4393, Test Loss = 6.3672\n",
      "Epoch 842/1000: Train Loss = 6.4774, Test Loss = 6.4693\n",
      "Epoch 843/1000: Train Loss = 6.4365, Test Loss = 6.3645\n",
      "Epoch 844/1000: Train Loss = 6.4748, Test Loss = 6.4668\n",
      "Epoch 845/1000: Train Loss = 6.4337, Test Loss = 6.3618\n",
      "Epoch 846/1000: Train Loss = 6.4721, Test Loss = 6.4644\n",
      "Epoch 847/1000: Train Loss = 6.4309, Test Loss = 6.3591\n",
      "Epoch 848/1000: Train Loss = 6.4695, Test Loss = 6.4619\n",
      "Epoch 849/1000: Train Loss = 6.4281, Test Loss = 6.3565\n",
      "Epoch 850/1000: Train Loss = 6.4669, Test Loss = 6.4595\n",
      "Epoch 851/1000: Train Loss = 6.4253, Test Loss = 6.3538\n",
      "Epoch 852/1000: Train Loss = 6.4643, Test Loss = 6.4571\n",
      "Epoch 853/1000: Train Loss = 6.4225, Test Loss = 6.3512\n",
      "Epoch 854/1000: Train Loss = 6.4616, Test Loss = 6.4547\n",
      "Epoch 855/1000: Train Loss = 6.4197, Test Loss = 6.3485\n",
      "Epoch 856/1000: Train Loss = 6.4590, Test Loss = 6.4523\n",
      "Epoch 857/1000: Train Loss = 6.4169, Test Loss = 6.3459\n",
      "Epoch 858/1000: Train Loss = 6.4565, Test Loss = 6.4499\n",
      "Epoch 859/1000: Train Loss = 6.4142, Test Loss = 6.3433\n",
      "Epoch 860/1000: Train Loss = 6.4539, Test Loss = 6.4475\n",
      "Epoch 861/1000: Train Loss = 6.4114, Test Loss = 6.3407\n",
      "Epoch 862/1000: Train Loss = 6.4513, Test Loss = 6.4451\n",
      "Epoch 863/1000: Train Loss = 6.4087, Test Loss = 6.3381\n",
      "Epoch 864/1000: Train Loss = 6.4487, Test Loss = 6.4428\n",
      "Epoch 865/1000: Train Loss = 6.4059, Test Loss = 6.3355\n",
      "Epoch 866/1000: Train Loss = 6.4462, Test Loss = 6.4404\n",
      "Epoch 867/1000: Train Loss = 6.4032, Test Loss = 6.3329\n",
      "Epoch 868/1000: Train Loss = 6.4436, Test Loss = 6.4381\n",
      "Epoch 869/1000: Train Loss = 6.4005, Test Loss = 6.3303\n",
      "Epoch 870/1000: Train Loss = 6.4411, Test Loss = 6.4357\n",
      "Epoch 871/1000: Train Loss = 6.3978, Test Loss = 6.3277\n",
      "Epoch 872/1000: Train Loss = 6.4386, Test Loss = 6.4334\n",
      "Epoch 873/1000: Train Loss = 6.3951, Test Loss = 6.3252\n",
      "Epoch 874/1000: Train Loss = 6.4361, Test Loss = 6.4311\n",
      "Epoch 875/1000: Train Loss = 6.3924, Test Loss = 6.3226\n",
      "Epoch 876/1000: Train Loss = 6.4335, Test Loss = 6.4288\n",
      "Epoch 877/1000: Train Loss = 6.3897, Test Loss = 6.3201\n",
      "Epoch 878/1000: Train Loss = 6.4310, Test Loss = 6.4264\n",
      "Epoch 879/1000: Train Loss = 6.3870, Test Loss = 6.3175\n",
      "Epoch 880/1000: Train Loss = 6.4285, Test Loss = 6.4241\n",
      "Epoch 881/1000: Train Loss = 6.3844, Test Loss = 6.3150\n",
      "Epoch 882/1000: Train Loss = 6.4261, Test Loss = 6.4219\n",
      "Epoch 883/1000: Train Loss = 6.3817, Test Loss = 6.3125\n",
      "Epoch 884/1000: Train Loss = 6.4236, Test Loss = 6.4196\n",
      "Epoch 885/1000: Train Loss = 6.3790, Test Loss = 6.3099\n",
      "Epoch 886/1000: Train Loss = 6.4211, Test Loss = 6.4173\n",
      "Epoch 887/1000: Train Loss = 6.3764, Test Loss = 6.3074\n",
      "Epoch 888/1000: Train Loss = 6.4186, Test Loss = 6.4150\n",
      "Epoch 889/1000: Train Loss = 6.3738, Test Loss = 6.3049\n",
      "Epoch 890/1000: Train Loss = 6.4162, Test Loss = 6.4128\n",
      "Epoch 891/1000: Train Loss = 6.3711, Test Loss = 6.3025\n",
      "Epoch 892/1000: Train Loss = 6.4137, Test Loss = 6.4105\n",
      "Epoch 893/1000: Train Loss = 6.3685, Test Loss = 6.3000\n",
      "Epoch 894/1000: Train Loss = 6.4113, Test Loss = 6.4083\n",
      "Epoch 895/1000: Train Loss = 6.3659, Test Loss = 6.2975\n",
      "Epoch 896/1000: Train Loss = 6.4089, Test Loss = 6.4060\n",
      "Epoch 897/1000: Train Loss = 6.3633, Test Loss = 6.2950\n",
      "Epoch 898/1000: Train Loss = 6.4064, Test Loss = 6.4038\n",
      "Epoch 899/1000: Train Loss = 6.3607, Test Loss = 6.2926\n",
      "Epoch 900/1000: Train Loss = 6.4040, Test Loss = 6.4016\n",
      "Epoch 901/1000: Train Loss = 6.3581, Test Loss = 6.2901\n",
      "Epoch 902/1000: Train Loss = 6.4016, Test Loss = 6.3994\n",
      "Epoch 903/1000: Train Loss = 6.3555, Test Loss = 6.2877\n",
      "Epoch 904/1000: Train Loss = 6.3992, Test Loss = 6.3972\n",
      "Epoch 905/1000: Train Loss = 6.3530, Test Loss = 6.2853\n",
      "Epoch 906/1000: Train Loss = 6.3968, Test Loss = 6.3950\n",
      "Epoch 907/1000: Train Loss = 6.3504, Test Loss = 6.2828\n",
      "Epoch 908/1000: Train Loss = 6.3944, Test Loss = 6.3928\n",
      "Epoch 909/1000: Train Loss = 6.3479, Test Loss = 6.2804\n",
      "Epoch 910/1000: Train Loss = 6.3920, Test Loss = 6.3906\n",
      "Epoch 911/1000: Train Loss = 6.3453, Test Loss = 6.2780\n",
      "Epoch 912/1000: Train Loss = 6.3897, Test Loss = 6.3884\n",
      "Epoch 913/1000: Train Loss = 6.3428, Test Loss = 6.2756\n",
      "Epoch 914/1000: Train Loss = 6.3873, Test Loss = 6.3862\n",
      "Epoch 915/1000: Train Loss = 6.3402, Test Loss = 6.2732\n",
      "Epoch 916/1000: Train Loss = 6.3849, Test Loss = 6.3841\n",
      "Epoch 917/1000: Train Loss = 6.3377, Test Loss = 6.2708\n",
      "Epoch 918/1000: Train Loss = 6.3826, Test Loss = 6.3819\n",
      "Epoch 919/1000: Train Loss = 6.3352, Test Loss = 6.2684\n",
      "Epoch 920/1000: Train Loss = 6.3802, Test Loss = 6.3798\n",
      "Epoch 921/1000: Train Loss = 6.3327, Test Loss = 6.2661\n",
      "Epoch 922/1000: Train Loss = 6.3779, Test Loss = 6.3776\n",
      "Epoch 923/1000: Train Loss = 6.3302, Test Loss = 6.2637\n",
      "Epoch 924/1000: Train Loss = 6.3756, Test Loss = 6.3755\n",
      "Epoch 925/1000: Train Loss = 6.3277, Test Loss = 6.2613\n",
      "Epoch 926/1000: Train Loss = 6.3733, Test Loss = 6.3734\n",
      "Epoch 927/1000: Train Loss = 6.3252, Test Loss = 6.2590\n",
      "Epoch 928/1000: Train Loss = 6.3709, Test Loss = 6.3712\n",
      "Epoch 929/1000: Train Loss = 6.3227, Test Loss = 6.2566\n",
      "Epoch 930/1000: Train Loss = 6.3686, Test Loss = 6.3691\n",
      "Epoch 931/1000: Train Loss = 6.3203, Test Loss = 6.2543\n",
      "Epoch 932/1000: Train Loss = 6.3663, Test Loss = 6.3670\n",
      "Epoch 933/1000: Train Loss = 6.3178, Test Loss = 6.2520\n",
      "Epoch 934/1000: Train Loss = 6.3640, Test Loss = 6.3649\n",
      "Epoch 935/1000: Train Loss = 6.3153, Test Loss = 6.2496\n",
      "Epoch 936/1000: Train Loss = 6.3618, Test Loss = 6.3628\n",
      "Epoch 937/1000: Train Loss = 6.3129, Test Loss = 6.2473\n",
      "Epoch 938/1000: Train Loss = 6.3595, Test Loss = 6.3608\n",
      "Epoch 939/1000: Train Loss = 6.3105, Test Loss = 6.2450\n",
      "Epoch 940/1000: Train Loss = 6.3572, Test Loss = 6.3587\n",
      "Epoch 941/1000: Train Loss = 6.3080, Test Loss = 6.2427\n",
      "Epoch 942/1000: Train Loss = 6.3549, Test Loss = 6.3566\n",
      "Epoch 943/1000: Train Loss = 6.3056, Test Loss = 6.2404\n",
      "Epoch 944/1000: Train Loss = 6.3527, Test Loss = 6.3545\n",
      "Epoch 945/1000: Train Loss = 6.3032, Test Loss = 6.2381\n",
      "Epoch 946/1000: Train Loss = 6.3504, Test Loss = 6.3525\n",
      "Epoch 947/1000: Train Loss = 6.3008, Test Loss = 6.2359\n",
      "Epoch 948/1000: Train Loss = 6.3482, Test Loss = 6.3504\n",
      "Epoch 949/1000: Train Loss = 6.2984, Test Loss = 6.2336\n",
      "Epoch 950/1000: Train Loss = 6.3459, Test Loss = 6.3484\n",
      "Epoch 951/1000: Train Loss = 6.2960, Test Loss = 6.2313\n",
      "Epoch 952/1000: Train Loss = 6.3437, Test Loss = 6.3463\n",
      "Epoch 953/1000: Train Loss = 6.2936, Test Loss = 6.2291\n",
      "Epoch 954/1000: Train Loss = 6.3415, Test Loss = 6.3443\n",
      "Epoch 955/1000: Train Loss = 6.2912, Test Loss = 6.2268\n",
      "Epoch 956/1000: Train Loss = 6.3393, Test Loss = 6.3423\n",
      "Epoch 957/1000: Train Loss = 6.2888, Test Loss = 6.2246\n",
      "Epoch 958/1000: Train Loss = 6.3370, Test Loss = 6.3403\n",
      "Epoch 959/1000: Train Loss = 6.2864, Test Loss = 6.2224\n",
      "Epoch 960/1000: Train Loss = 6.3348, Test Loss = 6.3382\n",
      "Epoch 961/1000: Train Loss = 6.2841, Test Loss = 6.2201\n",
      "Epoch 962/1000: Train Loss = 6.3326, Test Loss = 6.3362\n",
      "Epoch 963/1000: Train Loss = 6.2817, Test Loss = 6.2179\n",
      "Epoch 964/1000: Train Loss = 6.3304, Test Loss = 6.3342\n",
      "Epoch 965/1000: Train Loss = 6.2794, Test Loss = 6.2157\n",
      "Epoch 966/1000: Train Loss = 6.3282, Test Loss = 6.3322\n",
      "Epoch 967/1000: Train Loss = 6.2770, Test Loss = 6.2135\n",
      "Epoch 968/1000: Train Loss = 6.3261, Test Loss = 6.3302\n",
      "Epoch 969/1000: Train Loss = 6.2747, Test Loss = 6.2113\n",
      "Epoch 970/1000: Train Loss = 6.3239, Test Loss = 6.3283\n",
      "Epoch 971/1000: Train Loss = 6.2724, Test Loss = 6.2091\n",
      "Epoch 972/1000: Train Loss = 6.3217, Test Loss = 6.3263\n",
      "Epoch 973/1000: Train Loss = 6.2701, Test Loss = 6.2069\n",
      "Epoch 974/1000: Train Loss = 6.3196, Test Loss = 6.3243\n",
      "Epoch 975/1000: Train Loss = 6.2677, Test Loss = 6.2047\n",
      "Epoch 976/1000: Train Loss = 6.3174, Test Loss = 6.3223\n",
      "Epoch 977/1000: Train Loss = 6.2654, Test Loss = 6.2025\n",
      "Epoch 978/1000: Train Loss = 6.3152, Test Loss = 6.3204\n",
      "Epoch 979/1000: Train Loss = 6.2631, Test Loss = 6.2004\n",
      "Epoch 980/1000: Train Loss = 6.3131, Test Loss = 6.3184\n",
      "Epoch 981/1000: Train Loss = 6.2608, Test Loss = 6.1982\n",
      "Epoch 982/1000: Train Loss = 6.3110, Test Loss = 6.3165\n",
      "Epoch 983/1000: Train Loss = 6.2585, Test Loss = 6.1961\n",
      "Epoch 984/1000: Train Loss = 6.3088, Test Loss = 6.3145\n",
      "Epoch 985/1000: Train Loss = 6.2563, Test Loss = 6.1939\n",
      "Epoch 986/1000: Train Loss = 6.3067, Test Loss = 6.3126\n",
      "Epoch 987/1000: Train Loss = 6.2540, Test Loss = 6.1918\n",
      "Epoch 988/1000: Train Loss = 6.3046, Test Loss = 6.3107\n",
      "Epoch 989/1000: Train Loss = 6.2517, Test Loss = 6.1896\n",
      "Epoch 990/1000: Train Loss = 6.3025, Test Loss = 6.3087\n",
      "Epoch 991/1000: Train Loss = 6.2495, Test Loss = 6.1875\n",
      "Epoch 992/1000: Train Loss = 6.3004, Test Loss = 6.3068\n",
      "Epoch 993/1000: Train Loss = 6.2472, Test Loss = 6.1854\n",
      "Epoch 994/1000: Train Loss = 6.2983, Test Loss = 6.3049\n",
      "Epoch 995/1000: Train Loss = 6.2450, Test Loss = 6.1833\n",
      "Epoch 996/1000: Train Loss = 6.2962, Test Loss = 6.3030\n",
      "Epoch 997/1000: Train Loss = 6.2427, Test Loss = 6.1812\n",
      "Epoch 998/1000: Train Loss = 6.2941, Test Loss = 6.3011\n",
      "Epoch 999/1000: Train Loss = 6.2405, Test Loss = 6.1791\n",
      "Epoch 1000/1000: Train Loss = 6.2920, Test Loss = 6.2992\n",
      "Total runtime: 438.14 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = logistic_regression(learning_rate=0.1, epochs=1000)\n",
    "\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the LR model\n",
    "\n",
    "lr.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# End timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total runtime: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[ 242  170  805  623  512   52  126  153 1721  596]\n",
      " [  74  917  496  695  375  124  255  132  893 1039]\n",
      " [  76  126 1327  857 1070  141  361  176  593  273]\n",
      " [  45  175  798 1455  658  293  456  179  496  445]\n",
      " [  30   84 1165  772 1379  154  500  257  355  304]\n",
      " [  33  154  875 1341  800  418  393  184  493  309]\n",
      " [  28  137  812  977  919  222 1088  174  338  305]\n",
      " [  54  165  843  858  960  165  388  686  375  506]\n",
      " [  77  187  447  460  231   64   82   58 2752  642]\n",
      " [  94  454  387  488  354   77  251  139  941 1815]]\n",
      "Accuracy: 0.2416\n",
      "Precision: 0.2648\n",
      "Recall: 0.2416\n",
      "F1 Score: 0.2527\n",
      "\n",
      "Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[ 39  29 179 109  98  15  26  23 370 112]\n",
      " [ 12 168 112 131  71  35  52  17 187 215]\n",
      " [ 11  30 292 156 193  33  71  34 126  54]\n",
      " [ 11  37 161 290 132  63  80  28 120  78]\n",
      " [ 10  12 228 162 274  24 102  57  78  53]\n",
      " [ 11  31 170 276 153  79  72  43 111  54]\n",
      " [  4  31 155 201 184  37 220  44  65  59]\n",
      " [  8  40 170 153 182  38  57 147  84 121]\n",
      " [ 14  46  79 105  45  14  16   8 533 140]\n",
      " [ 16 101  69  97  80  12  52  30 185 358]]\n",
      "Accuracy: 0.2400\n",
      "Precision: 0.2590\n",
      "Recall: 0.2400\n",
      "F1 Score: 0.2491\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for train and test sets\n",
    "train_pred = lr.predict(X_train)\n",
    "\n",
    "# Calculate training metrics\n",
    "train_cm = confusion_matrix(y_train, train_pred)\n",
    "train_accuracy = accuracy(y_train, train_pred)\n",
    "train_precision = precision(train_cm)\n",
    "train_recall = recall(train_cm)\n",
    "train_f1 = f1_score(train_cm)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nTraining Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm)\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1 Score: {train_f1:.4f}\")\n",
    "\n",
    "\n",
    "test_pred = lr.predict(X_test)\n",
    "\n",
    "test_cm = confusion_matrix(y_test, test_pred)\n",
    "test_accuracy = accuracy(y_test, test_pred)\n",
    "test_precision = precision(test_cm)\n",
    "test_recall = recall(test_cm)\n",
    "test_f1 = f1_score(test_cm)\n",
    "\n",
    "print(\"\\nValidation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm)\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAIjCAYAAAA9VuvLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACK30lEQVR4nOzdd3hUZd7G8e+ZyWTSe0iBQKgSioAUBVaaFAERkbWBCpa1ARas2Bb7a10suJZdwd6FtSFFQZpIb9JbQu9ppE1mzvvHkNEQSgaSzITcn+s6F5lznjnzm/Ds6u1TjmGapomIiIiIiIh4WHxdgIiIiIiIiL9RUBIRERERETmGgpKIiIiIiMgxFJRERERERESOoaAkIiIiIiJyDAUlERERERGRYygoiYiIiIiIHENBSURERERE5BgKSiIiIiIiIsdQUBIRqaEmTpyIYRgsXrzY16WIiIj4HQUlEZFKUhJETnQsWLDA1yX6teHDh2MYBhEREeTn55e5vnHjRs/v8qWXXvJBheWzbds2v6px+PDhhIWF+boMERG/F+DrAkREznZPPvkk9evXL3O+UaNGPqimegkICCAvL4/vvvuOK6+8stS1jz/+mKCgIAoKCnxUnYiInM0UlEREKlnfvn1p166dr8uolux2O507d+bTTz8tE5Q++eQT+vfvz9dff+2j6kRE5GymqXciIj7216lZ//rXv6hXrx7BwcF07dqV1atXl2n/yy+/cOGFFxIaGkpUVBQDBw5k7dq1Zdrt3LmTm266ieTkZOx2O/Xr1+f222+nqKioVLvCwkJGjx5NfHw8oaGhDBo0iP3795+05pdeegnDMEhPTy9zbcyYMQQGBnL48GHAPUVu8ODBJCYmEhQURJ06dbj66qvJysoq1+9nyJAhTJkyhczMTM+5RYsWsXHjRoYMGXLc92RmZnL33XeTkpKC3W6nUaNGPP/887hcrjLfo1OnTsTGxhIcHEzbtm356quvytzPMAxGjhzJ5MmTadGiBXa7nebNm/PTTz+V6zuUx759+7jppptISEggKCiIVq1a8f7775dpd/DgQa677joiIiKIiopi2LBhrFixAsMwmDhxYoXV8+WXX9K2bVuCg4OJi4vj2muvZefOnaXa7NmzhxtuuIE6depgt9tJSkpi4MCBbNu2zdNm8eLF9OnTh7i4OIKDg6lfvz433nhjhdUpIlJZNKIkIlLJsrKyOHDgQKlzhmEQGxtb6twHH3xATk4OI0aMoKCggFdffZUePXqwatUqEhISAJgxYwZ9+/alQYMGjB07lvz8fF5//XU6d+7M0qVLSU1NBWDXrl106NCBzMxMbrnlFpo2bcrOnTv56quvyMvLIzAw0PO5o0aNIjo6mn/+859s27aNcePGMXLkSD7//PMTfqcrr7ySBx54gC+++IL777+/1LUvvviC3r17Ex0dTVFREX369KGwsJBRo0aRmJjIzp07+f7778nMzCQyMvKUv7/LL7+c2267jW+++cbzL9iffPIJTZs25bzzzivTPi8vj65du7Jz505uvfVW6taty/z58xkzZgy7d+9m3Lhxnravvvoql156KUOHDqWoqIjPPvuMK664gu+//57+/fuXuu/cuXP55ptvuOOOOwgPD+e1115j8ODBZGRklPm79FZ+fj7dunVj06ZNjBw5kvr16/Pll18yfPhwMjMzueuuuwBwuVwMGDCAhQsXcvvtt9O0aVP+97//MWzYsDP6/GNNnDiRG264gfbt2/Pcc8+xd+9eXn31VebNm8eyZcuIiooCYPDgwfzxxx+MGjWK1NRU9u3bx/Tp08nIyPC87t27N/Hx8Tz00ENERUWxbds2vvnmmwqtV0SkUpgiIlIpJkyYYALHPex2u6fd1q1bTcAMDg42d+zY4Tn/+++/m4B5zz33eM61bt3arFWrlnnw4EHPuRUrVpgWi8W8/vrrPeeuv/5602KxmIsWLSpTl8vlKlVfz549PedM0zTvuece02q1mpmZmSf9fh07djTbtm1b6tzChQtNwPzggw9M0zTNZcuWmYD55ZdfnvRexzNs2DAzNDTUNE3T/Pvf/25edNFFpmmaptPpNBMTE80nnnjC87t78cUXPe976qmnzNDQUHPDhg2l7vfQQw+ZVqvVzMjI8JzLy8sr1aaoqMhs0aKF2aNHj1LnATMwMNDctGmT59yKFStMwHz99ddP+j2OV+Oxxo0bZwLmRx99VKqWjh07mmFhYWZ2drZpmqb59ddfm4A5btw4Tzun02n26NHDBMwJEyactBbTLP17PZ6ioiKzVq1aZosWLcz8/HzP+e+//94EzMcff9w0TdM8fPjwKb/XpEmTTOC4/VBExN9p6p2ISCUbP34806dPL3VMmTKlTLvLLruM2rVre1536NCB888/nx9//BGA3bt3s3z5coYPH05MTIyn3bnnnkuvXr087VwuF5MnT2bAgAHHXRtlGEap17fcckupcxdeeCFOp/O40+r+6qqrrmLJkiVs3rzZc+7zzz/HbrczcOBAAM+I0dSpU8nLyzvp/U5myJAhzJo1iz179vDLL7+wZ8+eE067+/LLL7nwwguJjo7mwIEDnqNnz544nU5mz57taRscHOz5+fDhw2RlZXHhhReydOnSMvft2bMnDRs29Lw+99xziYiIYMuWLaf9vUr8+OOPJCYmcs0113jO2Ww27rzzTnJzc/n1118B+Omnn7DZbPzjH//wtLNYLIwYMeKMayixePFi9u3bxx133EFQUJDnfP/+/WnatCk//PAD4P7dBQYGMmvWLM80y2OVjDx9//33OByOCqtRRKQqKCiJiFSyDh060LNnz1JH9+7dy7Rr3LhxmXNNmjTxrPcoCS7nnHNOmXZpaWkcOHCAI0eOsH//frKzs2nRokW56qtbt26p19HR0QAn/JffEldccQUWi8UzRc80Tb788kv69u1LREQEAPXr12f06NH85z//IS4ujj59+jB+/Phyr08q0a9fP8LDw/n888/5+OOPad++/Ql3Ddy4cSM//fQT8fHxpY6ePXsC7rVAJb7//nsuuOACgoKCiImJIT4+nn//+9/Hre/Y3xO4f1en+j2VR3p6Oo0bN8ZiKf2P5bS0NM/1kj+TkpIICQkp1e7Y30V+fj579uwpdXhTCxy/nzVt2tRz3W638/zzzzNlyhQSEhLo0qULL7zwQqnP6tq1K4MHD+aJJ54gLi6OgQMHMmHCBAoLC8tdj4iIrygoiYjUcFar9bjnTdM86fuSk5O58MIL+eKLLwBYsGABGRkZXHXVVaXavfzyy6xcuZKHH36Y/Px87rzzTpo3b86OHTvKXaPdbufyyy/n/fffZ9KkSSccTQL3iFqvXr3KjOKVHIMHDwZgzpw5XHrppQQFBfHmm2/y448/Mn36dIYMGXLc7366vydf+Pzzz0lKSip1VIa7776bDRs28NxzzxEUFMRjjz1GWloay5YtA9yjl1999RW//fYbI0eOZOfOndx44420bduW3NzcSqlJRKSiaDMHERE/sXHjxjLnNmzY4NmgoV69egCsX7++TLt169YRFxdHaGgowcHBREREHHfHvIp21VVXcccdd7B+/Xo+//xzQkJCGDBgQJl2LVu2pGXLljz66KPMnz+fzp0789Zbb/H000+X+7OGDBnCe++9h8Vi4eqrrz5hu4YNG5Kbm+sZQTqRr7/+mqCgIKZOnYrdbvecnzBhQrlrqij16tVj5cqVuFyuUqNK69at81wv+XPmzJnk5eWVGlXatGlTqfv16dOH6dOnn3Yt4O5nPXr0KHVt/fr1nuslGjZsyL333su9997Lxo0bad26NS+//DIfffSRp80FF1zABRdcwDPPPMMnn3zC0KFD+eyzz7j55ptPq0YRkaqgESURET8xefLkUtsvL1y4kN9//52+ffsCkJSUROvWrXn//fdLbZW9evVqpk2bRr9+/QD3mpXLLruM7777jsWLF5f5nIocARk8eDBWq5VPP/2UL7/8kksuuYTQ0FDP9ezsbIqLi0u9p2XLllgsFq+nX3Xv3p2nnnqKN954g8TExBO2u/LKK/ntt9+YOnVqmWuZmZmeeqxWK4Zh4HQ6Pde3bdvG5MmTvaqrIvTr1489e/aU2mmwuLiY119/nbCwMLp27Qq4A5DD4eDdd9/1tHO5XIwfP77U/ZKSkspM9yyvdu3aUatWLd56661Sf0dTpkxh7dq1nt0A8/Lyyjzst2HDhoSHh3ved/jw4TL9rXXr1gCaficifk8jSiIilWzKlCmekYG/6tSpEw0aNPC8btSoEX/729+4/fbbKSwsZNy4ccTGxvLAAw942rz44ov07duXjh07ctNNN3m2B4+MjGTs2LGeds8++yzTpk2ja9eu3HLLLaSlpbF7926+/PJL5s6d61lkf6Zq1apF9+7deeWVV8jJySkz7e6XX35h5MiRXHHFFTRp0oTi4mI+/PBDrFarZwpceVksFh599NFTtrv//vv59ttvueSSSxg+fDht27blyJEjrFq1iq+++opt27YRFxdH//79eeWVV7j44osZMmQI+/btY/z48TRq1IiVK1d6VVt5/Pzzz2WCBbg38bjlllt4++23GT58OEuWLCE1NZWvvvqKefPmMW7cOMLDwz1tO3TowL333sumTZto2rQp3377LYcOHQLKbtRxIg6H47ijeTExMdxxxx08//zz3HDDDXTt2pVrrrnGsz14amoq99xzD+Ae7bzooou48soradasGQEBAUyaNIm9e/d6Rvzef/993nzzTQYNGkTDhg3Jycnh3XffJSIiwhPsRUT8li+33BMROZudbHtw/rKV81+3j3755ZfNlJQU0263mxdeeKG5YsWKMvedMWOG2blzZzM4ONiMiIgwBwwYYK5Zs6ZMu/T0dPP666834+PjTbvdbjZo0MAcMWKEWVhYWKq+Y7dunjlzpgmYM2fOLNf3fPfdd03ADA8PL7WdtGma5pYtW8wbb7zRbNiwoRkUFGTGxMSY3bt3N2fMmHHK+55qG2vTPPHW2zk5OeaYMWPMRo0amYGBgWZcXJzZqVMn86WXXjKLioo87f773/+ajRs3Nu12u9m0aVNzwoQJ5j//+U/z2H88AuaIESPKfH69evXMYcOGlavGEx0ffvihaZqmuXfvXvOGG24w4+LizMDAQLNly5bH3e57//795pAhQ8zw8HAzMjLSHD58uDlv3jwTMD/77LOT1mKa7t/riWpp2LChp93nn39utmnTxrTb7WZMTIw5dOjQUtvXHzhwwBwxYoTZtGlTMzQ01IyMjDTPP/9884svvvC0Wbp0qXnNNdeYdevWNe12u1mrVi3zkksuMRcvXnzKOkVEfM0wTT9chSoiUoNs27aN+vXr8+KLL3Lffff5uhyphiZPnsygQYOYO3cunTt39nU5IiJnBa1REhERqUby8/NLvXY6nbz++utERERw3nnn+agqEZGzj9YoiYiIVCOjRo0iPz+fjh07UlhYyDfffMP8+fN59tlnSz1AV0REzoyCkoiISDXSo0cPXn75Zb7//nsKCgpo1KgRr7/+OiNHjvR1aSIiZxWtURIRERERETmG1iiJiIiIiIgcQ0FJRERERETkGGf9GiWXy8WuXbsIDw8v94P4RERERETk7GOaJjk5OSQnJ2OxnHzM6KwPSrt27SIlJcXXZYiIiIiIiJ/Yvn07derUOWmbsz4ohYeHA+5fRkREhE9rcTgcTJs2jd69e2Oz2Xxai1QP6jPiLfUZ8Zb6jHhLfUa85U99Jjs7m5SUFE9GOJmzPiiVTLeLiIjwi6AUEhJCRESEzzuJVA/qM+It9RnxlvqMeEt9Rrzlj32mPEtytJmDiIiIiIjIMRSUREREREREjqGgJCIiIiIicoyzfo2SiIiIiPgfp9OJw+HwdRlSBRwOBwEBARQUFOB0Oiv1s6xWKwEBARXyWCAFJRERERGpUrm5uezYsQPTNH1dilQB0zRJTExk+/btVfJc05CQEJKSkggMDDyj+ygoiYiIiEiVcTqd7Nixg5CQEOLj46vkX5zFt1wuF7m5uYSFhZ3yIa9nwjRNioqK2L9/P1u3bqVx48Zn9HkKSiIiIiJSZRwOB6ZpEh8fT3BwsK/LkSrgcrkoKioiKCioUoMSQHBwMDabjfT0dM9nni5t5iAiIiIiVU4jSVJZKiqMKSiJiIiIiIgcQ0FJRERERETkGApKIiIiIiI+kJqayrhx48rdftasWRiGQWZmZqXVJH9SUBIREREROQnDME56jB079rTuu2jRIm655ZZyt+/UqRO7d+8mMjLytD6vvBTI3LTrnYiIiIjISezevdvz8+eff87jjz/O+vXrPefCwsI8P5umidPpJCDg1P+aHR8f71UdgYGBJCYmevUeOX0aURIRERERnzFNk7yiYp8c5X3gbWJioueIjIzEMAzP63Xr1hEeHs6UKVNo27YtdruduXPnsnnzZgYOHEhCQgJhYWG0b9+eGTNmlLrvsVPvDMPgP//5D4MGDSIkJITGjRvz7bffeq4fO9IzceJEoqKimDp1KmlpaYSFhXHxxReXCnbFxcXceeedREVFERsby4MPPsiwYcO47LLLTvvv7PDhw1x//fVER0cTEhJC37592bhxo+d6eno6AwYMIDo6mtDQUFq2bMm0adM87x06dKhne/jGjRszYcKE066lMmlESURERER8Jt/hpNnjU33y2Wue7ENIYMX86/BDDz3ESy+9RIMGDYiOjmb79u3069ePZ555BrvdzgcffMCAAQNYv349devWPeF9nnjiCV544QVefPFFXn/9dYYOHUp6ejoxMTHHbZ+Xl8dLL73Ehx9+iMVi4dprr+W+++7j448/BuD555/n448/ZsKECaSlpfHqq68yefJkunfvftrfdfjw4WzcuJFvv/2WiIgIHnzwQfr168eaNWuw2WyMGDGCoqIiZs+eTWhoKKtXr8ZqtQLw2GOPsWbNGqZMmUJcXBybNm0iPz//tGupTApKIiIiIiJn6Mknn6RXr16e1zExMbRq1crz+qmnnmLSpEl8++23jBw58oT3GT58ONdccw0Azz77LK+99hoLFy7k4osvPm57h8PBW2+9RcOGDQEYOXIkTz75pOf666+/zpgxYxg0aBAAb7zxBj/++ONpf8+SgDRv3jw6deoEwMcff0xKSgqTJ0/miiuuICMjg8GDB9OyZUvAPXKWnZ0NQEZGBm3atKFdu3aea/5KQakKGelzScpcBLltIbqOr8sRERER8blgm5U1T/bx2WdXlJJ/8S+Rm5vL2LFj+eGHH9i9ezfFxcXk5+eTkZFx0vuce+65np9DQ0OJiIhg3759J2wfEhLiCUkASUlJnvZZWVns3buXDh06eK5brVbatm2Ly+Xy6vuVWLt2LQEBAZx//vmec7GxsZxzzjmsXbsWgDvvvJPbb7+dadOm0bNnTwYNGuQJRLfffjuDBw9m6dKl9O7dm8suu8wTuPyN1ihVIcvPY+mw9XWM3St8XYqIiIiIXzAMg5DAAJ8chmFU2PcIDQ0t9fq+++5j0qRJPPvss8yZM4fly5fTsmVLioqKTnofm81W5vdzslBzvPblXXtVWW6++Wa2bNnCddddx6pVq+jQoQPvvPMOAH379iU9PZ177rmHXbt2cdFFF3Hffff5tN4TUVCqQtsOFQCwfk+2jysRERERkco0b948hg8fzqBBg2jZsiWJiYls27atSmuIjIwkISGBRYsWec45nU6WLl162vdMS0ujuLiY33//3XPu4MGDrF+/nmbNmnnOpaSkcNttt/HNN98wevRo3n//fc+1+Ph4hg0bxkcffcS4ceM8IcrfaOpdFSp0Hv2zuNi3hYiIiIhIpWrcuDHffPMNAwYMwDAMHnvssdOe7nYmRo0axXPPPUejRo1o2rQpr7/+OocPHy7XaNqqVasIDw/3vDYMg1atWjFw4ED+8Y9/8PbbbxMeHs5DDz1E7dq1GThwIAB33303ffv2pUmTJhw+fJhZs2ZxzjnnAPD444/Ttm1bmjdvTmFhId9//z1paWmV8+XPkIJSFTKPDuAZTqePKxERERGRyvTKK69w44030qlTJ+Li4njwwQc9GxpUpQcffJA9e/Zw/fXXY7VaueWWW+jTp49nF7qT6dKlS6nXVquV4uJiJkyYwF133cUll1xCUVERXbp04ccff/RMA3Q6nYwYMYIdO3YQERFBnz59eOKJJwD3s6DGjBnDtm3bCA4O5sILL+Szzz6r+C9eAQzT15MYK1l2djaRkZFkZWURERHh01r+eKYzzR2rWdLhFdr2u8mntUj14HA4+PHHH+nXr1+ZOcgix6M+I95SnxFvnWmfKSgoYOvWrdSvX5+goKBKqFBOxuVykZaWxpVXXslTTz1VZZ+ZnZ1NREQEFkvlr/w5WR/zJhtoRKkKlYwo4YNhVxERERGpedLT05k2bRpdu3alsLCQN954g61btzJkyBBfl+b3tJlDFXIZ7l+3qaAkIiIiIlXAYrEwceJE2rdvT+fOnVm1ahUzZszw23VB/kQjSlWpJCiZWqMkIiIiIpUvJSWFefPm+bqMakkjSlXIxL27iGlqRElERERExJ8pKFUh0zi6u4im3omIiIiI+DUFpSpUMqKEqecoiYiIiIj4MwWlKmR6NnM4q3dkFxERERGp9hSUqlBJUEJrlERERERE/JqCUpUqeY6Sdr0TEREREfFnCkpVyDP1TiNKIiIiIjVOt27duPvuuz2vU1NTGTdu3EnfYxgGkydPPuPPrqj71CQKSlWoJCgZeo6SiIiISLUxYMAALr744uNemzNnDoZhsHLlSq/vu2jRIm655ZYzLa+UsWPH0rp16zLnd+/eTd++fSv0s441ceJEoqKiKvUzqpKCUhUycW8Pbmp7cBEREZFq46abbmL69Ons2LGjzLUJEybQrl07zj33XK/vGx8fT0hISEWUeEqJiYnY7fYq+ayzhYJSVTL0wFkRERGRUkwTio745jDLtxPxJZdcQnx8PBMnTix1Pjc3ly+//JKbbrqJgwcPcs0111C7dm1CQkJo2bIln3766Unve+zUu40bN9KlSxeCgoJo1qwZ06dPL/OeBx98kCZNmhASEkKDBg147LHHcDgcgHtE54knnmDFihUYhoFhGJ6aj516t2rVKnr06EFwcDCxsbHccsst5Obmeq4PHz6cyy67jJdeeomkpCRiY2MZMWKE57NOR0ZGBgMHDiQsLIyIiAiuvPJK9u7d67m+YsUKunfvTnh4OBEREbRt25bFixcDkJ6ezoABA4iOjiY0NJTmzZvz448/nnYt5RFQqXeXUjT1TkREROQYjjx4Ntk3n/3wLggMPWWzgIAArr/+eiZOnMgjjzyCcfQ/fn/55Zc4nU6uueYacnNzadu2LQ8++CARERH88MMPXHfddTRs2JAOHTqc8jNcLheXX345CQkJ/P7772RlZZVaz1QiPDyciRMnkpyczKpVq/jHP/5BeHg4DzzwAFdddRWrV6/mp59+YsaMGQBERkaWuceRI0fo06cPHTt2ZNGiRezbt4+bb76ZkSNHlgqDM2fOJCkpiZkzZ7Jp0yauuuoqWrduzT/+8Y9Tfp/jfb9BgwYRFhbGr7/+SnFxMSNGjOCqq65i1qxZAAwdOpQ2bdrw73//G6vVyvLly7HZbACMGDGCoqIiZs+eTWhoKGvWrCEsLMzrOryhoFSFTOPo1DuNKImIiIhUKzfeeCMvvvgiv/76K926dQPc0+4GDx5MZGQkkZGR3HfffZ72o0aNYurUqXzxxRflCkozZsxg3bp1TJ06leRkd3B89tlny6wrevTRRz0/p6amct999/HZZ5/xwAMPEBwcTFhYGAEBASQmJp7wsz755BMKCgr44IMPCA11B8U33niDAQMG8Pzzz5OQkABAdHQ0b7zxBlarlaZNm9K/f39+/vnn0wpKv/76K6tWrWLr1q2kpKQA8MEHH9C8eXMWLVpE+/btycjI4P7776dp06YANG7c2PP+jIwMBg8eTMuWLQFo0KCB1zV4S0GpKh39rw9ojZKIiIiImy3EPbLjq88up6ZNm9KpUyfee+89unXrxqZNm5gzZw5PPvkkAE6nk2effZYvvviCnTt3UlRURGFhYbnXIK1du5aUlBRPSALo2LFjmXaff/45r732Gps3byY3N5fi4mIiIiLK/T1KPqtVq1aekATQuXNnXC4X69ev9wSl5s2bY7VaPW2SkpJYtWqVV59VYsOGDaSkpHhCEkCzZs2Iiopi7dq1tG/fntGjR3PzzTfz4Ycf0rNnT6644goaNmwIwJ133sntt9/OtGnT6NmzJ4MHDz6tdWHe0BqlqnR0REkPnBURERE5yjDc0998cZT8R+xyuummm/j666/JyclhwoQJNGzYkK5duwLw4osv8uqrr/Lggw8yc+ZMli9fTp8+fSgqKqqwX9Vvv/3G0KFD6devH99//z3Lli3jkUceqdDP+KuSaW8lDMPAVYn/wX/s2LH88ccf9O/fn19++YVmzZoxadIkAG6++Wa2bNnCddddx6pVq2jXrh2vv/56pdUCCkpVqmSNkoKSiIiISPVz5ZVXYrFY+OSTT/jggw+48cYbPeuV5s2bx8CBA7n22mtp1aoVDRo0YMOGDeW+d1paGtu3b2f37t2ecwsWLCjVZv78+dSrV49HHnmEdu3a0bhxY9LT00u1CQwMxOk8+Xr4tLQ0VqxYwZEjRzzn5s2bh8Vi4Zxzzil3zd5o0qQJ27dvZ/v27Z5za9asITMzk2bNmpVqd8899zBt2jQuv/xyJkyY4LmWkpLCbbfdxjfffMO9997Lu+++Wym1llBQqlLu/yEZLm3mICIiIlLdhIWFcdVVVzFmzBh2797N8OHDPdcaN27M9OnTmT9/PmvXruXWW28ttaPbqfTs2ZMmTZowbNgwVqxYwZw5c3jkkUdKtWncuDEZGRl89tlnbN68mddee80z4lIiNTWVrVu3snz5cg4cOEBhYWGZzxo6dChBQUEMGzaM1atXM3PmTEaNGsV1113nmXZ3upxOJ8uXLy91rF27lm7dutGyZUuGDh3K0qVLWbhwIddffz1du3alXbt25OfnM3LkSGbNmkV6ejrz5s1j0aJFpKWlAXD33XczdepUtm7dytKlS5k5c6bnWmXxaVB67rnnaN++PeHh4dSqVYvLLruM9evXl2pTUFDAiBEjiI2NJSwsjMGDB3vV6fyJaTm6mQMaURIRERGpjm666SYOHz5Mnz59Sq0nevTRRznvvPPo06cP3bp1IzExkcsuu6zc97VYLEyaNIn8/Hw6dOjAzTffzDPPPFOqzaWXXso999zDyJEjad26NfPnz+exxx4r1Wbw4MFcfPHFdO/enfj4+ONuUR4SEsLUqVM5dOgQ7du35+9//zsXXXQRb7zxhne/jOPIzc2lTZs2pY6BAwdiGAaTJk0iOjqaLl260LNnTxo0aMDnn38OgNVq5eDBg1x//fU0adKEK6+8kr59+/LEE08A7gA2YsQI0tLSuPjii2nSpAlvvvnmGdd7MoZplnMD+Upw8cUXc/XVV9O+fXuKi4t5+OGHWb16NWvWrPEsLrv99tv54YcfmDhxIpGRkYwcORKLxcK8efPK9RnZ2dlERkaSlZXl9UK3ijZ//D/otP8Lfku6jo63nnlHlLOfw+Hgxx9/pF+/fmXmCYscj/qMeEt9Rrx1pn2moKCArVu3Ur9+fYKCgiqhQvE3LpeL7OxsIiIisFgqf5zmZH3Mm2zg013vfvrpp1KvJ06cSK1atViyZAldunQhKyuL//73v3zyySf06NEDcG/DmJaWxoIFC7jgggt8UfbpK3mOEj7LpiIiIiIiUg5+tT14VlYWADExMQAsWbIEh8NBz549PW2aNm1K3bp1+e23344blAoLC0vNxczOzgbc//XjTJ4kXBFcR9couZxOn9ci1UNJP1F/kfJSnxFvqc+It860zzgcDkzTxOVyVeoOauI/Siawlfy9VzaXy4VpmjgcjlLbm4N3/dZvgpLL5eLuu++mc+fOtGjRAoA9e/YQGBhIVFRUqbYJCQns2bPnuPd57rnnPHMZ/2ratGnl3se+stiycgDIyTrMjz/+6NNapHqZPn26r0uQakZ9RrylPiPeOt0+U/Iw1Nzc3Erb1lr8U05OTpV8TlFREfn5+cyePZvi4uJS1/Ly8sp9H78JSiNGjGD16tXMnTv3jO4zZswYRo8e7XmdnZ1NSkoKvXv39vkapd93TYe9EBEeTo9+/Xxai1QPDoeD6dOn06tXL60dkHJRnxFvqc+It860zxQUFLB9+3bCwsK0RqmGME2TnJwcwsPDPdupV6aCggKCg4Pp0qXLcdcolZdfBKWRI0fy/fffM3v2bOrUqeM5n5iYSFFREZmZmaVGlfbu3UtiYuJx72W327Hb7WXO22w2n/8DwDg69GfB5fNapHrxh/4r1Yv6jHhLfUa8dbp9xul0YhgGhmFUycJ+8b2S6XZV9Xde0r+O10e96bM+7Z2maTJy5EgmTZrEL7/8Qv369Utdb9u2LTabjZ9//tlzbv369WRkZNCxY8eqLvfMGSVzJDUfV0RERGqmkjUjmnYnlaVket2Z/scfn44ojRgxgk8++YT//e9/hIeHe9YdRUZGEhwcTGRkJDfddBOjR48mJiaGiIgIRo0aRceOHavfjnfg2fUOLVwUERGRGiogIICQkBD279+PzWbTqFIN4HK5KCoqoqCgoFL/vk3TJC8vj3379hEVFVVmIwdv+TQo/fvf/wagW7dupc5PmDDB86Tjf/3rX1gsFgYPHkxhYSF9+vSp9IdLVZqS7cFNBSURERGpmQzDICkpia1bt5Kenu7rcqQKmKZJfn4+wcHBVbJGKSoq6oTLdLzh06BUnmfdBgUFMX78eMaPH18FFVUyi6beiYiIiAQGBtK4cWNNv6shHA4Hs2fPpkuXLpW+FtJms53xSFIJv9jMocY4mqAN0+njQkRERER8y2KxaNe7GsJqtVJcXExQUFC12jRGk0KrUslmDuUYSRMREREREd9RUKpKWqMkIiIiIlItKChVIcMTlDT1TkRERETEnykoVSHToql3IiIiIiLVgYJSFdKIkoiIiIhI9aCgVJWOjigZaERJRERERMSfKShVpaMjSmgzBxERERERv6agVIU8U+/0wFkREREREb+moFSVLNoeXERERESkOlBQqkLG0QfOajMHERERERH/pqBUlUpGlLSZg4iIiIiIX1NQqkrazEFEREREpFpQUKpCJVPvLApKIiIiIiJ+TUGpKlm0652IiIiISHWgoFSFLCUPnNWIkoiIiIiIX1NQqkolu95pRElERERExK8pKFUhw/McJe16JyIiIiLizxSUqpBhaI2SiIiIiEh1oKBUlbRGSURERESkWlBQqkIlU+8sGlESEREREfFrCkpVyNCIkoiIiIhItaCgVIU8mzmgzRxERERERPyZglIVKtnMwYLTx5WIiIiIiMjJKChVIc/UO40oiYiIiIj4NQWlKmQcfeCsRWuURERERET8moJSFfpzjZKCkoiIiIiIP1NQqkKGVVPvRERERESqAwWlKuTZzEFT70RERERE/JqCUhWyWALcf2rqnYiIiIiIX1NQqkqWku3BFZRERERERPyZglIV0vbgIiIiIiLVg4JSFbJ6RpQUlERERERE/JmCUlXyBCWnjwsREREREZGTUVCqQiWbOWjqnYiIiIiIf1NQqkKWoyNKVm0PLiIiIiLi1xSUqpA2cxARERERqR4UlKpQSVDS9uAiIiIiIv5NQakKWfQcJRERERGRakFBqQoZ1pIRJU29ExERERHxZwpKVchydOpdgKERJRERERERf6agVIVKghKA6Sz2YSUiIiIiInIyCkpVyBpg9/zsLC7yYSUiIiIiInIyCkpVyGqzeX4udigoiYiIiIj4KwWlKmSzBXp+digoiYiIiIj4LQWlKhQQ8OeIklNBSURERETEb/k0KM2ePZsBAwaQnJyMYRhMnjy51PXc3FxGjhxJnTp1CA4OplmzZrz11lu+KbYCWK0Wikz3hg7FWqMkIiIiIuK3fBqUjhw5QqtWrRg/fvxxr48ePZqffvqJjz76iLVr13L33XczcuRIvv322yqutOIUEwBoRElERERExJ8F+PLD+/btS9++fU94ff78+QwbNoxu3boBcMstt/D222+zcOFCLr300iqqsmIV4x5RcjocPq5EREREREROxKdB6VQ6derEt99+y4033khycjKzZs1iw4YN/Otf/zrhewoLCyksLPS8zs7OBsDhcODwcThxOByeoFRYkO/zesT/lfQR9RUpL/UZ8Zb6jHhLfUa85U99xpsa/Doovf7669xyyy3UqVOHgIAALBYL7777Ll26dDnhe5577jmeeOKJMuenTZtGSEhIZZZbLh2PBqVFixawcvNuH1cj1cX06dN9XYJUM+oz4i31GfGW+ox4yx/6TF5eXrnb+n1QWrBgAd9++y316tVj9uzZjBgxguTkZHr27Hnc94wZM4bRo0d7XmdnZ5OSkkLv3r2JiIioqtKPy+FwkLnUHZRatWxBg1YX+rQe8X8Oh4Pp06fTq1cvbH95DpfIiajPiLfUZ8Rb6jPiLX/qMyWzzcrDb4NSfn4+Dz/8MJMmTaJ///4AnHvuuSxfvpyXXnrphEHJbrdjt9vLnLfZbD7/iwFwHv2VG7j8oh6pHvyl/0r1oT4j3lKfEW+pz4i3/KHPePP5fvscpZI1RRZL6RKtVisul8tHVZ25YsM9ouTS9uAiIiIiIn7LpyNKubm5bNq0yfN669atLF++nJiYGOrWrUvXrl25//77CQ4Opl69evz666988MEHvPLKKz6s+syUbObgKvb9YjYRERERETk+nwalxYsX0717d8/rkrVFw4YNY+LEiXz22WeMGTOGoUOHcujQIerVq8czzzzDbbfd5quSz5hTQUlERERExO/5NCh169YN0zRPeD0xMZEJEyZUYUWVzxOUnJp6JyIiIiLir/x2jdLZqmQzB40oiYiIiIj4LwWlKuY0NPVORERERMTfKShVsZKpd6ZTQUlERERExF8pKFWxkhElBSUREREREf+loFTFXJ4RJW3mICIiIiLirxSUqtifI0rFPq5EREREREROREGpipXseqcRJRERERER/6WgVMVcR0eU0IiSiIiIiIjfUlCqYp6g5NJmDiIiIiIi/kpBqYqVbOaAdr0TEREREfFbCkpV7M+pdwpKIiIiIiL+SkGpirmMgKM/aI2SiIiIiIi/UlCqYlqjJCIiIiLi/xSUqlhJUDI0oiQiIiIi4rcUlKqY6QlKGlESEREREfFXCkpVTCNKIiIiIiL+T0GpipmeNUoKSiIiIiIi/kpBqYqVBCWLpt6JiIiIiPgtBaUqpql3IiIiIiL+T0Gpqh19jpLF1IiSiIiIiIi/UlCqYk6LDQCrq8jHlYiIiIiIyIkoKFUx0xIIQICrwMeViIiIiIjIiSgoVTHTagfA5iz0cSUiIiIiInIiCkpVzeoeUbKZGlESEREREfFXCkpV7WhQCnRpRElERERExF8pKFW1AHdQCkIjSiIiIiIi/kpBqYpZAtxrlIJMjSiJiIiIiPgrBaUqZikZUTIcuJxOH1cjIiIiIiLHo6BUxUpGlAAK8nN9WImIiIiIiJyIglIVs1htnp/z8xSURERERET8kYJSFbNYLOSb7ul3hQpKIiIiIiJ+SUHJBwoM9/Q7BSUREREREf+koOQDRRwNSgUKSiIiIiIi/khByQcKLe6gVJx/xMeViIiIiIjI8Sgo+UCREQSAo0BBSURERETEHyko+UCxxR2UigsVlERERERE/JGCkg8UW91ByamgJCIiIiLilxSUfMB5NCgVa+qdiIiIiIhfUlDyAac90v1n/mEfVyIiIiIiIsejoOQDLnsUAIaCkoiIiIiIX1JQ8oWQGAACChWURERERET8kYKSD1hC3UEpsCjTt4WIiIiIiMhxKSj5gC0sFoBgR5aPKxERERERkeNRUPKB4Ig4AEKc2T6uREREREREjkdByQeCI+MBiDBzME3Tx9WIiIiIiMixFJR8ICzKHZQiySWvsNjH1YiIiIiIyLF8GpRmz57NgAEDSE5OxjAMJk+eXKbN2rVrufTSS4mMjCQ0NJT27duTkZFR9cVWoOBI99S7AMPFoUMHfFyNiIiIiIgcy6dB6ciRI7Rq1Yrx48cf9/rmzZv529/+RtOmTZk1axYrV67kscceIygoqIorrViGLZg83N/hwN4dPq5GRERERESOFeDLD+/bty99+/Y94fVHHnmEfv368cILL3jONWzYsCpKq3SHA2oRUpxB9t5tQHtflyMiIiIiIn/h06B0Mi6Xix9++IEHHniAPn36sGzZMurXr8+YMWO47LLLTvi+wsJCCgsLPa+zs907yzkcDhwOR2WXfVIln+9wOMgNSoTcDPL3bfN5XeK//tpnRMpDfUa8pT4j3lKfEW/5U5/xpgbD9JNt1wzDYNKkSZ4QtGfPHpKSkggJCeHpp5+me/fu/PTTTzz88MPMnDmTrl27Hvc+Y8eO5Yknnihz/pNPPiEkJKQyv4JX4ta+R+eCWXweeDlBzS/zdTkiIiIiIme9vLw8hgwZQlZWFhERESdt69cjSgADBw7knnvuAaB169bMnz+ft95664RBacyYMYwePdrzOjs7m5SUFHr37n3KX0ZlczgcTJ8+nV69erE1byGsn0WcJZcu/fr5tC7xX3/tMzabzdflSDWgPiPeUp8Rb6nPiLf8qc+UzDYrD78NSnFxcQQEBNCsWbNS59PS0pg7d+4J32e327Hb7WXO22w2n//FlLDZbITVSoX1EFq412/qEv/lT/1Xqgf1GfGW+ox4S31GvOUPfcabz/fb5ygFBgbSvn171q9fX+r8hg0bqFevno+qqjjxdRoBkOjcQ1ae7+drioiIiIjIn3w6opSbm8umTZs8r7du3cry5cuJiYmhbt263H///Vx11VV06dLFs0bpu+++Y9asWb4ruoKEJLtHylKMfazYfYDzGib5uCIRERERESnh0xGlxYsX06ZNG9q0aQPA6NGjadOmDY8//jgAgwYN4q233uKFF16gZcuW/Oc//+Hrr7/mb3/7my/LrhhhtThihGE1TPZuXe3rakRERERE5C98OqLUrVs3TrXp3o033siNN95YRRVVIcPgUEgqoUdWc2TnWqCXrysSEREREZGj/HaNUk3giG4MgOXg+lO0FBERERGRqqSg5EOBiU0BiMjd4uNKRERERETkrxSUfCi6XksAahdvJ6dAO9+JiIiIiPgLBSUfCq3t3vmugbGHjXsyfVuMiIiIiIh4KCj5UlRdioxA7IaDnVvX+boaERERERE5SkHJlyxWDgWnApCbscq3tYiIiIiIiIeCko8Vxrg3dLDsX+PjSkREREREpISCko/Z65wLQHTuxlM+U0pERERERKqGgpKPxdRvDUBDVzq7swp8W4yIiIiIiAAKSj4XmOweUUo19rBhx14fVyMiIiIiIqCg5HthtcixRmE1TPZtWenrakREREREBAUl3zMMssIbA1C0a7WPixERERERETiNoLR9+3Z27Njheb1w4ULuvvtu3nnnnQotrCYxE5oDEHxorY8rEREREREROI2gNGTIEGbOnAnAnj176NWrFwsXLuSRRx7hySefrPACa4KIuq0ASCzYQoHD6eNqRERERETE66C0evVqOnToAMAXX3xBixYtmD9/Ph9//DETJ06s6PpqhIjU1gCcY2SwaV+ub4sRERERERHvg5LD4cButwMwY8YMLr30UgCaNm3K7t27K7a6GsKIb4oLC3FGNpu3bfF1OSIiIiIiNZ7XQal58+a89dZbzJkzh+nTp3PxxRcDsGvXLmJjYyu8wBohMITD9joAZG9d5uNiRERERETE66D0/PPP8/bbb9OtWzeuueYaWrVyr6/59ttvPVPyxHsFsWkAmHu0852IiIiIiK8FePuGbt26ceDAAbKzs4mOjvacv+WWWwgJCanQ4mqSoJQ2sGsqMTnrcLpMrBbD1yWJiIiIiNRYXo8o5efnU1hY6AlJ6enpjBs3jvXr11OrVq0KL7CmiGrYHoA0cwtbDxzxcTUiIiIiIjWb10Fp4MCBfPDBBwBkZmZy/vnn8/LLL3PZZZfx73//u8ILrCmsye4pjPWNPazL2OXjakREREREajavg9LSpUu58MILAfjqq69ISEggPT2dDz74gNdee63CC6wxwuLJtsVhMUwOblri62pERERERGo0r4NSXl4e4eHhAEybNo3LL78ci8XCBRdcQHp6eoUXWJPkRjcHwLV7pY8rERERERGp2bwOSo0aNWLy5Mls376dqVOn0rt3bwD27dtHREREhRdYk9jqtAYgMmstpmn6thgRERERkRrM66D0+OOPc99995GamkqHDh3o2LEj4B5datOmTYUXWJNEH93Q4RzXFjIO5fm4GhERERGRmsvr7cH//ve/87e//Y3du3d7nqEEcNFFFzFo0KAKLa6mCajt/n02Nnbwc8YB6sWG+rgiEREREZGayeugBJCYmEhiYiI7duwAoE6dOnrYbEWITCHPGk6IM4e9W5ZBm3q+rkhEREREpEbyeuqdy+XiySefJDIyknr16lGvXj2ioqJ46qmncLlclVFjzWEYZEc1A6B4x3Lf1iIiIiIiUoN5PaL0yCOP8N///pf/+7//o3PnzgDMnTuXsWPHUlBQwDPPPFPhRdYk1uRWcPB3wjPdGzoYhuHrkkREREREahyvg9L777/Pf/7zHy699FLPuXPPPZfatWtzxx13KCidociG7WHVOzRybmZXVgG1o4J9XZKIiIiISI3j9dS7Q4cO0bRp0zLnmzZtyqFDhyqkqJossHZrANKMDFZkHPRtMSIiIiIiNZTXQalVq1a88cYbZc6/8cYbpXbBk9MU25BCSzAhRiE7NurBsyIiIiIivuD11LsXXniB/v37M2PGDM8zlH777Te2b9/Ojz/+WOEF1jgWK9lRzYg/tARHxmKgn68rEhERERGpcbweUeratSsbNmxg0KBBZGZmkpmZyeWXX8769eu58MILK6PGGicgpR0AUYdXUezUToIiIiIiIlXttJ6jlJycXGbThh07dnDLLbfwzjvvVEhhNVlk446w4m1asIl1e3JoUTvS1yWJiIiIiNQoXo8oncjBgwf573//W1G3q9EsddoCkGaks3LbXh9XIyIiIiJS81RYUJIKFJnCEVsMgYaTA5sW+7oaEREREZEaR0HJHxkGefHuHQQtu5b6uBgRERERkZpHQclPhdbvAEDtvDVk5Tt8XI2IiIiISM1S7s0cLr/88pNez8zMPNNa5C9C6p8P86CVsZmVOzK5sHG8r0sSEREREakxyh2UIiNPvvNaZGQk119//RkXJEcltwGggWUPMzanKyiJiIiIiFShcgelCRMmVGYdcqyQGLJC6hKZl0HO1oVAO19XJCIiIiJSY5zRGqVPP/2UI0eOVFQtcgxn0nkABO9bjmmaPq5GRERERKTmOKOgdOutt7J3r57zU1kiGp4PQJPiDWw5oEAqIiIiIlJVzigoaZSjcgXUde9819qymUVbDvq4GhERERGRmsOn24PPnj2bAQMGkJycjGEYTJ48+YRtb7vtNgzDYNy4cVVWn88ltqTYsBFnZLNlwypfVyMiIiIiUmOcUVCaMmUKycnJp/3+I0eO0KpVK8aPH3/SdpMmTWLBggVn9FnVUoCdI3HnAmBmLPBxMSIiIiIiNUe5d707nr/97W9n9OF9+/alb9++J22zc+dORo0axdSpU+nfv/8ZfV51FNywM+xfQsP8VezKzCc5KtjXJYmIiIiInPW8Dkpt2rTBMIwy5w3DICgoiEaNGjF8+HC6d+9+xsW5XC6uu+467r//fpo3b16u9xQWFlJYWOh5nZ2dDYDD4cDhcJxxTWei5PO9qcNS93xYAO0t6/lt034ubZVUWeWJHzqdPiM1m/qMeEt9RrylPiPe8qc+400NXgeliy++mH//+9+0bNmSDh3cmw0sWrSIlStXMnz4cNasWUPPnj355ptvGDhwoLe3L+X5558nICCAO++8s9zvee6553jiiSfKnJ82bRohISFnVE9FmT59ernb2opz6Qc0tOzmxV/nELAzrPIKE7/lTZ8RAfUZ8Z76jHhLfUa85Q99Ji8vr9xtvQ5KBw4c4N577+Wxxx4rdf7pp58mPT2dadOm8c9//pOnnnrqjILSkiVLePXVV1m6dOlxR7BOZMyYMYwePdrzOjs7m5SUFHr37k1ERMRp11MRHA4H06dPp1evXthstnK/L2fbK4TnbKKWYwf9+o0+9RvkrHG6fUZqLvUZ8Zb6jHhLfUa85U99pmS2WXl4HZS++OILlixZUub81VdfTdu2bXn33Xe55ppreOWVV7y9dSlz5sxh37591K1b13PO6XRy7733Mm7cOLZt23bc99ntdux2e5nzNpvN538xJbytxdmgM6zYRO3cFeQWmUSHBlZideKP/Kn/SvWgPiPeUp8Rb6nPiLf8oc948/le73oXFBTE/Pnzy5yfP38+QUFBgHttUcnPp+u6665j5cqVLF++3HMkJydz//33M3Xq1DO6d3UT1KAz4F6ntGjbIR9XIyIiIiJy9vN6RGnUqFHcdtttLFmyhPbt2wPuNUr/+c9/ePjhhwGYOnUqrVu3PuW9cnNz2bRpk+f11q1bWb58OTExMdStW5fY2NhS7W02G4mJiZxzzjnell291T0fgBbGVl7ZtIvezRN9XJCIiIiIyNnN66D06KOPUr9+fd544w0+/PBDAM455xzeffddhgwZArgfDnv77bef8l6LFy8utTteydqiYcOGMXHiRG9LO3tF1SM/qBbBBfvI2vw7cJ6vKxIREREROaud1nOUhg4dytChQ094PTi4fM/66datG6ZplvtzT7Qu6axnGBh1L4AN3xJ7aCmZeUVEhWidkoiIiIhIZTntB84uWbKEtWvXAtC8eXPatGlTYUVJWUEN/wYbvqW9sY7fNh+kb0s9T0lEREREpLJ4HZT27dvH1VdfzaxZs4iKigIgMzOT7t2789lnnxEfH1/RNQpAvU4AtLOs5/827lFQEhERERGpRF7vejdq1ChycnL4448/OHToEIcOHWL16tVkZ2d79WBY8VKt5hQFRhFqFHJ4w2++rkZERERE5KzmdVD66aefePPNN0lLS/Oca9asGePHj2fKlCkVWpz8hcWCpUEXAOrnLGH7ofI/VVhERERERLzjdVByuVzHfVCTzWbD5XJVSFFyfAENuwHQ2foH8zYd8G0xIiIiIiJnMa+DUo8ePbjrrrvYtWuX59zOnTu55557uOiiiyq0ODlGg24AtDE28vuGHb6tRURERETkLOZ1UHrjjTfIzs4mNTWVhg0b0rBhQ+rXr092djavvfZaZdQoJWIaUBSShN0opmDLfFyu8m+tLiIiIiIi5ef1rncpKSksXbqUGTNmsG7dOgDS0tLo2bNnhRcnxzAMAhp1g5Wfcm7RctbszqZF7UhfVyUiIiIictY5recoGYZBr1696NWrl+fcunXruPTSS9mwYUOFFSdlWRp0hZWf0tHiXqekoCQiIiIiUvG8nnp3IoWFhWzevLmibicnUt+9811LYysL1271cTEiIiIiImenCgtKUkUia+OIaoDVMLFtn092gcPXFYmIiIiInHUUlKohW6PuAFxgrGLOBm0TLiIiIiJS0RSUqqOG7qDU1bKCn9ft9XExIiIiIiJnn3Jv5hAdHY1hGCe8XlxcXCEFSTnU74rLYqM+e9mybiVOVyuslhP/3YiIiIiIiHfKHZTGjRtXiWWIV4IioO4FsG0OrQsXsWLHAM6rG+3rqkREREREzhrlDkrDhg2rzDrES5bGvWHbHLpblvPL2n0KSiIiIiIiFUhrlKqrxr0BuMCylrlrM3xcjIiIiIjI2UVBqbqKPwdnRB3shoPofb+zKzPf1xWJiIiIiJw1FJSqK8PA2sQ9qtTdspyf12r3OxERERGRiqKgVJ01/jMo/bR6t4+LERERERE5eygoVWf1u2BaAkmx7Gff1tUczC30dUUiIiIiImeFcu96V8LpdDJx4kR+/vln9u3bh8vlKnX9l19+qbDi5BQCQzFSO8OWmXQzljJ9TR+u7lDX11WJiIiIiFR7Xgelu+66i4kTJ9K/f39atGhx0ofQShU4px9smUlv62JeX71HQUlEREREpAJ4HZQ+++wzvvjiC/r161cZ9Yi3mvaHKffT1tjIhk2byMxrTVRIoK+rEhERERGp1rxeoxQYGEijRo0qoxY5HZG1oXZbLIZJD2Mx09Zo9zsRERERkTPldVC69957efXVVzFNszLqkdORNgCAiy0LmbJKu9+JiIiIiJwpr6fezZ07l5kzZzJlyhSaN2+OzWYrdf2bb76psOKknJoOgBlj6WhZwz2btpGV34bIYNup3yciIiIiIsfldVCKiopi0KBBlVGLnK64RhCfhm3/Wi40lzH1j45c2S7F11WJiIiIiFRbXgelCRMmVEYdcqbSLoH9a7nYuoj3l+5UUBIREREROQN64OzZ4ug6pa6WFSzfuotdmfk+LkhEREREpPryekQJ4KuvvuKLL74gIyODoqKiUteWLl1aIYWJlxLPhai6BGdm0M1YzuTl53JHN+1OKCIiIiJyOrweUXrttde44YYbSEhIYNmyZXTo0IHY2Fi2bNlC3759K6NGKQ/DgObutWOXWuczaelO7UwoIiIiInKavA5Kb775Ju+88w6vv/46gYGBPPDAA0yfPp0777yTrKysyqhRyqvlFQD0sCxnz759/LEr28cFiYiIiIhUT14HpYyMDDp16gRAcHAwOTk5AFx33XV8+umnFVudeCehBcQ3xW446GNdxKRlO31dkYiIiIhIteR1UEpMTOTQoUMA1K1blwULFgCwdetWTfXyNcOAln8H4FLLfP63fBfFTpePixIRERERqX68Dko9evTg22+/BeCGG27gnnvuoVevXlx11VV6vpI/aOEOSp2tf2Dk7mXm+v0+LkhEREREpPrxete7d955B5fLPUoxYsQIYmNjmT9/Ppdeeim33nprhRcoXoqpD3XaY92xiP7WBXzye2N6NUvwdVUiIiIiItWK10HJYrFgsfw5EHX11Vdz9dVXV2hRcoZaXgE7FjHQOp/LN1zMzsx8akcF+7oqEREREZFq47QeODtnzhyuvfZaOnbsyM6d7g0DPvzwQ+bOnVuhxclpaj4IDAttLJtIZTefL9ru64pERERERKoVr4PS119/TZ8+fQgODmbZsmUUFhYCkJWVxbPPPlvhBcppCKsFjXoCcKV1Fl8s2q5NHUREREREvOB1UHr66ad56623ePfdd7HZbJ7znTt3ZunSpRVanJyBNtcB8PeAOezPPqJNHUREREREvOB1UFq/fj1dunQpcz4yMpLMzMyKqEkqQpOLITSeeDLpblnOJ7+n+7oiEREREZFq47Seo7Rp06Yy5+fOnUuDBg0qpCipAAGB0Mq9ycZV1lnM2rCfbQeO+LYmEREREZFqwuug9I9//IO77rqL33//HcMw2LVrFx9//DH33Xcft99+e2XUKKerzfUA9LAuI848zMT523xbj4iIiIhINeH19uAPPfQQLpeLiy66iLy8PLp06YLdbue+++5j1KhRlVGjnK74JpByPtbtvzPYOocPF8cxuncTIoJsp36viIiIiEgN5vWIkmEYPPLIIxw6dIjVq1ezYMEC9u/fz1NPPVUZ9cmZOrqpw3X2X8krcvCFtgoXERERETml03qOEkBgYCDNmjWjQ4cOhIWFndY9Zs+ezYABA0hOTsYwDCZPnuy55nA4ePDBB2nZsiWhoaEkJydz/fXXs2vXrtMtuWZqPgjskdR27aaLZRUT52/D6TJ9XZWIiIiIiF8r99S7G2+8sVzt3nvvvXJ/+JEjR2jVqhU33ngjl19+ealreXl5LF26lMcee4xWrVpx+PBh7rrrLi699FIWL15c7s+o8exh0GYoLHiTmwOncd3hVkxfs4eLWyT5ujIREREREb9V7qA0ceJE6tWrR5s2bTDNihmR6Nu3L3379j3utcjISKZPn17q3BtvvEGHDh3IyMigbt26FVJDjdD+ZljwJn9jOfWMPbwzewt9midiGIavKxMRERER8UvlDkq33347n376KVu3buWGG27g2muvJSYmpjJrKyMrKwvDMIiKijphm8LCQgoLCz2vs7OzAfdUPofDUdklnlTJ51d5HRF1sTbsiWXzDIbbZvBERiLzNu7j/PpV+/cn3vNZn5FqS31GvKU+I95SnxFv+VOf8aYGw/RieKiwsJBvvvmG9957j/nz59O/f39uuukmevfufcajE4ZhMGnSJC677LLjXi8oKKBz5840bdqUjz/++IT3GTt2LE888USZ85988gkhISFnVGN1VitrBR23vMwRQmhf8AYpkYHc0czl67JERERERKpMXl4eQ4YMISsri4iIiJO29Soo/VV6ejoTJ07kgw8+oLi4mD/++OO0N3WAkwclh8PB4MGD2bFjB7NmzTrplzreiFJKSgoHDhw45S+jsjkcDqZPn06vXr2w2ap4i27TRcBbF2Ac2sKjxTfyUXFPvr71fM6tE1m1dYhXfNpnpFpSnxFvqc+It9RnxFv+1Geys7OJi4srV1Dy+jlKJSwWC4ZhYJomTqfzdG9zSg6HgyuvvJL09HR++eWXU34hu92O3W4vc95ms/n8L6aEz2rpcAv89BCjQmbwcXYP3p6zjXeub1f1dYjX/Kn/SvWgPiPeUp8Rb6nPiLf8oc948/lebQ9eWFjIp59+Sq9evWjSpAmrVq3ijTfeICMj44xGk06kJCRt3LiRGTNmEBsbW+GfUaO0uRaCIkkoyqC3dQnT1uxlw94cX1clIiIiIuJ3yh2U7rjjDpKSkvi///s/LrnkErZv386XX35Jv379sFhO73FMubm5LF++nOXLlwOwdetWli9fTkZGBg6Hg7///e8sXryYjz/+GKfTyZ49e9izZw9FRUWn9Xk1nj3cvQMe8FDYT4DJaz9v9G1NIiIiIiJ+qNxT79566y3q1q1LgwYN+PXXX/n111+P2+6bb74p94cvXryY7t27e16PHj0agGHDhjF27Fi+/fZbAFq3bl3qfTNnzqRbt27l/hz5i/Nvg/lvUL9wLecb6/h+pcEd3bJpluzb9VsiIiIiIv6k3EHp+uuvr/Dn7nTr1u2kz2SqqOc1yV+E1XI/gHbxezwePY3+h9J4Zfp6/jOsva8rExERERHxG149cFbOEp1GwZKJNM/7nWaWDGashaUZhzmvbrSvKxMRERER8Qunt7hIqreYBtDsMgCejZsKwMvT1vuwIBERERER/6KgVFNdeC8ArbJn0cy6g3mbDjJv0wEfFyUiIiIi4h8UlGqqxBbQ7DIMTF6MnwLAMz+sxenSujAREREREQWlmqzbQ4BB88yZtA3awZrd2XyzdIevqxIRERER8TkFpZqsVhq0uByAl+Lco0ovTl1PXlGxL6sSEREREfE5BaWaruuDgEH9AzO5KGoX+3IKefvXLb6uSkRERETEpxSUarr4c6DlFQA8H/41YPL27M3sysz3bV0iIiIiIj6koCTQ4xGwBhK3/zf+kbSVAoeLp75f4+uqRERERER8RkFJIDoVOtwCwL2Wj7FZTKas3sPM9ft8W5eIiIiIiI8oKInbhfdCUCRBB9fy8jlrAfjn//6gwOH0cWEiIiIiIlVPQUncQmI8D6G95MB71As3yDiUx5uzNvu4MBERERGRqqegJH/qcCtEpmDJ2cV/Gs0B4K1Zm9l64IiPCxMRERERqVoKSvInWxD0fhqARhv+y9/rOyhyunhk0ipM0/RxcSIiIiIiVUdBSUprNhAadMNwFvJk0EcE2SzM33yQTxZm+LoyEREREZEqo6AkpRkG9H0BLAGEbJ3Oa23dO989+8NadhzO83FxIiIiIiJVQ0FJyoo/By64A4Be6f+iY91QjhQ5GfONpuCJiIiISM2goCTH1/UBCE/COLyV8alzsAdYmLPxAJ8v2u7rykREREREKp2CkhyfPdyzsUPMktd4qnMgAE9rCp6IiIiI1AAKSnJiLQZDo17gLOKKnc/TNiWC3MJi7vl8OU6XpuCJiIiIyNlLQUlOzDDgkn9BYBjGjt95N20FYfYAFm07zPiZm3xdnYiIiIhIpVFQkpOLSoGeYwGI+e1ZXuodDcCrP29kSfphHxYmIiIiIlJ5FJTk1NrdBHU7guMIF2/5Pwa2SsLpMrnrs2VkFzh8XZ2IiIiISIVTUJJTs1jg0tfBaofNP/N/9ZdSJzqYHYfzeWTSam0ZLiIiIiJnHQUlKZ+4xnDRYwAE//wYb/WNxGox+G7FLj5ckO7j4kREREREKpaCkpTfBSMg9UJw5NHi9/t5uE9DAJ76fg3LMrReSURERETOHgpKUn4WCwx6C4IiYecSbnR+Rd8WiTicJiM+XsqhI0W+rlBEREREpEIoKIl3Iuu4twwHjDkv8VLHQurHhbIrq4C7Plum5yuJiIiIyFlBQUm812IwnHsVmC5Cv72Vt69oQJDNwpyNB3hh6jpfVyciIiIicsYUlOT09HsRolMhK4Mm8x/i+ctbAvD2r1v4eskO39YmIiIiInKGFJTk9ARFwhXvgzUQ1v/AwPzJjOzeCIAx36xiSfohHxcoIiIiInL6FJTk9CW3houfc/8845+MbppJn+YJFDld3PrhEnZm5vu0PBERERGR06WgJGem3U3Q/HJwFWP5+gZeuSSFtKQIDuQWcfP7izlSWOzrCkVEREREvKagJGfGMODS1yC2EWTvJPR/N/Gfa1sRFxbI2t3ZjPp0GcVOl6+rFBERERHxioKSnDl7OFz1EQSGwbY51F7wBO9c3w57gIVf1u3jkUmrMU1tGy4iIiIi1YeCklSMWmlw+buAAYv+w3n7JvPGkPOwGPD54u38a8ZGX1coIiIiIlJuCkpScZr2gx6PuH/+8T56hW7m6cvc24a/9vNGPv493YfFiYiIiIiUn4KSVKwL74Pmg8BVDJ9fx5Bz4M6LGgPw2OTVTP1jj48LFBERERE5NQUlqViGAQPHQ2JLyDsAH1/BPZ3jubp9Ci4TRn2yjNkb9vu6ShERERGRk1JQkooXGArXfAbhybB/HcZnQ3j6kkb0bZFIkdPFLR8uZsGWg76uUkRERETkhBSUpHJE1oFrvwJ7BGTMJ+B/t/HqVa3ofk48BQ4XN01cxNKMw76uUkRERETkuBSUpPIkNIerPwZrIKz5H4EzHuHfQ8+jc6NYjhQ5GfbeQlbvzPJ1lSIiIiIiZSgoSeWq3wUu+7f759/fImjRm7x7fTvap0aTU1DMtf/9XWFJRERERPyOgpJUvpZ/h97PuH+e/hgha7/iveHtaZ0SRWaegyHvLmCZpuGJiIiIiB9RUJKq0WkkXDDC/fPkOwjfMoUPb+pA+9RosguKue6/C1m07ZBvaxQREREROUpBSapO76eh9bVgOuGrGwnf/ivv39iBTg1jyS0s5vr/LmT+pgO+rlJERERExLdBafbs2QwYMIDk5GQMw2Dy5MmlrpumyeOPP05SUhLBwcH07NmTjRs3+qZYOXMWC1z6GjS/HFwO+HwoITt/473h7enaJJ58h5PhExfx02o9lFZEREREfMunQenIkSO0atWK8ePHH/f6Cy+8wGuvvcZbb73F77//TmhoKH369KGgoKCKK5UKY7HC5e9Ak4uhuAA+uYqg3Yt45/q29G6WQFGxi9s/XsLEeVt9XamIiIiI1GA+DUp9+/bl6aefZtCgQWWumabJuHHjePTRRxk4cCDnnnsuH3zwAbt27Soz8iTVjNUGV7wPDbqD4wh89Hfsu5fw5tDzGHp+XUwTxn63hmd/XIvLZfq6WhERERGpgQJ8XcCJbN26lT179tCzZ0/PucjISM4//3x+++03rr766uO+r7CwkMLCQs/r7OxsABwOBw6Ho3KLPoWSz/d1Hf7BCn9/H+tnV2PJmI/5wUD4+wf8s39XkiLsvDR9I+/M3sLOQ3k8f3lz7Darrwv2CfUZ8Zb6jHhLfUa8pT4j3vKnPuNNDX4blPbsca9TSUhIKHU+ISHBc+14nnvuOZ544oky56dNm0ZISEjFFnmapk+f7usS/IY16gY6HM6mVs5qLJ9dxeL6d5IS2YZrGxl8utnCD6v3sGLrbm5o4iQuyNfV+o76jHhLfUa8pT4j3lKfEW/5Q5/Jy8srd1u/DUqna8yYMYwePdrzOjs7m5SUFHr37k1ERIQPK3Mn2OnTp9OrVy9sNptPa/Erxf1w/e92LOu+5fz0N3FeMo5+/a6g95aD3PX5SnYccfCvNUE8e1kz+rZI9HW1VUp9RrylPiPeUp8Rb6nPiLf8qc+UzDYrD78NSomJ7n8h3rt3L0lJSZ7ze/fupXXr1id8n91ux263lzlvs9l8/hdTwp9q8Qs2G1zxHnw5HGPd9wT87w4ozqNLu5v48a4LGfXJMhanH+bOz1dy4dJdPH5JMxonhPu66iqlPiPeUp8Rb6nPiLfUZ8Rb/tBnvPl8v32OUv369UlMTOTnn3/2nMvOzub333+nY8eOPqxMKoXVBld+CO1uBEz44V74+QmSwgP59JYLuL1bQwKtFuZsPECvf83m5vcXs2J7pq+rFhEREZGzlE+DUm5uLsuXL2f58uWAewOH5cuXk5GRgWEY3H333Tz99NN8++23rFq1iuuvv57k5GQuu+wyX5YtlcVigX4vQ49H3a/n/gs+vRqbq5AHL27K93f+jQ6pMQDMWLuXgePnMXD8PLbsz8U0tTueiIiIiFQcnwalxYsX06ZNG9q0aQPA6NGjadOmDY8//jgADzzwAKNGjeKWW26hffv25Obm8tNPPxEUVINX9Z/tLBbocj/0fwWsdtg4Dd5oD/s30CQhnC9u68gbQ9oQGeweNl2xPZMeL//K/V+tZE+Wnq8lIiIiIhXDp0GpW7dumKZZ5pg4cSIAhmHw5JNPsmfPHgoKCpgxYwZNmjTxZclSVdrfBFd9BMExkLUdxreHX56BwhwuOTeZuQ925/qO9TzNv1qygwue+5kZa/ZqdElEREREzpjfrlESoUlvuHU2xDR0v579AjxXB357k3AbPDmwBb/c25W0pD93M7z5g8Xc9+VKDuYWnuCmIiIiIiKnpqAk/i0qBUYtgdZD/zw3dQw8FQc/jaGBcyuTR3TinwOaeS5/vXQHvf81m9U7s3xQsIiIiIicDRSUxP8ZBlz2Jtz8c+nzC96Et/6G/dMruOEcJzPv60ZihHv92sEjRVzy+lzGz9xEYbHTB0WLiIiISHWmoCTVR5128M9MuGBE6fObf4Y32lJ/+Uv8dNffGPaXtUsvTl3P1e8sYOuBI1Vbq4iIiIhUawpKUr0YBlz8LNw+H0Jrlb429xWixqcxtrOd/w5r5zm9LCOT7i/N4qslO3C5tNGDiIiIiJyagpJUTwnN4b4N0Oe50ufzDmK80Y6LDn3GnAe6c0GDGM+l+75cwXXv/c72Q3lVXKyIiIiIVDcKSlJ9GQZ0vANumwfRqaWvTX+clMmD+WBIUx6/5M+NHuZtOsiFL8xk8rKd2kZcRERERE5IQUmqv8QWMHIxdH2o9PmM+QS+lMqN0Sv45d6uNIgP9Vy6+/PlXPrGPI0uiYiIiMhxKSjJ2cFqg+5jYOhXZa99OYwGs0by3R0XMKZvU8/pVTuzuPCFmbw/f5tGl0RERESkFAUlObs07gX3bYLGfUqf/2MSoc8ncGuDg/x6fzfiw+2eS//89g/aPT2Dzftzq7hYEREREfFXCkpy9gmLh6FfwCX/Knvtv72oN/cB5tzfjbF/eUjtwSNFXPTyr4z99o8qLFRERERE/JWCkpy92t3o3kb82I0eln1E0LOxDE89xMIxfz6kFmDi/G2kPvQDv20+iFNbiYuIiIjUWApKcnZLaA6jlkLPsWWvvduDWjPuYsGIpjzSL63UpWveXcA17y5g3Z7sqqlTRERERPyKgpKc/SxW+Ns9cOM0iKpb+tqqL2FcS/4ROoffH+hMi9oRnksLtx7i4nFzeOjrlRzILaziokVERETElxSUpOaoez6MWAQ9Hi193nTCd3eS8Ekvvh9g8No1bUpd/mzRdto9PYMJ87ZWYbEiIiIi4ksKSlKz2IKgy/1wyyxIOb/0tQMbYGJ/Lt32DCse7MhtXRuWuvzEd2tIfegHVmzP1HbiIiIiImc5BSWpmZLbwHWT4bK3yl5b9hGRr9bnoVoL+OXervRMSyh1eeD4eQz+93yWb8+sklJFREREpOopKEnNFRgCra+BezdA88vLXv/uLhp81JF3uxcz9c5OBNn+/J/L0oxMLhs/j2veWaDnL4mIiIichRSURMIT4IoJ7q3ELbbS17IyMN7rwznTrmPdLbE8fVmLUpd/23KQi17+lQe+WkF+kbMKixYRERGRyqSgJFIioTk8fgC6PlT22rY58N9eXLv9n6x6sB23dm1Q6vIXi3eQ9vhPPPndGjIO5lVRwSIiIiJSWRSURI7VfQzctRIa9Sp77Y9JhL/ahDEBn/HbXa0Y3im11OX35m2ly4szeXTyKo0wiYiIiFRjCkoixxNdD679yr3hQ61mZa/PG0fS280ZGzaZlfe2ods58aUuf7Qgg7THf+K5KWvZfkgjTCIiIiLVjYKSyMk07A43/wxDvjz+9dkvEDE+jYn1f2HyHZ1oWy+61OW3f93ChS/M5K7PlrE3u6AKChYRERGRiqCgJHIqgSHQpDeMzYKLHj9+m1nP0fq9VL5qvYwZNzeic6PYUpf/t3wX5z/7M3d9toy1u7OroGgRERERORMKSiLeuPBeuH8LtLvxuJeNqQ/T6KMOfJw6lR/u6EBaUkSp6/9bvou+r87h4nGzWbztUFVULCIiIiKnQUFJxFuhsXDJv2DkEjjv+uO3mfsKzd9rxJRm0/lhaCLdj1nDtG5PDn9/6zd6vDyL71fuosChjR9ERERE/ImCksjpimsEl74OD2VA2qXHbzPvVZp/3YMJIa+xeFRamcC0Zf8RRn6yjKaP/cTbv27WTnkiIiIifkJBSeRMBUXCVR/CP36BJn2P32btd8S924YJPMHcv5tcf0HdMk2em7KOtMd/4m6tYxIRERHxOQUlkYpSuy1c8ymMWgppA47fJn0udb4fypN/9GbzkALu7pJcpsnko+uYBo6fx8/r9qFBJhEREZGqp6AkUpEMA2IbwlUfwcjF0PSS47dz5GH95kbuXtKbdRfOZfyltQm2WUs1WbE9k9s+Xs79CwN47ZdNHMgtrIIvICIiIiKgoCRSeeIaw9Ufu0eYOtxy/DbOIoIWvUn/aV1ZmzaBby520Dg+tEyz12duod3TMxg4fh7zNh2o5MJFREREREFJpLLFNoR+L8KYndD5rhO32/AT580axrTcQSztt4ObWpcNTCu2ZzL0P7/T+JEfeXHqOnZl5ldi4SIiIiI1l4KSSFWxh0GvJ+H+zXDZWydsZpguYn55gMfWDWRd8w+4o04G4UGlp+U5nCbjZ26m0//9whVvzed/y3dSVOyq7G8gIiIiUmMoKIlUtdA4aH0NjM2Cv78HCS1O2DRo8088cOAhVgbfzqwLltGpbnCZNou2Heauz5bT5NEpPDJplR5kKyIiIlIBAnxdgEiN1mIwNLsMcvbA9Mdg9dfHbWbkHyJ1+Yt8AjjSujPVfjEjl6eUaffx7xl8/HsGAI/2T6N701o0jA+rxC8gIiIicnbSiJKIr1msEFnbPbp01wq4+P9O2ty2dSaXrHuQbUFD+O28X7gp9fibOzz9w1ouevlXLnzhF/63fCd7sgoqo3oRERGRs5JGlET8SXQqXHC7+/hjEq55r2PZteSEzZPW/IfHgIejY1hc7xbGrk1mbWFcqTbbD+Vz12fLAejYIJabL6xP23rRRIUEVt73EBEREanmFJRE/FXzQTgb92fK999wcdBSrAvfPmFTa/4hzl/3f0wxIDelDT/a+/HCpjocILJUu9+2HOS3LQcB6N0sgcFt69ClcTzBgdbj3VZERESkxlJQEvFnhkGxNRhXr2ewdn8Ydq9wr2XaveKEbwnbv4wrWcaVQbAvviOfF3Xmzb3NyCeoVLtpa/Yybc1eAHqmJXBp62T6tUgkwKoZuSIiIiIKSiLVRXAUNOgKt86GfWth3ffwy9MnfUut/b8xit8YFQQ7Y85nYm4H/pt9Pq5jlifOWLuXGWv3cifQuVEsA1vX5u/n1cFiMSrv+4iIiIj4MQUlkeqoVpr76HI/bJgGyz+GNZNP+pbah37nEX7nkaDX2R5xHu8f6ciEIxfgxAL8GYjmbTrIvE0HeeCrlTSuFcbwzql0P6cWyVFltyYXEREROVspKIlUd016u4/iIndgWvxf2LPqpG9JyV7Koyzl0aDx7A1pzNv5F/FTfjN2UXojiI37cnlk0moA7AEWRvVoRKdGcZxXN7rSvo6IiIiIP1BQEjlbBARCuxvcR85eWPo+LPsQMjNO+raEvI08zkYeD4K8gCheL+jLXGdzVpkNSrUrLHbx0rQNMG0DAJe3qc3FLRLpdk4tAgO0rklERETOLgpKImej8ATo+oD72LcWln/iDk4FWSd9W0hxJg8GfMqDR/+f4WNnT6Y62zLH1RLzmHVN3yzbyTfLdgJQLzaEGzql0i41hha1I4+9rYiIiEi1o6AkcrarlQa9n3IfGQvgj0nw+1vleutQ6wyGWmcAsIrGvO/owTJXIzabtUu1Sz+Yx9jv1nheD2iVTM+0WnRvWotwewCGoU0hREREpHpRUBKpSepe4D76Pg9bZsG6H2DhO+V6a0s28pJtIwBOLHxQ3ItfXecy19WS4mP+r+S7Fbv4bsUuAGxWgxs616dDagw9mtbSTnoiIiJSLfh1UHI6nYwdO5aPPvqIPXv2kJyczPDhw3n00Uf1X6hFzlSDbu6j34uwdTas/wkWjC/XW624uCFgKjcwFYAtrkQ+c3Znqasxi82mpdo6nCbvzN7CO7O3AJAcGcTl59Wh2znxtKgdSZBND7sVERER/+PXQen555/n3//+N++//z7Nmzdn8eLF3HDDDURGRnLnnXf6ujyRs0f9Lu6j99OwZ6V7pGnBm1CUW663N7Ds4WHLp57XC1xp/ORsz1xXC7aaSTj5MwztyirgjZmbeGPmJvd740PplZbAgFbJpESHEBliq9jvJiIiInIa/DoozZ8/n4EDB9K/f38AUlNT+fTTT1m4cKGPKxM5S1kskNzafXR7yL35w9IPYOXnsG/Nqd7tcYFlLRdY1npe/+o8lzmulnzvvIDDhFNIoOfalv1HeHv/Ft4+OuJ0TkI45zeI4ZJzk6kbE0JiZFBFfTsRERGRcvProNSpUyfeeecdNmzYQJMmTVixYgVz587llVdeOeF7CgsLKSws9LzOzs4GwOFw4HA4Kr3mkyn5fF/XIdWHz/uMLRzOH+E+nEUYayZj2fATlnXfenWbrtaVdLWu5FHbxwDMdrbkN1dzprnass+MJocQT9v1e3NYvzeHD35LByAyOICujePp1awWjeJDaVQrrOK+31nI531Gqh31GfGW+ox4y5/6jDc1GKZpmpVYyxlxuVw8/PDDvPDCC1itVpxOJ8888wxjxow54XvGjh3LE088Ueb8J598QkhIyHHeISLesriKCC/YRXLmQuod/BV7cc4Z3W+1K5WZrtbMc7VgiyuJfZz8gbaNIkxaRLtIDTepGwpWPcZJREREyiEvL48hQ4aQlZVFRETESdv6dVD67LPPuP/++3nxxRdp3rw5y5cv5+677+aVV15h2LBhx33P8UaUUlJSOHDgwCl/GZXN4XAwffp0evXqhc2mdRhyatWmzzgdGKu+wLJ5BpZ1353x7fJMO1Nd7ZjjbMlasx7rzJQyz3H6q/iwQHo3S6BrkzhaJEcQH24/4xqqq2rTZ8RvqM+It9RnxFv+1Geys7OJi4srV1Dy66l3999/Pw899BBXX301AC1btiQ9PZ3nnnvuhEHJbrdjt5f9lySbzebzv5gS/lSLVA9+32dsNmg/3H04i+HABtg0HVZ/DbtXeH27EKOQQdZ5DLLO85z7w1WP2a5zmeVszXqzDpmEe67tzy3i44Xb+Xjhds+59qnRXNwiidYpUTSKD6txm0T4fZ8Rv6M+I95SnxFv+UOf8ebz/Too5eXlYbGU/q/IVqsVl8vlo4pE5JSsAZDQzH10vgtMEzb/AjsWwdIPIXvHad22uSWd5pZ0bg9wj1gVmjbmuZozzdWOP1ypbDGTOEKwp/2ibYdZtO3wn2VZDC45N4n2qTFc0CCWlJhg7AHamlxERESOz6+D0oABA3jmmWeoW7cuzZs3Z9myZbzyyivceOONvi5NRMrLMKDRRe6j20NQXAjp8yB9Pqz4HLIyTuu2dsNBD+tyeliXe85lmSFMdbZnsdmE5a5GbDWTcBz9vzmny+R/y3fxv+W7PO1TYoLp3DCOi9ISOCchnNrRwVj1QFwRERHBz4PS66+/zmOPPcYdd9zBvn37SE5O5tZbb+Xxxx/3dWkicroC7NCwh/vo8SjkZ8L+9e6H3q777rSm6pWINPK4MuBXruRXz7kdZhyznefyq6sVa8267DZjPeFp+6F8Pju0nc8W/Tllr15sCBc1TaBdajRNEsJoEBeGReFJRESkxvHroBQeHs64ceMYN26cr0sRkcoSHAV1z3cfXe+HwhzI3g0bfoJtc2Hj1DO6fR3jAEMCfmEIv3jObXMlMNPVmiWuJmwxk1hj1gPcYSj9YB7vzdvKe/O2/nmP6GC6n1OLCxrEcm6dSOLC7AQHatqeiIjI2cyvg5KI1ED2cIgPh/gm0PlOKC6ColxY9z3sWAyrvgRH3hl9RKplLzdYpnIDf4awXWYMs5ytWOI6h6VmYw6YkZ7nO+04nM+HC9L5cEG6p31iRBAdG8bSuVEcKdHBdKgfg2Fo5ElERORsoaAkIv4tIBACYuC8693HgFfBWQQZC2DPKneA2rUMigvO6GOSjUMMCZjJEGZ6zu01o1jsasJvruZsMxNZ6GqKAysmFvZkFzBp2U4mLdtZ6j4902rRLDmSCxvHkRgRREqMnt8mIiJSHSkoiUj1YhjudU4NurqPTiPd5w+nw6HNsO5Hd4jau+qMPyrByKS/dSH9rQtLnZ/rbM5qsz5zXC3ZYcaTbiZQMnVvxtp9zFi7j9d+3vjnfSLs9Ghai2ZJEbSpG02D+FBCAvV/vyIiIv5M/6QWkbNDdD330bCH+7WjAI7sc29NvnsFrP0OjuyvkI/6m/UP/sYf3Mb3nnMHzAjmuFqy0tWAP1yp/GGmkocdEwt7swv59C/PeCrROiWK8+pG0z41moTIIM6tHYnVYmgKn4iIiB9QUBKRs5MtCKLqQtvh7teX/Mv9TKe9q+HARtj0s3vK3r4/KuTj4ozsMg/JBVjuashqVyqLXeew04xjtZlKAYGYWFi+PZPl2zNLbRwB0LZeNM2SIujUMJb68aHEh9mJDSv7IG0RERGpPApKIlJzGAYktnQfLS53nyvKg8Jsd3Dav879ZwWFJ4DWls20tmzmWn4udX6lqz6rXan87kpji5nMPjOKvcQAsCT9MEvSD5faPAIgNTaEzo3iaFwrjA71Y4kNCyQhIqjCahUREZE/KSiJSM0WGOI+2gx1v+79lPvPw9vc25TvXAzbF8LGaWe8YcRfnWvZyrmWraU2jwD3c59+czZjs5nMQldT9prR7CWaYgLYdjCPbQfLPqA3NNBKx4ZxtKoTScO4EDILIaegmBibrcLqFRERqWkUlEREjic61X3U6/jnOWexe9Tp4EbY+wdsnQMHNkD+oQr72DrGAa4ImF3mfKFpY4mrMcvNRmwxk1juakimGc4BIjlS5GTG2r3MWLv3aOsA/rn0F6wWg9TYEDo2jKVxrXBa1I6gQVwYofYAAgMsFVaziIjI2UhBSUSkvKwBkNjCfTQf9Of5/EzIPwzp8+DQFveuezuXVOgIlN1w0Mm6hk6sKXNt39FtzNe66rLRrMNasy7ZZgiHXRFs3n+EzfuPlHlPZLCNRrXCOL9+DHVjQmiVEkXt6GDCAgOwWLSZhIiIiIKSiMiZCo5yHzH1S58vOgK5+yBrO6TPd48+bZ3j3o2vAtUyMulnXUi/Y7YxB/dUvqWuxmx01WadWZeNZm2yzVAO5Ud41kIdKy4skMa1wmmfGk2d6BDa1I2iVkQQ4XaFKBERqTkUlEREKktgqDs8xdSH+l1KX8vZA3kH4eAm9+jT3jXuESlHXoWWUMc4QB3rAbCWvbbfjGSxqwmbzNqsdqWy1Uwiywxlb24MB3IP8tuWg2XeExEUQNPECM6rF01KTDDn1Y2mVridiGAbNqum84mIyNlDQUlExBfCE91HQnNoNrD0tayd7nVPBzbA7pWwbw1k/A6FWRVaQryRRV/rImBRmWuFZgBLXE1Yb6bwh5nKFlcSmYSRXpDAwm0HWbjt+OuymiaG07ZeNPXjQmmdEkVSVDDhQQFEBGljCRERqV4UlERE/E1kbfeR2BJaDC59LWcvFGTCvrXuY/9a2LEYsndWaAl2o/iEa6IANrmSWWPWY62rHpvMZA6Z4Ww1k9i4x8m6PTnHfU94UAAdG8TSID6M1imRpMSEEBFkIyUmpEJrFxERqQgKSiIi1Ul4gvuIPweaX1b6Wt4hHEcOs2TKh7SvH4n10GbYuRSyd0BBxY5GNbLsohG7uNT6W5lrhWYAq836bHYls9asy0EzgrVmPfYWRPHLmgKmneAfPc2SImhYK4zmyRE0rhVGRLCNZkkRBAZYNK1PRESqnIKSiMjZIiQGbOHsjWyD64J+WP/6HKWiI+6H6+77AzIz4OBm2LXMvdHEoS0VWobdKKatsZG2lo3HvZ5lhrDLjGOVqz7bzXjWmXXZbtYia3cw3+2O47sVu8q8x2Y1aBAXRpPEcNKSwkmMCOLcOpHEhNoJtVuxBxxnEZaIiMgZUFASEakJAkPdR1i3stdcTnAWuZ8NlbsPMtPdIepwOmxfUOGlRBp5RBoZpFnKPjy3xBZXImvMemwxk9jkqk2mGUb6vgSm7Y3huxU2oOzue3FhgUfXR4WRlhROamwooXYr9WJDCbAYGIZ27BMRkfJTUBIRqeksVrAEQ512x79umnB4K+Qdcq+F2r3S/XrHIvfoVCVoYNlDA/ac8PpuM4a9ZjRrXHXZYcazzUxk25FE/lgTwq9mJAUEcmyYigqx0aRWOPXjQmmaFE5MaCDn1Y0mPtyOxTD0EF4RESlFQUlERE7OMCCmgfugXdld+ory4Mh+9059e1a5w9O+te6fc/dW6IN3SyQZh0gyDtHasvmEbbLMEJa5GrPLjGGjWYc9BTEcSo/g523JfE8gRwgue9/IIFrUjuSchHBS40JpmhiOPcBCUlQwoYFWjUqJiNQgCkoiInJmAkMgsB5E14PkNmWvFx0BRwHsXe0OVJnpsGc1ZO2AHWUfkltRIo08ullXnLRNjhnMZjOJ9a667CGG9Tl12Lc2iplr7Wwya+PEQvEx/6hsmhhOrYggzq0dSVJUEHVjQjgnMRwDg+gQGwHaeEJE5KygoCQiIpWrZH1Ug67Hv26akL3L/QDevAPu0ajD29wP4d272r0deiUJN/JpbWyhteXEG1rkmkHsNaNJNxPYbCZzcH8Ea/fVY+bGSPIJZIcZjwMrJn8GpPCgANrVc0/ra1knitjQQBIigmiSEIbFMAi2WbFYNDolIuLPFJRERMS3DOPPZ0cBNOxRtk1BNjjy4eAm90hUzm53oMraAdt/B5ej0soLMwoIM3bTkN30YPkJ2+WaQWw1E9llxrG5OJnDm8LYaNbmkyUx5GFnvxlFPoGlAlXtqGBaHh2ZapYUQUJEEBHBNhrEh2IPsGCzWBSoRER8REFJRET8X1CE+whPOP5104TCHMjZA4XZcGgrHNzoXi+19w/3+SP7KrXEMKOAlsY2WrLtlG1XuVI5YEayOTeZfeui2GPG8qGZQA4h5JpB7CeKv25GUTsqmLQk90YUqXGhpMaGEmSzUi82hKhg9zbwmvInIlKxFJRERKT6M4w/wxSceAc/R4F7ndShze5gdTgd9q87uiX6Ciis2AfznkhLyzYAunPyNVTbXAkcJIL0Iwls3xhP5oYwfjTrcNgMpxAbO804irDhOjpKFR4UQFpSBGmJ4cSH22maGEGI3UpiRBB1okNwmSb2AIs2pRARKQcFJRERqTlsQRCV4j5OxJHv3oAi/7B75z5HnnvN1P717ql+u5ZWWbmplr2kspe2HP/hvX+1zZXAAVck23Yksn97JFvNRBaaMRwxgzhIBHvNaIoJwHH0H/0pMcG0SI4kNtRG4T6DgDV7iQ0PpmF8GCGBVgKshh7kKyI1moKSiIjIX9mC3UdoHMQ1PnG7/EwoynWvn9r7h3vTicwM9zqq3L2wc0lVVQz8GaraseGk7YpNC3nYychNYMv6JA6YkWwzE/ghfRGF2Nho1iHftJNPILmEANAwPpQG8WEkRNhJjQ0lNiyQ2lEhpMaGYOJ+RpVClYicbRSURERETkdwlPuIBBKanbhdQdafoerQVvdDe3P3ugNVzt6jo1ZHqqZmIMBwEUE+LYxttCjHeiqAjKx4dmbGs5codpjxrDND+MlMZP/Rh/seNsM5QCTFWKgdHUp4kI3GtcKoEx1MfLidxrXCCQ8KICTQSu3oYGxWCwEWQ1MARcSvKSiJiIhUpqBI9wGQ0Pz4bUwTXMXuAOXIh9x97rVThTnuaX+Z6X+OVvlAXct+6rK/XG3z8uzk5wWy62As6WYiB8wIZpq1OGBGUEwAm8xkjhBMoRnAQSIJCLCSlhRJg7hQ4sPtpMSEEBEUQHJUMHVjQjBNiAy2ERxoxTRNhSsRqTIKSiIiIr5mGGC1QWQd9+u4xpDa+cTt8w9DUZ47SO1b415HlbMbDm52h6y9q92hywdCjEJCKCTWyCnXDoAAjn1WMvbWIptQdphx7Dej+MOMZrcZSw7B5JtB7DDjKCAQqz0Ua1AY9WJDSI0NJS7MTr3YEGJCAwkOtFIvNpTIYBumaRIaGKDt1UXktCkoiYiIVDfB0e4DoFbTk7ctyILiQsjaDkcOuMNVZob7ddYOzAMbKcrej92ZW/l1n4DNcNLQ2A1AG8oxalYI7ILdO2M4aEZwyAxnpxlHFqH8aMZzwIzEgZV9ZjQHzQiKsJEYH4slMIR6ce4pgVHBNhrEhxFssxIeFEDdmBCsVoNAq0U7A4oIoKAkIiJydiuZ9hdW67iXix0OfvrxR/r164fNMN2jU5kZ4HRA/iH3bn+F2e4d/zIzIO8Q7PujCr/AiSUZh0gyDpWvcc7RPw/CTjOWPDOIA2Yku4hhoxnKt2Y8h8xwXBhkmAlkE4IlwEZYRCzWkCiSYsJJiQ4mKsRG3ZhQAgMMYkLt1I1xb7seEmglKMCKYaCQJXKWUFASERERt4BA9xEc9ee5xr1O3N6R7x6hcuQf3aRin/t19i7I3uH+89AW986AeQcqvfzyqm0cBAMas/PUjY8cPfZDgWnDQQAHzAi2m7XYTiiLzWgOmpEUYSXdTCTHDME0DArtcVhDY0iMDiM6OoaYUDuJUcGEBFqpFR5ESnQITtMkzB5ATGggLtPUBhcifkZBSURERE5PyVbqANH1Tt3eWQzOQneAcjrcI1UHNrqfW5W71z1qlXcQDmxwTw30M0GGgyAchBv51OcUa8BMIPfo8Zevss2VcPRhwRHsIZpsM4S9ZgyHCcPEIN1MIM8IoU5MOIFhUdjD44kKCyIqMpK4MDvJUUEAxIXZSYoMothpEmSzEmRzP3RYQUuk4igoiYiISNWwBriPvz6fqu4FJ39PcRG4HO4AlbsfnEXunw9uguIC90hW9m73iNX+9e6pg34s1VLOTTZyjh67y17a5EomlyCWmOHsNyPJJZg9ZgyZhGExDA4H1IKQKKLDw4kICyEwMonw0CCMgBDiI4KoFW4nwGoh1G4lMSKIAIsFW4BBSKD+tVDkr/S/CBEREfFfAYFAIASGQlTd8r+vKM8dqrJ3uacGOo64n1uVvQMKcyFnj3sUq2S3wOL8SvsKFa2RZdepG+UdPU6Qy/aY0Rw2w9hGEPvMKDLNMA4RziEzHIcRiMMWSU5ALOEREcSHBeAIq010eBh2eyChYVFsyzJYlpGJPdDmfjaWxYLFAuFBtor8qiI+paAkIiIiZ5/AECCk9Hqr8nAUuANWZgZgukNV1g4oyHTvIJiz2z2iVbK5RXGh+1o1k2gcJtE4fOIGJuAADh49jmcL7DOjKMZgjxnJbjOGIwRxyIwghxBMWwgHiMEeFkl8iJUjwbUJi4gk1G4jMDSS2Nh4QoMCcRruXQeDbFZMTCKCbARaLdraXXxOQUlERESkhC3IfSS28O59Lqd73dWR/e6HBzvy4dBm988FWe6RrbyD7g0vcvZAUa77YcJ+PlXwVGoZmYA7eLU49rlZ5tE/S6YRlsNWVwIHCCKHYA6aEeSYIRwinCJbJFGhQeQFxkJwDCEhwQQF2rDHpBAa4t55MD4hmYiQIEyLjRB7ADEhgRgG2u5dTpuCkoiIiMiZsljdR1TKn+cSmpX//YW57rVYjnw4nA6G5egzr9LdoSo/0z1VsCDLPZKVvetoCMus6G/iU/VPtIarZHMML2WZIWwx4ynCSqYZzpGASIosIRyxRmIERxJqDyA/JAl7SAR2KwREJBARFUNAQCCGxUpCQjI2mw2nNZDoEPdIV4DVckbfUaoPBSURERERX7OHuf8MjoaIZO/e63K6Q1PeQffPjnz3tuyG4d6aPWcX5B8+Grb2uXcbzMz4c3rhWSzSyCPSSP/zhAk4jx5Fp3fPQ2YYmWYYxVg5TARFgdHkW4IptkViDQrFDAyF0FoEBIURQDFBcakEh4TicrkIjYojIiIarFastmCiIiIwDEPhy08pKImIiIhUZyWjWX8NWPFNvLtHcZF7bVbu0REdR/7RBw8XuacH5u5zTyssyMSVs5dDuzOIDSzCyEwH01Vx36UaiDFyiTFKhrd2QvHRH4twP3PrNDhMKyZQhI39Rix5VvemGkW2CMzAcBy2CIzgKAxbEBZbMPaoRAKCwnA58olIaoDdHoLT5SIkPJqQsCgMq4WgoBCwBroDs5wWBSURERGRmq7kYcMlI1twwnVaToeDeT/+SL9+/bDZ/rLLXXERmE73GixM98YYJdMFnQ73qFb2TncIyztwdJTrsHuqYVGuezTMdFbu9/RTNsP9vQNxEsZO94gXuENYBWzIWIgNE4NiAsi0xpIfEIHLEogjMAKXLQzTHo41OAozIAiLzU5QdBIBgaEUOwqISEjFFhSC6TKxh4QTFBaNJcDmnh5qj3AHsbM0jCkoiYiIiMiZCwh0/xlT/89z3qzTAjBN95F/2P2crJJ1WLn73NdLpg26it1TCY/sd6/vyt4J+YfcYSt7Z0V8m7OKHcfRn4oIc+b9GcQqYVf8bEsURwKiMCwWHLZwLED4kW2khLTB5bwYqtEO8gpKIiIiIuIfSkYnQmP/crLe6d+vMMcdqooL3RtgWKxHpxVud49eFeXCkYPuYFaY7V7nVZgLR/ZB1k73BhvO01zMVENFuDKJKMp0vyj483xYziYKnC7sPqnq9CgoiYiIiMjZyR7+58/hiX/+XPcC7+7jOroOq2TUyuWAIweg6Ig7cBXmuJ+theGeanhkn/ta3kH3yJezEA5ugcIsd5uzfBON4zkY3ID6gdUrelSvakVEREREqprl6K50oXF/nousc2b3dDndI12F2e4NMYoLIWs7WGxQnA+5+93XnA53+Mo7GszyDrlHwgqy3e0dR+fPOQvPrJ5KFhCT6usSvKagJCIiIiJS1SxWCAxxHyX+ur7rdJimOzgVF7iDWFGuO1gZhvtcZgZYAo5OOTzg/rMoz70OrDDXPQKWd+DPcFaBz+kqCE05dSM/o6AkIiIiInI2MIxjwld86fBVr9Pp37u40B2+CjLdQcp0uke4cvaCNcAd0LJ2uGsoynNPUyzMgcJcnKG1OJTv5Zb1fsDvg9LOnTt58MEHmTJlCnl5eTRq1IgJEybQrl07X5cmIiIiIlIzBBzdhuGvI2AAiS1P+VaXwwE//lgJRVUuvw5Khw8fpnPnznTv3p0pU6YQHx/Pxo0biY6O9nVpIiIiIiJyFvProPT888+TkpLChAkTPOfq1z/DuZsiIiIiIiKn4NdB6dtvv6VPnz5cccUV/Prrr9SuXZs77riDf/zjHyd8T2FhIYWFf+76kZ2dDYDD4cDhcJzobVWi5PN9XYdUH+oz4i31GfGW+ox4S31GvOVPfcabGgzTNP12I/egoCAARo8ezRVXXMGiRYu46667eOuttxg2bNhx3zN27FieeOKJMuc/+eQTQkJCjvMOERERERGpCfLy8hgyZAhZWVlERESctK1fB6XAwEDatWvH/PnzPefuvPNOFi1axG+//Xbc9xxvRCklJYUDBw6c8pdR2RwOB9OnT6dXr17YbDaf1iLVg/qMeEt9RrylPiPeUp8Rb/lTn8nOziYuLq5cQcmvp94lJSXRrFmzUufS0tL4+uuvT/geu92O3W4vc95ms/n8L6aEP9Ui1YP6jHhLfUa8pT4j3lKfEW/5Q5/x5vMtlVjHGevcuTPr168vdW7Dhg3Uq1fPRxWJiIiIiEhN4NdB6Z577mHBggU8++yzbNq0iU8++YR33nmHESNG+Lo0ERERERE5i/l1UGrfvj2TJk3i008/pUWLFjz11FOMGzeOoUOH+ro0ERERERE5i/n1GiWASy65hEsuucTXZYiIiIiISA3i1yNKIiIiIiIivqCgJCIiIiIicgwFJRERERERkWMoKImIiIiIiBxDQUlEREREROQYCkoiIiIiIiLHUFASERERERE5hoKSiIiIiIjIMfz+gbNnyjRNALKzs31cCTgcDvLy8sjOzsZms/m6HKkG1GfEW+oz4i31GfGW+ox4y5/6TEkmKMkIJ3PWB6WcnBwAUlJSfFyJiIiIiIj4g5ycHCIjI0/axjDLE6eqMZfLxa5duwgPD8cwDJ/Wkp2dTUpKCtu3byciIsKntUj1oD4j3lKfEW+pz4i31GfEW/7UZ0zTJCcnh+TkZCyWk69COutHlCwWC3Xq1PF1GaVERET4vJNI9aI+I95SnxFvqc+It9RnxFv+0mdONZJUQps5iIiIiIiIHENBSURERERE5BgKSlXIbrfzz3/+E7vd7utSpJpQnxFv/X979x8Tdf3HAfx5eHLcgcivuEMSxWSAig5F8YRqJQuQWZrVdJc7reVIMOiHaRhlM4LVZquWVC7pD00WTYmcP0ZgGg75FSD4A21aOBXICAF/Ivf6/tH6fP0c5L59B3cIz8d2G/d+vzle7/Ecd6/d594wM/RvMTP0bzEz9G/dq5kZ9oc5EBERERER/Vt8R4mIiIiIiMgOGyUiIiIiIiI7bJSIiIiIiIjssFEiIiIiIiKyw0bJQT799FNMnDgRbm5uiI6ORmVlpbNLIifIzs7G7NmzMWbMGPj7+2PRokVoampSrblx4wZSUlLg6+sLDw8PLFmyBK2trao1zc3NSEpKgsFggL+/P9auXYvbt287civkJDk5OdBoNEhPT1fGmBnqz4ULF/Dss8/C19cXer0eERERqK6uVuZFBG+99RYCAgKg1+sRFxeHM2fOqB6jvb0dFosFnp6e8PLywvPPP4/u7m5Hb4UcoLe3F5mZmQgODoZer8cDDzyATZs24c4zv5iZke3w4cNYuHAhxo0bB41Gg8LCQtX8QOXj2LFjePDBB+Hm5obx48fj/fffH+yt/TOhQZefny+urq6ybds2OX78uLzwwgvi5eUlra2tzi6NHCw+Pl7y8vKksbFR6urqZMGCBRIUFCTd3d3KmuTkZBk/fryUlJRIdXW1zJ07V+bNm6fM3759W6ZNmyZxcXFSW1sre/fuFT8/P3njjTecsSVyoMrKSpk4caJMnz5d0tLSlHFmhuy1t7fLhAkTZMWKFVJRUSFnz56VAwcOyC+//KKsycnJkbFjx0phYaHU19fL448/LsHBwXL9+nVlTUJCgsyYMUOOHj0qP/30k0yePFmWLVvmjC3RIMvKyhJfX1/Zs2ePnDt3TgoKCsTDw0M++ugjZQ0zM7Lt3btXNmzYILt27RIAsnv3btX8QOTjypUrYjQaxWKxSGNjo+zcuVP0er18/vnnjtqmChslB5gzZ46kpKQo93t7e2XcuHGSnZ3txKpoKGhraxMAcujQIRER6ejokNGjR0tBQYGy5uTJkwJAysvLReSvP1QuLi7S0tKirMnNzRVPT0+5efOmYzdADtPV1SUhISFSXFwsDz/8sNIoMTPUn3Xr1klsbOw/zttsNjGZTPLBBx8oYx0dHaLT6WTnzp0iInLixAkBIFVVVcqaffv2iUajkQsXLgxe8eQUSUlJ8txzz6nGnnzySbFYLCLCzJCafaM0UPnYsmWLeHt7q56b1q1bJ6GhoYO8o/7x0rtBduvWLdTU1CAuLk4Zc3FxQVxcHMrLy51YGQ0FV65cAQD4+PgAAGpqatDT06PKS1hYGIKCgpS8lJeXIyIiAkajUVkTHx+Pzs5OHD9+3IHVkyOlpKQgKSlJlQ2AmaH+FRUVISoqCk8//TT8/f0RGRmJrVu3KvPnzp1DS0uLKjdjx45FdHS0KjdeXl6IiopS1sTFxcHFxQUVFRWO2ww5xLx581BSUoLTp08DAOrr61FWVobExEQAzAzd3UDlo7y8HA899BBcXV2VNfHx8WhqasKff/7poN38l9bhP3GEuXz5Mnp7e1UvUADAaDTi1KlTTqqKhgKbzYb09HTExMRg2rRpAICWlha4urrCy8tLtdZoNKKlpUVZ01+e/p6j4Sc/Px8///wzqqqq+swxM9Sfs2fPIjc3F6+88goyMjJQVVWFl156Ca6urrBarcrvvb9c3Jkbf39/1bxWq4WPjw9zMwytX78enZ2dCAsLw6hRo9Db24usrCxYLBYAYGborgYqHy0tLQgODu7zGH/PeXt7D0r9/4SNEpGTpKSkoLGxEWVlZc4uhYaw8+fPIy0tDcXFxXBzc3N2OXSPsNlsiIqKwnvvvQcAiIyMRGNjIz777DNYrVYnV0dD0TfffIMdO3bg66+/xtSpU1FXV4f09HSMGzeOmaERi5feDTI/Pz+MGjWqzwlUra2tMJlMTqqKnC01NRV79uzBwYMHcf/99yvjJpMJt27dQkdHh2r9nXkxmUz95unvORpeampq0NbWhpkzZ0Kr1UKr1eLQoUP4+OOPodVqYTQamRnqIyAgAFOmTFGNhYeHo7m5GcB/f+93e24ymUxoa2tTzd++fRvt7e3MzTC0du1arF+/HkuXLkVERASWL1+Ol19+GdnZ2QCYGbq7gcrHUHu+YqM0yFxdXTFr1iyUlJQoYzabDSUlJTCbzU6sjJxBRJCamordu3ejtLS0z9vLs2bNwujRo1V5aWpqQnNzs5IXs9mMhoYG1R+b4uJieHp69nlhRPe++fPno6GhAXV1dcotKioKFotF+ZqZIXsxMTF9/vXA6dOnMWHCBABAcHAwTCaTKjednZ2oqKhQ5aajowM1NTXKmtLSUthsNkRHRztgF+RI165dg4uL+mXhqFGjYLPZADAzdHcDlQ+z2YzDhw+jp6dHWVNcXIzQ0FCHX3YHgMeDO0J+fr7odDr56quv5MSJE7Jq1Srx8vJSnUBFI8OLL74oY8eOlR9//FEuXbqk3K5du6asSU5OlqCgICktLZXq6moxm81iNpuV+b+Pen7sscekrq5O9u/fL/fddx+Peh5B7jz1ToSZob4qKytFq9VKVlaWnDlzRnbs2CEGg0G2b9+urMnJyREvLy/57rvv5NixY/LEE0/0e5RvZGSkVFRUSFlZmYSEhPCo52HKarVKYGCgcjz4rl27xM/PT15//XVlDTMzsnV1dUltba3U1tYKANm8ebPU1tbKb7/9JiIDk4+Ojg4xGo2yfPlyaWxslPz8fDEYDDwefLj75JNPJCgoSFxdXWXOnDly9OhRZ5dETgCg31teXp6y5vr167J69Wrx9vYWg8EgixcvlkuXLqke59dff5XExETR6/Xi5+cnr776qvT09Dh4N+Qs9o0SM0P9+f7772XatGmi0+kkLCxMvvjiC9W8zWaTzMxMMRqNotPpZP78+dLU1KRa88cff8iyZcvEw8NDPD09ZeXKldLV1eXIbZCDdHZ2SlpamgQFBYmbm5tMmjRJNmzYoDqmmZkZ2Q4ePNjvaxir1SoiA5eP+vp6iY2NFZ1OJ4GBgZKTk+OoLfahEbnjXy4TERERERERP6NERERERERkj40SERERERGRHTZKREREREREdtgoERERERER2WGjREREREREZIeNEhERERERkR02SkRERERERHbYKBEREREREdlho0RERHQXGo0GhYWFzi6DiIgcjI0SERENWStWrIBGo+lzS0hIcHZpREQ0zGmdXQAREdHdJCQkIC8vTzWm0+mcVA0REY0UfEeJiIiGNJ1OB5PJpLp5e3sD+OuyuNzcXCQmJkKv12PSpEn49ttvVd/f0NCARx99FHq9Hr6+vli1ahW6u7tVa7Zt24apU6dCp9MhICAAqampqvnLly9j8eLFMBgMCAkJQVFR0eBumoiInI6NEhER3dMyMzOxZMkS1NfXw2KxYOnSpTh58iQA4OrVq4iPj4e3tzeqqqpQUFCAH374QdUI5ebmIiUlBatWrUJDQwOKioowefJk1c9455138Mwzz+DYsWNYsGABLBYL2tvbHbpPIiJyLI2IiLOLICIi6s+KFSuwfft2uLm5qcYzMjKQkZEBjUaD5ORk5ObmKnNz587FzJkzsWXLFmzduhXr1q3D+fPn4e7uDgDYu3cvFi5ciIsXL8JoNCIwMBArV67Eu+++228NGo0Gb775JjZt2gTgr+bLw8MD+/bt42eliIiGMX5GiYiIhrRHHnlE1QgBgI+Pj/K12WxWzZnNZtTV1QEATp48iRkzZihNEgDExMTAZrOhqakJGo0GFy9exPz58+9aw/Tp05Wv3d3d4enpiba2tv93S0REdA9go0REREOau7t7n0vhBoper/+f1o0ePVp1X6PRwGazDUZJREQ0RPAzSkREdE87evRon/vh4eEAgPDwcNTX1+Pq1avK/JEjR+Di4oLQ0FCMGTMGEydORElJiUNrJiKioY/vKBER0ZB28+ZNtLS0qMa0Wi38/PwAAAUFBYiKikJsbCx27NiByspKfPnllwAAi8WCt99+G1arFRs3bsTvv/+ONWvWYPny5TAajQCAjRs3Ijk5Gf7+/khMTERXVxeOHDmCNWvWOHajREQ0pLBRIiKiIW3//v0ICAhQjYWGhuLUqVMA/jqRLj8/H6tXr0ZAQAB27tyJKVOmAAAMBgMOHDiAtLQ0zJ49GwaDAUuWLMHmzZuVx7Jarbhx4wY+/PBDvPbaa/Dz88NTTz3luA0SEdGQxFPviIjonqXRaLB7924sWrTI2aUQEdEww88oERERERER2WGjREREREREZIefUSIionsWrx4nIqLBwneUiIiIiIiI7LBRIiIiIiIissNGiYiIiIiIyA4bJSIiIiIiIjtslIiIiIiIiOywUSIiIiIiIrLDRomIiIiIiMgOGyUiIiIiIiI7/wE4E+oVDogkpwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch vs log-loss\n",
    "epochs = list(range(1, len(lr.train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, lr.train_losses, label='Training Loss')\n",
    "plt.plot(epochs, lr.test_losses, label='Validation Loss')\n",
    "\n",
    "plt.title('Epoch vs Mean Log-Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Log-Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.01, lambda_param=0.01, n_epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weights = None\n",
    "        self.accuracy_per_epoch = []\n",
    "\n",
    "    def fit(self, X, y, num_classes, x_test = None, y_test = None):\n",
    "        _, n_features = X.shape\n",
    "        self.weights = np.zeros((num_classes, n_features))\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for c in range(num_classes):\n",
    "                y_binary = np.where(y == c, 1, -1)\n",
    "                w = self.weights[c,:]\n",
    "                \n",
    "                for idx, x_i in enumerate(X):\n",
    "                    condition = y_binary[idx] * (np.dot(x_i, w)) # Check if current sample passes margin condition\n",
    "                    if condition >= 1:\n",
    "                        w -= self.learning_rate * (2 * self.lambda_param * w)\n",
    "                    else:\n",
    "                        w -= self.learning_rate * (2 * self.lambda_param * w - x_i * y_binary[idx])\n",
    "                            \n",
    "                self.weights[c, :] = w\n",
    "            \n",
    "            # Evaluate after each epoch\n",
    "            if x_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(x_test)\n",
    "                accuracy = np.mean(y_pred == y_test)\n",
    "                self.accuracy_per_epoch.append(accuracy)\n",
    "                print(f\"Epoch {epoch + 1}/{self.n_epochs} - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    def predictProb(self, X):\n",
    "        linear_output = np.dot(X, self.weights.T)\n",
    "        return linear_output\n",
    "    \n",
    "    def predict(self, linear_output):\n",
    "        return np.argmax(self.predictProb(linear_output), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n",
      "Epoch 1/200 - Accuracy: 25.88%\n",
      "Epoch 2/200 - Accuracy: 29.39%\n",
      "Epoch 3/200 - Accuracy: 31.21%\n",
      "Epoch 4/200 - Accuracy: 32.05%\n",
      "Epoch 5/200 - Accuracy: 32.89%\n",
      "Epoch 6/200 - Accuracy: 33.39%\n",
      "Epoch 7/200 - Accuracy: 33.89%\n",
      "Epoch 8/200 - Accuracy: 34.29%\n",
      "Epoch 9/200 - Accuracy: 34.48%\n",
      "Epoch 10/200 - Accuracy: 34.83%\n",
      "Epoch 11/200 - Accuracy: 35.01%\n",
      "Epoch 12/200 - Accuracy: 35.09%\n",
      "Epoch 13/200 - Accuracy: 35.17%\n",
      "Epoch 14/200 - Accuracy: 35.45%\n",
      "Epoch 15/200 - Accuracy: 35.59%\n",
      "Epoch 16/200 - Accuracy: 35.72%\n",
      "Epoch 17/200 - Accuracy: 35.83%\n",
      "Epoch 18/200 - Accuracy: 35.99%\n",
      "Epoch 19/200 - Accuracy: 36.15%\n",
      "Epoch 20/200 - Accuracy: 36.40%\n",
      "Epoch 21/200 - Accuracy: 36.46%\n",
      "Epoch 22/200 - Accuracy: 36.45%\n",
      "Epoch 23/200 - Accuracy: 36.47%\n",
      "Epoch 24/200 - Accuracy: 36.52%\n",
      "Epoch 25/200 - Accuracy: 36.70%\n",
      "Epoch 26/200 - Accuracy: 36.71%\n",
      "Epoch 27/200 - Accuracy: 36.76%\n",
      "Epoch 28/200 - Accuracy: 36.83%\n",
      "Epoch 29/200 - Accuracy: 36.87%\n",
      "Epoch 30/200 - Accuracy: 37.01%\n",
      "Epoch 31/200 - Accuracy: 37.03%\n",
      "Epoch 32/200 - Accuracy: 37.07%\n",
      "Epoch 33/200 - Accuracy: 37.20%\n",
      "Epoch 34/200 - Accuracy: 37.20%\n",
      "Epoch 35/200 - Accuracy: 37.32%\n",
      "Epoch 36/200 - Accuracy: 37.35%\n",
      "Epoch 37/200 - Accuracy: 37.49%\n",
      "Epoch 38/200 - Accuracy: 37.54%\n",
      "Epoch 39/200 - Accuracy: 37.62%\n",
      "Epoch 40/200 - Accuracy: 37.61%\n",
      "Epoch 41/200 - Accuracy: 37.66%\n",
      "Epoch 42/200 - Accuracy: 37.74%\n",
      "Epoch 43/200 - Accuracy: 37.79%\n",
      "Epoch 44/200 - Accuracy: 37.85%\n",
      "Epoch 45/200 - Accuracy: 37.86%\n",
      "Epoch 46/200 - Accuracy: 37.92%\n",
      "Epoch 47/200 - Accuracy: 37.94%\n",
      "Epoch 48/200 - Accuracy: 37.99%\n",
      "Epoch 49/200 - Accuracy: 38.03%\n",
      "Epoch 50/200 - Accuracy: 37.95%\n",
      "Epoch 51/200 - Accuracy: 37.94%\n",
      "Epoch 52/200 - Accuracy: 38.00%\n",
      "Epoch 53/200 - Accuracy: 38.00%\n",
      "Epoch 54/200 - Accuracy: 38.01%\n",
      "Epoch 55/200 - Accuracy: 38.01%\n",
      "Epoch 56/200 - Accuracy: 37.99%\n",
      "Epoch 57/200 - Accuracy: 38.08%\n",
      "Epoch 58/200 - Accuracy: 38.08%\n",
      "Epoch 59/200 - Accuracy: 38.08%\n",
      "Epoch 60/200 - Accuracy: 38.13%\n",
      "Epoch 61/200 - Accuracy: 38.16%\n",
      "Epoch 62/200 - Accuracy: 38.22%\n",
      "Epoch 63/200 - Accuracy: 38.19%\n",
      "Epoch 64/200 - Accuracy: 38.26%\n",
      "Epoch 65/200 - Accuracy: 38.25%\n",
      "Epoch 66/200 - Accuracy: 38.29%\n",
      "Epoch 67/200 - Accuracy: 38.32%\n",
      "Epoch 68/200 - Accuracy: 38.30%\n",
      "Epoch 69/200 - Accuracy: 38.38%\n",
      "Epoch 70/200 - Accuracy: 38.36%\n",
      "Epoch 71/200 - Accuracy: 38.40%\n",
      "Epoch 72/200 - Accuracy: 38.38%\n",
      "Epoch 73/200 - Accuracy: 38.33%\n",
      "Epoch 74/200 - Accuracy: 38.36%\n",
      "Epoch 75/200 - Accuracy: 38.37%\n",
      "Epoch 76/200 - Accuracy: 38.35%\n",
      "Epoch 77/200 - Accuracy: 38.40%\n",
      "Epoch 78/200 - Accuracy: 38.40%\n",
      "Epoch 79/200 - Accuracy: 38.38%\n",
      "Epoch 80/200 - Accuracy: 38.42%\n",
      "Epoch 81/200 - Accuracy: 38.43%\n",
      "Epoch 82/200 - Accuracy: 38.43%\n",
      "Epoch 83/200 - Accuracy: 38.48%\n",
      "Epoch 84/200 - Accuracy: 38.49%\n",
      "Epoch 85/200 - Accuracy: 38.45%\n",
      "Epoch 86/200 - Accuracy: 38.49%\n",
      "Epoch 87/200 - Accuracy: 38.46%\n",
      "Epoch 88/200 - Accuracy: 38.47%\n",
      "Epoch 89/200 - Accuracy: 38.51%\n",
      "Epoch 90/200 - Accuracy: 38.54%\n",
      "Epoch 91/200 - Accuracy: 38.56%\n",
      "Epoch 92/200 - Accuracy: 38.56%\n",
      "Epoch 93/200 - Accuracy: 38.61%\n",
      "Epoch 94/200 - Accuracy: 38.61%\n",
      "Epoch 95/200 - Accuracy: 38.56%\n",
      "Epoch 96/200 - Accuracy: 38.57%\n",
      "Epoch 97/200 - Accuracy: 38.57%\n",
      "Epoch 98/200 - Accuracy: 38.56%\n",
      "Epoch 99/200 - Accuracy: 38.59%\n",
      "Epoch 100/200 - Accuracy: 38.56%\n",
      "Epoch 101/200 - Accuracy: 38.59%\n",
      "Epoch 102/200 - Accuracy: 38.57%\n",
      "Epoch 103/200 - Accuracy: 38.60%\n",
      "Epoch 104/200 - Accuracy: 38.69%\n",
      "Epoch 105/200 - Accuracy: 38.69%\n",
      "Epoch 106/200 - Accuracy: 38.64%\n",
      "Epoch 107/200 - Accuracy: 38.65%\n",
      "Epoch 108/200 - Accuracy: 38.68%\n",
      "Epoch 109/200 - Accuracy: 38.69%\n",
      "Epoch 110/200 - Accuracy: 38.68%\n",
      "Epoch 111/200 - Accuracy: 38.70%\n",
      "Epoch 112/200 - Accuracy: 38.73%\n",
      "Epoch 113/200 - Accuracy: 38.76%\n",
      "Epoch 114/200 - Accuracy: 38.73%\n",
      "Epoch 115/200 - Accuracy: 38.74%\n",
      "Epoch 116/200 - Accuracy: 38.75%\n",
      "Epoch 117/200 - Accuracy: 38.80%\n",
      "Epoch 118/200 - Accuracy: 38.79%\n",
      "Epoch 119/200 - Accuracy: 38.80%\n",
      "Epoch 120/200 - Accuracy: 38.80%\n",
      "Epoch 121/200 - Accuracy: 38.78%\n",
      "Epoch 122/200 - Accuracy: 38.81%\n",
      "Epoch 123/200 - Accuracy: 38.82%\n",
      "Epoch 124/200 - Accuracy: 38.81%\n",
      "Epoch 125/200 - Accuracy: 38.83%\n",
      "Epoch 126/200 - Accuracy: 38.86%\n",
      "Epoch 127/200 - Accuracy: 38.87%\n",
      "Epoch 128/200 - Accuracy: 38.83%\n",
      "Epoch 129/200 - Accuracy: 38.86%\n",
      "Epoch 130/200 - Accuracy: 38.81%\n",
      "Epoch 131/200 - Accuracy: 38.82%\n",
      "Epoch 132/200 - Accuracy: 38.80%\n",
      "Epoch 133/200 - Accuracy: 38.82%\n",
      "Epoch 134/200 - Accuracy: 38.86%\n",
      "Epoch 135/200 - Accuracy: 38.83%\n",
      "Epoch 136/200 - Accuracy: 38.78%\n",
      "Epoch 137/200 - Accuracy: 38.92%\n",
      "Epoch 138/200 - Accuracy: 38.89%\n",
      "Epoch 139/200 - Accuracy: 38.87%\n",
      "Epoch 140/200 - Accuracy: 38.91%\n",
      "Epoch 141/200 - Accuracy: 38.93%\n",
      "Epoch 142/200 - Accuracy: 38.92%\n",
      "Epoch 143/200 - Accuracy: 38.93%\n",
      "Epoch 144/200 - Accuracy: 38.85%\n",
      "Epoch 145/200 - Accuracy: 38.89%\n",
      "Epoch 146/200 - Accuracy: 38.94%\n",
      "Epoch 147/200 - Accuracy: 38.91%\n",
      "Epoch 148/200 - Accuracy: 38.91%\n",
      "Epoch 149/200 - Accuracy: 38.92%\n",
      "Epoch 150/200 - Accuracy: 38.91%\n",
      "Epoch 151/200 - Accuracy: 38.89%\n",
      "Epoch 152/200 - Accuracy: 38.89%\n",
      "Epoch 153/200 - Accuracy: 38.91%\n",
      "Epoch 154/200 - Accuracy: 38.90%\n",
      "Epoch 155/200 - Accuracy: 38.87%\n",
      "Epoch 156/200 - Accuracy: 38.87%\n",
      "Epoch 157/200 - Accuracy: 38.88%\n",
      "Epoch 158/200 - Accuracy: 38.90%\n",
      "Epoch 159/200 - Accuracy: 38.83%\n",
      "Epoch 160/200 - Accuracy: 38.86%\n",
      "Epoch 161/200 - Accuracy: 38.87%\n",
      "Epoch 162/200 - Accuracy: 38.84%\n",
      "Epoch 163/200 - Accuracy: 38.86%\n",
      "Epoch 164/200 - Accuracy: 38.88%\n",
      "Epoch 165/200 - Accuracy: 38.88%\n",
      "Epoch 166/200 - Accuracy: 38.89%\n",
      "Epoch 167/200 - Accuracy: 38.90%\n",
      "Epoch 168/200 - Accuracy: 38.90%\n",
      "Epoch 169/200 - Accuracy: 38.89%\n",
      "Epoch 170/200 - Accuracy: 38.88%\n",
      "Epoch 171/200 - Accuracy: 38.89%\n",
      "Epoch 172/200 - Accuracy: 38.91%\n",
      "Epoch 173/200 - Accuracy: 38.89%\n",
      "Epoch 174/200 - Accuracy: 38.93%\n",
      "Epoch 175/200 - Accuracy: 38.94%\n",
      "Epoch 176/200 - Accuracy: 38.90%\n",
      "Epoch 177/200 - Accuracy: 38.92%\n",
      "Epoch 178/200 - Accuracy: 38.90%\n",
      "Epoch 179/200 - Accuracy: 38.93%\n",
      "Epoch 180/200 - Accuracy: 38.94%\n",
      "Epoch 181/200 - Accuracy: 38.91%\n",
      "Epoch 182/200 - Accuracy: 38.98%\n",
      "Epoch 183/200 - Accuracy: 38.95%\n",
      "Epoch 184/200 - Accuracy: 38.95%\n",
      "Epoch 185/200 - Accuracy: 38.97%\n",
      "Epoch 186/200 - Accuracy: 39.00%\n",
      "Epoch 187/200 - Accuracy: 38.98%\n",
      "Epoch 188/200 - Accuracy: 38.96%\n",
      "Epoch 189/200 - Accuracy: 39.04%\n",
      "Epoch 190/200 - Accuracy: 39.00%\n",
      "Epoch 191/200 - Accuracy: 39.01%\n",
      "Epoch 192/200 - Accuracy: 39.01%\n",
      "Epoch 193/200 - Accuracy: 39.02%\n",
      "Epoch 194/200 - Accuracy: 39.01%\n",
      "Epoch 195/200 - Accuracy: 39.07%\n",
      "Epoch 196/200 - Accuracy: 38.98%\n",
      "Epoch 197/200 - Accuracy: 39.01%\n",
      "Epoch 198/200 - Accuracy: 39.00%\n",
      "Epoch 199/200 - Accuracy: 39.03%\n",
      "Epoch 200/200 - Accuracy: 39.08%\n",
      "Total runtime: 584.07 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the SVM\n",
    "print(\"Training SVM...\")\n",
    "svm = SVM(learning_rate=1e-6, lambda_param=0.1, n_epochs=200)\n",
    "\n",
    "# Decode labels from one-hot encoding to integers\n",
    "y_train_decoded = np.argmax(y_train, axis=1)\n",
    "y_test_decoded = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the SVM model\n",
    "svm.fit(X_train, y_train_decoded, num_classes, x_test=X_test, y_test=y_test_decoded)\n",
    "\n",
    "# End timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total runtime: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVM...\n",
      "\n",
      "SVM Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[2410  320   46  192   52  176  130  253  971  450]\n",
      " [ 310 2494   23  134   51  199  215  180  495  899]\n",
      " [ 760  287  636  521  420  590  812  473  327  174]\n",
      " [ 342  386  135 1353  114 1108  595  273  333  361]\n",
      " [ 454  197  297  364 1191  535  936  589  222  215]\n",
      " [ 231  286  222  830  181 1956  493  333  279  189]\n",
      " [ 140  291  122  659  304  489 2419  202  162  212]\n",
      " [ 258  326  107  359  352  451  279 2152  235  481]\n",
      " [ 797  410   21   97   19  226   57   87 2749  537]\n",
      " [ 293  884   24  120   34  152  156  174  567 2596]]\n",
      "Accuracy: 0.3991\n",
      "Precision: 0.3990\n",
      "Recall: 0.3991\n",
      "F1 Score: 0.3991\n",
      "\n",
      "SVM Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[503  61  11  40  11  28  27  42 185  92]\n",
      " [ 63 486   5  30   7  44  42  35 118 170]\n",
      " [158  55 117 126  84 111 171  87  61  30]\n",
      " [ 65  76  38 270  26 229 104  53  57  82]\n",
      " [ 78  35  72  81 210 111 205 126  49  33]\n",
      " [ 49  50  50 166  38 376  87  83  71  30]\n",
      " [ 26  53  31 133  52  84 511  34  29  47]\n",
      " [ 56  64  29  64  62  84  52 421  57 111]\n",
      " [154  95   2  23   4  54  12  17 523 116]\n",
      " [ 69 168   4  34   9  27  38  37 123 491]]\n",
      "Accuracy: 0.3908\n",
      "Precision: 0.3866\n",
      "Recall: 0.3908\n",
      "F1 Score: 0.3887\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating SVM...\")\n",
    "y_pred_train = svm.predict(X_train)\n",
    "y_pred_test = svm.predict(X_test)\n",
    "\n",
    "# Training Metrics\n",
    "train_cm_svm = confusion_matrix(y_train, y_pred_train)\n",
    "train_accuracy_svm = accuracy(y_train, y_pred_train)\n",
    "train_precision_svm = precision(train_cm_svm)\n",
    "train_recall_svm = recall(train_cm_svm)\n",
    "train_f1_svm = f1_score(train_cm_svm)\n",
    "\n",
    "print(\"\\nSVM Training Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm_svm)\n",
    "print(f\"Accuracy: {train_accuracy_svm:.4f}\")\n",
    "print(f\"Precision: {train_precision_svm:.4f}\")\n",
    "print(f\"Recall: {train_recall_svm:.4f}\")\n",
    "print(f\"F1 Score: {train_f1_svm:.4f}\")\n",
    "\n",
    "# Test Metrics\n",
    "test_cm_svm = confusion_matrix(y_test, y_pred_test)\n",
    "test_accuracy_svm = accuracy(y_test, y_pred_test)\n",
    "test_precision_svm = precision(test_cm_svm)\n",
    "test_recall_svm = recall(test_cm_svm)\n",
    "test_f1_svm = f1_score(test_cm_svm)\n",
    "\n",
    "print(\"\\nSVM Validation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm_svm)\n",
    "print(f\"Accuracy: {test_accuracy_svm:.4f}\")\n",
    "print(f\"Precision: {test_precision_svm:.4f}\")\n",
    "print(f\"Recall: {test_recall_svm:.4f}\")\n",
    "print(f\"F1 Score: {test_f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models, weights):\n",
    "        assert len(models) == len(weights), \"Models and weights must have the same length\"\n",
    "        self.models = models\n",
    "        self.weights = np.array(weights)\n",
    "        self.weights /= np.sum(self.weights)  # Normalize weights\n",
    "\n",
    "    def predictProb(self, X):\n",
    "        weighted_probs = [\n",
    "            model.predictProb(X) * weight\n",
    "            for model, weight in zip(self.models, self.weights)\n",
    "        ]\n",
    "        avg_probs = np.sum(weighted_probs, axis=0)\n",
    "        return avg_probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        avg_probs = self.predictProb(X)\n",
    "        return np.argmax(avg_probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleModel(models=[lr, svm], weights=[test_accuracy, test_accuracy_svm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[ 414  181  712  601  428   53  117  156 1781  557]\n",
      " [  71 1206  409  615  310  112  238  124  858 1057]\n",
      " [  90  136 1305  861 1022  146  388  189  612  251]\n",
      " [  52  186  735 1546  607  327  468  179  472  428]\n",
      " [  41   84 1110  782 1399  156  526  273  350  279]\n",
      " [  34  153  825 1401  723  512  394  188  486  284]\n",
      " [  24  142  752  996  883  230 1199  172  314  288]\n",
      " [  57  170  770  832  912  169  353  852  364  521]\n",
      " [ 109  200  370  415  190   68   71   51 2910  616]\n",
      " [ 100  499  317  440  285   68  214  130  895 2052]]\n",
      "Accuracy: 0.2679\n",
      "Precision: 0.2987\n",
      "Recall: 0.2679\n",
      "F1 Score: 0.2825\n",
      "\n",
      "Ensemble Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[ 71  30 166 109  84  15  21  22 375 107]\n",
      " [ 11 231 101 115  61  33  44  15 179 210]\n",
      " [ 14  29 281 170 190  33  72  39 125  47]\n",
      " [  9  40 144 309 128  72  79  31 112  76]\n",
      " [  9  11 216 165 288  25 102  60  76  48]\n",
      " [ 12  33 166 282 143  97  70  42 105  50]\n",
      " [  4  32 149 211 167  33 250  41  61  52]\n",
      " [  6  41 163 153 170  43  51 181  83 109]\n",
      " [ 16  47  67  96  38  16  13   6 563 138]\n",
      " [ 13 113  60  89  59  12  50  33 175 396]]\n",
      "Accuracy: 0.2667\n",
      "Precision: 0.2994\n",
      "Recall: 0.2667\n",
      "F1 Score: 0.2821\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Ensemble Model\n",
    "y_pred_train = ensemble.predict(X_train)\n",
    "y_pred_test = ensemble.predict(X_test)\n",
    "\n",
    "# Training Metrics\n",
    "train_cm_ensemble = confusion_matrix(y_train, y_pred_train)\n",
    "train_accuracy_ensemble = accuracy(y_train, y_pred_train)\n",
    "train_precision_ensemble = precision(train_cm_ensemble)\n",
    "train_recall_ensemble = recall(train_cm_ensemble)\n",
    "train_f1_ensemble = f1_score(train_cm_ensemble)\n",
    "\n",
    "print(\"\\nEnsemble Training Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm_ensemble)\n",
    "print(f\"Accuracy: {train_accuracy_ensemble:.4f}\")\n",
    "print(f\"Precision: {train_precision_ensemble:.4f}\")\n",
    "print(f\"Recall: {train_recall_ensemble:.4f}\")\n",
    "print(f\"F1 Score: {train_f1_ensemble:.4f}\")\n",
    "\n",
    "# Test Metrics\n",
    "test_cm_ensemble = confusion_matrix(y_test, y_pred_test)\n",
    "test_accuracy_ensemble = accuracy(y_test, y_pred_test)\n",
    "test_precision_ensemble = precision(test_cm_ensemble)\n",
    "test_recall_ensemble = recall(test_cm_ensemble)\n",
    "test_f1_ensemble = f1_score(test_cm_ensemble)\n",
    "\n",
    "print(\"\\nEnsemble Validation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm_ensemble)\n",
    "print(f\"Accuracy: {test_accuracy_ensemble:.4f}\")\n",
    "print(f\"Precision: {test_precision_ensemble:.4f}\")\n",
    "print(f\"Recall: {test_recall_ensemble:.4f}\")\n",
    "print(f\"F1 Score: {test_f1_ensemble:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
