{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS613 Final Project: Image Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='bytes')\n",
    "    \n",
    "def load_cifar10():\n",
    "    train_data, train_labels = [] , []\n",
    "    for i in range(1,6):\n",
    "        batch = unpickle(f\"./cifar-10-python/cifar-10-batches-py/data_batch_{i}\")\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    # Load test batch\n",
    "    test_batch = unpickle(f\"./cifar-10-python/cifar-10-batches-py/test_batch\")\n",
    "    test_data = np.array(test_batch[b'data'])\n",
    "    test_labels = np.array(test_batch[b'labels'])\n",
    "\n",
    "    # Reshape the data to (N, 32, 32, 3)\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, labels, file_path):\n",
    "    # Combine labels and data\n",
    "    combined = np.column_stack((labels, data))\n",
    "    # Save as a CSV file\n",
    "    np.savetxt(file_path, combined, delimiter=\",\", fmt=\"%f\")\n",
    "    print(f\"Saved {file_path} successfully!\")\n",
    "    \n",
    "# Prepare data\n",
    "def normalize_images(data):\n",
    "    return data / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot = np.zeros((labels.size, num_classes))\n",
    "    one_hot[np.arange(labels.size),labels] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10 dataset...\n",
      "Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "x_train, y_train, x_test, y_test = load_cifar10()\n",
    "\n",
    "# Preprocess data\n",
    "print(\"Preprocessing data...\")\n",
    "x_train = normalize_images(x_train).reshape(x_train.shape[0], -1)\n",
    "x_test = normalize_images(x_test).reshape(x_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to CSV...\n",
      "Saved train.csv successfully!\n",
      "Saved test.csv successfully!\n",
      "CSV files created.\n"
     ]
    }
   ],
   "source": [
    "# Save csv files if needed\n",
    "print(\"Saving to CSV...\")\n",
    "save_to_csv(x_train, y_train, \"train.csv\")\n",
    "save_to_csv(x_test, y_test, \"test.csv\")\n",
    "print(\"CSV files created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    num_classes = y_true.shape[1]\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    y_true_indices = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    for t, p in zip(y_true_indices, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true_label = np.argmax(y_true, axis=1)  #Converting one-hot encoded back to label encoded\n",
    "    return np.sum(y_true_label == y_pred) / len(y_true_label)\n",
    "\n",
    "\n",
    "def precision(cm):\n",
    "    \n",
    "    precisions = []\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        if tp + fp > 0:\n",
    "            precisions.append(tp / (tp + fp))\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "def recall(cm):\n",
    "    \n",
    "    recalls = []\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        if tp + fn > 0:\n",
    "            recalls.append(tp / (tp + fn))\n",
    "        else:\n",
    "            recalls.append(0)\n",
    "    return np.mean(recalls)\n",
    "\n",
    "def f1_score(cm):\n",
    "    \n",
    "    precisions = precision(cm)\n",
    "    recalls = recall(cm)\n",
    "    \n",
    "    if (precisions + recalls) > 0:\n",
    "        return 2 * (precisions * recalls) / (precisions + recalls) \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        #Weight Initalization\n",
    "        self.num_features = None\n",
    "        self.num_classes = None\n",
    "        self.weights = None\n",
    "\n",
    "        # Initialize lists for tracking losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    # Logistic Regression Functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.softmax(np.dot(X, self.weights))\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    def log_loss(self, y, y_pred, epsilon=1e-15):\n",
    "        return -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
    "    \n",
    "    def categorical_crossentropy(self, y, y_pred, epsilon=1e-15):\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon) \n",
    "        return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  \n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def gradient_descent(self, X, y, weights, lr):\n",
    "        y_pred = self.softmax(np.dot(X, weights))\n",
    "        error = y_pred - y\n",
    "        gradient = np.dot(X.T, error) / len(y)\n",
    "        return weights - lr * gradient\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test=None, y_test=None):\n",
    "        self.num_features = X_train.shape[1]\n",
    "        self.num_classes = y_train.shape[1]\n",
    "        self.weights = np.random.randn(self.num_features, self.num_classes)\n",
    "    \n",
    "        for epoch in range(self.epochs):\n",
    "            # Update Weights\n",
    "            self.weights = self.gradient_descent(X_train, y_train, self.weights, self.learning_rate)\n",
    "\n",
    "            train_pred = self.softmax(np.dot(X_train, self.weights))\n",
    "            train_loss = self.categorical_crossentropy(y_train, train_pred)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_pred = self.softmax(np.dot(X_test, self.weights))\n",
    "                test_loss = self.categorical_crossentropy(y_test, test_pred)\n",
    "                self.test_losses.append(test_loss)\n",
    "            else:\n",
    "                test_loss = None\n",
    "\n",
    "            # if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = one_hot_encode(y_train, num_classes)\n",
    "y_test = one_hot_encode(y_test, num_classes)\n",
    "\n",
    "# Add biases to X\n",
    "X_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "X_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Train Loss = 24.7734, Test Loss = 24.8922\n",
      "Epoch 2/1000: Train Loss = 20.5121, Test Loss = 20.5999\n",
      "Epoch 3/1000: Train Loss = 18.3513, Test Loss = 18.4212\n",
      "Epoch 4/1000: Train Loss = 17.4198, Test Loss = 17.5589\n",
      "Epoch 5/1000: Train Loss = 16.5867, Test Loss = 16.6907\n",
      "Epoch 6/1000: Train Loss = 16.3677, Test Loss = 16.4836\n",
      "Epoch 7/1000: Train Loss = 16.2152, Test Loss = 16.3227\n",
      "Epoch 8/1000: Train Loss = 16.0762, Test Loss = 16.1903\n",
      "Epoch 9/1000: Train Loss = 15.9340, Test Loss = 16.0442\n",
      "Epoch 10/1000: Train Loss = 15.7986, Test Loss = 15.9101\n",
      "Epoch 11/1000: Train Loss = 15.6620, Test Loss = 15.7709\n",
      "Epoch 12/1000: Train Loss = 15.5298, Test Loss = 15.6387\n",
      "Epoch 13/1000: Train Loss = 15.3981, Test Loss = 15.5048\n",
      "Epoch 14/1000: Train Loss = 15.2693, Test Loss = 15.3753\n",
      "Epoch 15/1000: Train Loss = 15.1417, Test Loss = 15.2465\n",
      "Epoch 16/1000: Train Loss = 15.0169, Test Loss = 15.1213\n",
      "Epoch 17/1000: Train Loss = 14.8942, Test Loss = 14.9975\n",
      "Epoch 18/1000: Train Loss = 14.7741, Test Loss = 14.8764\n",
      "Epoch 19/1000: Train Loss = 14.6561, Test Loss = 14.7571\n",
      "Epoch 20/1000: Train Loss = 14.5405, Test Loss = 14.6406\n",
      "Epoch 21/1000: Train Loss = 14.4274, Test Loss = 14.5262\n",
      "Epoch 22/1000: Train Loss = 14.3166, Test Loss = 14.4142\n",
      "Epoch 23/1000: Train Loss = 14.2081, Test Loss = 14.3046\n",
      "Epoch 24/1000: Train Loss = 14.1018, Test Loss = 14.1974\n",
      "Epoch 25/1000: Train Loss = 13.9978, Test Loss = 14.0922\n",
      "Epoch 26/1000: Train Loss = 13.8960, Test Loss = 13.9891\n",
      "Epoch 27/1000: Train Loss = 13.7963, Test Loss = 13.8882\n",
      "Epoch 28/1000: Train Loss = 13.6986, Test Loss = 13.7896\n",
      "Epoch 29/1000: Train Loss = 13.6030, Test Loss = 13.6932\n",
      "Epoch 30/1000: Train Loss = 13.5095, Test Loss = 13.5989\n",
      "Epoch 31/1000: Train Loss = 13.4180, Test Loss = 13.5067\n",
      "Epoch 32/1000: Train Loss = 13.3284, Test Loss = 13.4164\n",
      "Epoch 33/1000: Train Loss = 13.2407, Test Loss = 13.3280\n",
      "Epoch 34/1000: Train Loss = 13.1549, Test Loss = 13.2415\n",
      "Epoch 35/1000: Train Loss = 13.0709, Test Loss = 13.1567\n",
      "Epoch 36/1000: Train Loss = 12.9887, Test Loss = 13.0738\n",
      "Epoch 37/1000: Train Loss = 12.9083, Test Loss = 12.9926\n",
      "Epoch 38/1000: Train Loss = 12.8295, Test Loss = 12.9132\n",
      "Epoch 39/1000: Train Loss = 12.7523, Test Loss = 12.8353\n",
      "Epoch 40/1000: Train Loss = 12.6767, Test Loss = 12.7589\n",
      "Epoch 41/1000: Train Loss = 12.6026, Test Loss = 12.6841\n",
      "Epoch 42/1000: Train Loss = 12.5299, Test Loss = 12.6108\n",
      "Epoch 43/1000: Train Loss = 12.4587, Test Loss = 12.5391\n",
      "Epoch 44/1000: Train Loss = 12.3888, Test Loss = 12.4689\n",
      "Epoch 45/1000: Train Loss = 12.3203, Test Loss = 12.4000\n",
      "Epoch 46/1000: Train Loss = 12.2531, Test Loss = 12.3324\n",
      "Epoch 47/1000: Train Loss = 12.1872, Test Loss = 12.2663\n",
      "Epoch 48/1000: Train Loss = 12.1225, Test Loss = 12.2014\n",
      "Epoch 49/1000: Train Loss = 12.0591, Test Loss = 12.1377\n",
      "Epoch 50/1000: Train Loss = 11.9969, Test Loss = 12.0752\n",
      "Epoch 51/1000: Train Loss = 11.9359, Test Loss = 12.0140\n",
      "Epoch 52/1000: Train Loss = 11.8760, Test Loss = 11.9539\n",
      "Epoch 53/1000: Train Loss = 11.8172, Test Loss = 11.8949\n",
      "Epoch 54/1000: Train Loss = 11.7595, Test Loss = 11.8371\n",
      "Epoch 55/1000: Train Loss = 11.7029, Test Loss = 11.7803\n",
      "Epoch 56/1000: Train Loss = 11.6473, Test Loss = 11.7245\n",
      "Epoch 57/1000: Train Loss = 11.5926, Test Loss = 11.6697\n",
      "Epoch 58/1000: Train Loss = 11.5390, Test Loss = 11.6159\n",
      "Epoch 59/1000: Train Loss = 11.4863, Test Loss = 11.5630\n",
      "Epoch 60/1000: Train Loss = 11.4345, Test Loss = 11.5111\n",
      "Epoch 61/1000: Train Loss = 11.3836, Test Loss = 11.4601\n",
      "Epoch 62/1000: Train Loss = 11.3336, Test Loss = 11.4099\n",
      "Epoch 63/1000: Train Loss = 11.2845, Test Loss = 11.3606\n",
      "Epoch 64/1000: Train Loss = 11.2362, Test Loss = 11.3121\n",
      "Epoch 65/1000: Train Loss = 11.1887, Test Loss = 11.2645\n",
      "Epoch 66/1000: Train Loss = 11.1421, Test Loss = 11.2177\n",
      "Epoch 67/1000: Train Loss = 11.0962, Test Loss = 11.1717\n",
      "Epoch 68/1000: Train Loss = 11.0511, Test Loss = 11.1264\n",
      "Epoch 69/1000: Train Loss = 11.0067, Test Loss = 11.0819\n",
      "Epoch 70/1000: Train Loss = 10.9631, Test Loss = 11.0380\n",
      "Epoch 71/1000: Train Loss = 10.9202, Test Loss = 10.9949\n",
      "Epoch 72/1000: Train Loss = 10.8779, Test Loss = 10.9525\n",
      "Epoch 73/1000: Train Loss = 10.8364, Test Loss = 10.9107\n",
      "Epoch 74/1000: Train Loss = 10.7955, Test Loss = 10.8697\n",
      "Epoch 75/1000: Train Loss = 10.7552, Test Loss = 10.8293\n",
      "Epoch 76/1000: Train Loss = 10.7155, Test Loss = 10.7894\n",
      "Epoch 77/1000: Train Loss = 10.6765, Test Loss = 10.7502\n",
      "Epoch 78/1000: Train Loss = 10.6380, Test Loss = 10.7116\n",
      "Epoch 79/1000: Train Loss = 10.6001, Test Loss = 10.6737\n",
      "Epoch 80/1000: Train Loss = 10.5628, Test Loss = 10.6362\n",
      "Epoch 81/1000: Train Loss = 10.5261, Test Loss = 10.5994\n",
      "Epoch 82/1000: Train Loss = 10.4899, Test Loss = 10.5631\n",
      "Epoch 83/1000: Train Loss = 10.4542, Test Loss = 10.5274\n",
      "Epoch 84/1000: Train Loss = 10.4191, Test Loss = 10.4921\n",
      "Epoch 85/1000: Train Loss = 10.3845, Test Loss = 10.4574\n",
      "Epoch 86/1000: Train Loss = 10.3504, Test Loss = 10.4231\n",
      "Epoch 87/1000: Train Loss = 10.3168, Test Loss = 10.3893\n",
      "Epoch 88/1000: Train Loss = 10.2837, Test Loss = 10.3560\n",
      "Epoch 89/1000: Train Loss = 10.2510, Test Loss = 10.3233\n",
      "Epoch 90/1000: Train Loss = 10.2188, Test Loss = 10.2910\n",
      "Epoch 91/1000: Train Loss = 10.1871, Test Loss = 10.2591\n",
      "Epoch 92/1000: Train Loss = 10.1559, Test Loss = 10.2276\n",
      "Epoch 93/1000: Train Loss = 10.1251, Test Loss = 10.1966\n",
      "Epoch 94/1000: Train Loss = 10.0947, Test Loss = 10.1660\n",
      "Epoch 95/1000: Train Loss = 10.0647, Test Loss = 10.1358\n",
      "Epoch 96/1000: Train Loss = 10.0352, Test Loss = 10.1060\n",
      "Epoch 97/1000: Train Loss = 10.0060, Test Loss = 10.0765\n",
      "Epoch 98/1000: Train Loss = 9.9773, Test Loss = 10.0475\n",
      "Epoch 99/1000: Train Loss = 9.9489, Test Loss = 10.0188\n",
      "Epoch 100/1000: Train Loss = 9.9210, Test Loss = 9.9905\n",
      "Epoch 101/1000: Train Loss = 9.8934, Test Loss = 9.9626\n",
      "Epoch 102/1000: Train Loss = 9.8661, Test Loss = 9.9351\n",
      "Epoch 103/1000: Train Loss = 9.8393, Test Loss = 9.9079\n",
      "Epoch 104/1000: Train Loss = 9.8128, Test Loss = 9.8811\n",
      "Epoch 105/1000: Train Loss = 9.7866, Test Loss = 9.8546\n",
      "Epoch 106/1000: Train Loss = 9.7608, Test Loss = 9.8284\n",
      "Epoch 107/1000: Train Loss = 9.7353, Test Loss = 9.8026\n",
      "Epoch 108/1000: Train Loss = 9.7101, Test Loss = 9.7771\n",
      "Epoch 109/1000: Train Loss = 9.6853, Test Loss = 9.7519\n",
      "Epoch 110/1000: Train Loss = 9.6607, Test Loss = 9.7270\n",
      "Epoch 111/1000: Train Loss = 9.6365, Test Loss = 9.7024\n",
      "Epoch 112/1000: Train Loss = 9.6126, Test Loss = 9.6781\n",
      "Epoch 113/1000: Train Loss = 9.5890, Test Loss = 9.6540\n",
      "Epoch 114/1000: Train Loss = 9.5656, Test Loss = 9.6303\n",
      "Epoch 115/1000: Train Loss = 9.5426, Test Loss = 9.6068\n",
      "Epoch 116/1000: Train Loss = 9.5198, Test Loss = 9.5836\n",
      "Epoch 117/1000: Train Loss = 9.4973, Test Loss = 9.5607\n",
      "Epoch 118/1000: Train Loss = 9.4750, Test Loss = 9.5381\n",
      "Epoch 119/1000: Train Loss = 9.4530, Test Loss = 9.5157\n",
      "Epoch 120/1000: Train Loss = 9.4313, Test Loss = 9.4936\n",
      "Epoch 121/1000: Train Loss = 9.4098, Test Loss = 9.4717\n",
      "Epoch 122/1000: Train Loss = 9.3886, Test Loss = 9.4501\n",
      "Epoch 123/1000: Train Loss = 9.3676, Test Loss = 9.4287\n",
      "Epoch 124/1000: Train Loss = 9.3469, Test Loss = 9.4076\n",
      "Epoch 125/1000: Train Loss = 9.3264, Test Loss = 9.3867\n",
      "Epoch 126/1000: Train Loss = 9.3061, Test Loss = 9.3660\n",
      "Epoch 127/1000: Train Loss = 9.2861, Test Loss = 9.3456\n",
      "Epoch 128/1000: Train Loss = 9.2663, Test Loss = 9.3254\n",
      "Epoch 129/1000: Train Loss = 9.2467, Test Loss = 9.3054\n",
      "Epoch 130/1000: Train Loss = 9.2273, Test Loss = 9.2856\n",
      "Epoch 131/1000: Train Loss = 9.2082, Test Loss = 9.2661\n",
      "Epoch 132/1000: Train Loss = 9.1892, Test Loss = 9.2468\n",
      "Epoch 133/1000: Train Loss = 9.1705, Test Loss = 9.2276\n",
      "Epoch 134/1000: Train Loss = 9.1519, Test Loss = 9.2087\n",
      "Epoch 135/1000: Train Loss = 9.1336, Test Loss = 9.1899\n",
      "Epoch 136/1000: Train Loss = 9.1154, Test Loss = 9.1714\n",
      "Epoch 137/1000: Train Loss = 9.0975, Test Loss = 9.1530\n",
      "Epoch 138/1000: Train Loss = 9.0797, Test Loss = 9.1349\n",
      "Epoch 139/1000: Train Loss = 9.0621, Test Loss = 9.1169\n",
      "Epoch 140/1000: Train Loss = 9.0446, Test Loss = 9.0991\n",
      "Epoch 141/1000: Train Loss = 9.0274, Test Loss = 9.0814\n",
      "Epoch 142/1000: Train Loss = 9.0103, Test Loss = 9.0640\n",
      "Epoch 143/1000: Train Loss = 8.9934, Test Loss = 9.0467\n",
      "Epoch 144/1000: Train Loss = 8.9767, Test Loss = 9.0296\n",
      "Epoch 145/1000: Train Loss = 8.9601, Test Loss = 9.0127\n",
      "Epoch 146/1000: Train Loss = 8.9437, Test Loss = 8.9959\n",
      "Epoch 147/1000: Train Loss = 8.9274, Test Loss = 8.9793\n",
      "Epoch 148/1000: Train Loss = 8.9113, Test Loss = 8.9629\n",
      "Epoch 149/1000: Train Loss = 8.8954, Test Loss = 8.9466\n",
      "Epoch 150/1000: Train Loss = 8.8796, Test Loss = 8.9304\n",
      "Epoch 151/1000: Train Loss = 8.8640, Test Loss = 8.9144\n",
      "Epoch 152/1000: Train Loss = 8.8485, Test Loss = 8.8986\n",
      "Epoch 153/1000: Train Loss = 8.8332, Test Loss = 8.8829\n",
      "Epoch 154/1000: Train Loss = 8.8180, Test Loss = 8.8674\n",
      "Epoch 155/1000: Train Loss = 8.8029, Test Loss = 8.8520\n",
      "Epoch 156/1000: Train Loss = 8.7880, Test Loss = 8.8367\n",
      "Epoch 157/1000: Train Loss = 8.7732, Test Loss = 8.8216\n",
      "Epoch 158/1000: Train Loss = 8.7585, Test Loss = 8.8066\n",
      "Epoch 159/1000: Train Loss = 8.7440, Test Loss = 8.7918\n",
      "Epoch 160/1000: Train Loss = 8.7296, Test Loss = 8.7770\n",
      "Epoch 161/1000: Train Loss = 8.7154, Test Loss = 8.7625\n",
      "Epoch 162/1000: Train Loss = 8.7012, Test Loss = 8.7480\n",
      "Epoch 163/1000: Train Loss = 8.6872, Test Loss = 8.7337\n",
      "Epoch 164/1000: Train Loss = 8.6733, Test Loss = 8.7194\n",
      "Epoch 165/1000: Train Loss = 8.6596, Test Loss = 8.7054\n",
      "Epoch 166/1000: Train Loss = 8.6459, Test Loss = 8.6914\n",
      "Epoch 167/1000: Train Loss = 8.6324, Test Loss = 8.6776\n",
      "Epoch 168/1000: Train Loss = 8.6190, Test Loss = 8.6638\n",
      "Epoch 169/1000: Train Loss = 8.6057, Test Loss = 8.6502\n",
      "Epoch 170/1000: Train Loss = 8.5925, Test Loss = 8.6367\n",
      "Epoch 171/1000: Train Loss = 8.5794, Test Loss = 8.6234\n",
      "Epoch 172/1000: Train Loss = 8.5664, Test Loss = 8.6101\n",
      "Epoch 173/1000: Train Loss = 8.5536, Test Loss = 8.5969\n",
      "Epoch 174/1000: Train Loss = 8.5408, Test Loss = 8.5839\n",
      "Epoch 175/1000: Train Loss = 8.5282, Test Loss = 8.5709\n",
      "Epoch 176/1000: Train Loss = 8.5156, Test Loss = 8.5581\n",
      "Epoch 177/1000: Train Loss = 8.5032, Test Loss = 8.5454\n",
      "Epoch 178/1000: Train Loss = 8.4908, Test Loss = 8.5327\n",
      "Epoch 179/1000: Train Loss = 8.4786, Test Loss = 8.5202\n",
      "Epoch 180/1000: Train Loss = 8.4664, Test Loss = 8.5077\n",
      "Epoch 181/1000: Train Loss = 8.4544, Test Loss = 8.4954\n",
      "Epoch 182/1000: Train Loss = 8.4424, Test Loss = 8.4831\n",
      "Epoch 183/1000: Train Loss = 8.4305, Test Loss = 8.4710\n",
      "Epoch 184/1000: Train Loss = 8.4187, Test Loss = 8.4588\n",
      "Epoch 185/1000: Train Loss = 8.4070, Test Loss = 8.4469\n",
      "Epoch 186/1000: Train Loss = 8.3954, Test Loss = 8.4350\n",
      "Epoch 187/1000: Train Loss = 8.3839, Test Loss = 8.4233\n",
      "Epoch 188/1000: Train Loss = 8.3725, Test Loss = 8.4115\n",
      "Epoch 189/1000: Train Loss = 8.3611, Test Loss = 8.4000\n",
      "Epoch 190/1000: Train Loss = 8.3499, Test Loss = 8.3883\n",
      "Epoch 191/1000: Train Loss = 8.3387, Test Loss = 8.3770\n",
      "Epoch 192/1000: Train Loss = 8.3276, Test Loss = 8.3655\n",
      "Epoch 193/1000: Train Loss = 8.3166, Test Loss = 8.3545\n",
      "Epoch 194/1000: Train Loss = 8.3057, Test Loss = 8.3430\n",
      "Epoch 195/1000: Train Loss = 8.2949, Test Loss = 8.3323\n",
      "Epoch 196/1000: Train Loss = 8.2841, Test Loss = 8.3209\n",
      "Epoch 197/1000: Train Loss = 8.2735, Test Loss = 8.3104\n",
      "Epoch 198/1000: Train Loss = 8.2630, Test Loss = 8.2992\n",
      "Epoch 199/1000: Train Loss = 8.2526, Test Loss = 8.2891\n",
      "Epoch 200/1000: Train Loss = 8.2423, Test Loss = 8.2780\n",
      "Epoch 201/1000: Train Loss = 8.2322, Test Loss = 8.2683\n",
      "Epoch 202/1000: Train Loss = 8.2224, Test Loss = 8.2574\n",
      "Epoch 203/1000: Train Loss = 8.2127, Test Loss = 8.2485\n",
      "Epoch 204/1000: Train Loss = 8.2036, Test Loss = 8.2380\n",
      "Epoch 205/1000: Train Loss = 8.1946, Test Loss = 8.2301\n",
      "Epoch 206/1000: Train Loss = 8.1869, Test Loss = 8.2208\n",
      "Epoch 207/1000: Train Loss = 8.1790, Test Loss = 8.2144\n",
      "Epoch 208/1000: Train Loss = 8.1741, Test Loss = 8.2075\n",
      "Epoch 209/1000: Train Loss = 8.1677, Test Loss = 8.2030\n",
      "Epoch 210/1000: Train Loss = 8.1681, Test Loss = 8.2014\n",
      "Epoch 211/1000: Train Loss = 8.1628, Test Loss = 8.1980\n",
      "Epoch 212/1000: Train Loss = 8.1725, Test Loss = 8.2064\n",
      "Epoch 213/1000: Train Loss = 8.1648, Test Loss = 8.1998\n",
      "Epoch 214/1000: Train Loss = 8.1872, Test Loss = 8.2224\n",
      "Epoch 215/1000: Train Loss = 8.1701, Test Loss = 8.2043\n",
      "Epoch 216/1000: Train Loss = 8.2040, Test Loss = 8.2408\n",
      "Epoch 217/1000: Train Loss = 8.1723, Test Loss = 8.2051\n",
      "Epoch 218/1000: Train Loss = 8.2118, Test Loss = 8.2499\n",
      "Epoch 219/1000: Train Loss = 8.1690, Test Loss = 8.2005\n",
      "Epoch 220/1000: Train Loss = 8.2095, Test Loss = 8.2483\n",
      "Epoch 221/1000: Train Loss = 8.1621, Test Loss = 8.1922\n",
      "Epoch 222/1000: Train Loss = 8.2017, Test Loss = 8.2410\n",
      "Epoch 223/1000: Train Loss = 8.1532, Test Loss = 8.1820\n",
      "Epoch 224/1000: Train Loss = 8.1915, Test Loss = 8.2310\n",
      "Epoch 225/1000: Train Loss = 8.1429, Test Loss = 8.1706\n",
      "Epoch 226/1000: Train Loss = 8.1798, Test Loss = 8.2194\n",
      "Epoch 227/1000: Train Loss = 8.1316, Test Loss = 8.1584\n",
      "Epoch 228/1000: Train Loss = 8.1672, Test Loss = 8.2067\n",
      "Epoch 229/1000: Train Loss = 8.1195, Test Loss = 8.1454\n",
      "Epoch 230/1000: Train Loss = 8.1539, Test Loss = 8.1933\n",
      "Epoch 231/1000: Train Loss = 8.1067, Test Loss = 8.1319\n",
      "Epoch 232/1000: Train Loss = 8.1402, Test Loss = 8.1795\n",
      "Epoch 233/1000: Train Loss = 8.0937, Test Loss = 8.1181\n",
      "Epoch 234/1000: Train Loss = 8.1263, Test Loss = 8.1655\n",
      "Epoch 235/1000: Train Loss = 8.0803, Test Loss = 8.1041\n",
      "Epoch 236/1000: Train Loss = 8.1124, Test Loss = 8.1513\n",
      "Epoch 237/1000: Train Loss = 8.0669, Test Loss = 8.0900\n",
      "Epoch 238/1000: Train Loss = 8.0985, Test Loss = 8.1372\n",
      "Epoch 239/1000: Train Loss = 8.0535, Test Loss = 8.0760\n",
      "Epoch 240/1000: Train Loss = 8.0846, Test Loss = 8.1231\n",
      "Epoch 241/1000: Train Loss = 8.0402, Test Loss = 8.0620\n",
      "Epoch 242/1000: Train Loss = 8.0708, Test Loss = 8.1091\n",
      "Epoch 243/1000: Train Loss = 8.0269, Test Loss = 8.0481\n",
      "Epoch 244/1000: Train Loss = 8.0572, Test Loss = 8.0953\n",
      "Epoch 245/1000: Train Loss = 8.0138, Test Loss = 8.0344\n",
      "Epoch 246/1000: Train Loss = 8.0436, Test Loss = 8.0816\n",
      "Epoch 247/1000: Train Loss = 8.0007, Test Loss = 8.0207\n",
      "Epoch 248/1000: Train Loss = 8.0302, Test Loss = 8.0680\n",
      "Epoch 249/1000: Train Loss = 7.9878, Test Loss = 8.0072\n",
      "Epoch 250/1000: Train Loss = 8.0170, Test Loss = 8.0545\n",
      "Epoch 251/1000: Train Loss = 7.9751, Test Loss = 7.9939\n",
      "Epoch 252/1000: Train Loss = 8.0039, Test Loss = 8.0412\n",
      "Epoch 253/1000: Train Loss = 7.9625, Test Loss = 7.9807\n",
      "Epoch 254/1000: Train Loss = 7.9909, Test Loss = 8.0281\n",
      "Epoch 255/1000: Train Loss = 7.9500, Test Loss = 7.9677\n",
      "Epoch 256/1000: Train Loss = 7.9780, Test Loss = 8.0150\n",
      "Epoch 257/1000: Train Loss = 7.9377, Test Loss = 7.9548\n",
      "Epoch 258/1000: Train Loss = 7.9654, Test Loss = 8.0022\n",
      "Epoch 259/1000: Train Loss = 7.9255, Test Loss = 7.9420\n",
      "Epoch 260/1000: Train Loss = 7.9528, Test Loss = 7.9894\n",
      "Epoch 261/1000: Train Loss = 7.9134, Test Loss = 7.9294\n",
      "Epoch 262/1000: Train Loss = 7.9404, Test Loss = 7.9769\n",
      "Epoch 263/1000: Train Loss = 7.9015, Test Loss = 7.9169\n",
      "Epoch 264/1000: Train Loss = 7.9281, Test Loss = 7.9644\n",
      "Epoch 265/1000: Train Loss = 7.8897, Test Loss = 7.9046\n",
      "Epoch 266/1000: Train Loss = 7.9159, Test Loss = 7.9521\n",
      "Epoch 267/1000: Train Loss = 7.8781, Test Loss = 7.8925\n",
      "Epoch 268/1000: Train Loss = 7.9039, Test Loss = 7.9399\n",
      "Epoch 269/1000: Train Loss = 7.8666, Test Loss = 7.8804\n",
      "Epoch 270/1000: Train Loss = 7.8921, Test Loss = 7.9279\n",
      "Epoch 271/1000: Train Loss = 7.8552, Test Loss = 7.8686\n",
      "Epoch 272/1000: Train Loss = 7.8803, Test Loss = 7.9160\n",
      "Epoch 273/1000: Train Loss = 7.8440, Test Loss = 7.8568\n",
      "Epoch 274/1000: Train Loss = 7.8687, Test Loss = 7.9043\n",
      "Epoch 275/1000: Train Loss = 7.8329, Test Loss = 7.8453\n",
      "Epoch 276/1000: Train Loss = 7.8572, Test Loss = 7.8926\n",
      "Epoch 277/1000: Train Loss = 7.8220, Test Loss = 7.8338\n",
      "Epoch 278/1000: Train Loss = 7.8459, Test Loss = 7.8812\n",
      "Epoch 279/1000: Train Loss = 7.8112, Test Loss = 7.8226\n",
      "Epoch 280/1000: Train Loss = 7.8347, Test Loss = 7.8698\n",
      "Epoch 281/1000: Train Loss = 7.8005, Test Loss = 7.8114\n",
      "Epoch 282/1000: Train Loss = 7.8236, Test Loss = 7.8586\n",
      "Epoch 283/1000: Train Loss = 7.7900, Test Loss = 7.8005\n",
      "Epoch 284/1000: Train Loss = 7.8127, Test Loss = 7.8475\n",
      "Epoch 285/1000: Train Loss = 7.7796, Test Loss = 7.7896\n",
      "Epoch 286/1000: Train Loss = 7.8019, Test Loss = 7.8366\n",
      "Epoch 287/1000: Train Loss = 7.7693, Test Loss = 7.7789\n",
      "Epoch 288/1000: Train Loss = 7.7913, Test Loss = 7.8259\n",
      "Epoch 289/1000: Train Loss = 7.7592, Test Loss = 7.7684\n",
      "Epoch 290/1000: Train Loss = 7.7808, Test Loss = 7.8152\n",
      "Epoch 291/1000: Train Loss = 7.7492, Test Loss = 7.7580\n",
      "Epoch 292/1000: Train Loss = 7.7704, Test Loss = 7.8047\n",
      "Epoch 293/1000: Train Loss = 7.7393, Test Loss = 7.7477\n",
      "Epoch 294/1000: Train Loss = 7.7602, Test Loss = 7.7944\n",
      "Epoch 295/1000: Train Loss = 7.7295, Test Loss = 7.7376\n",
      "Epoch 296/1000: Train Loss = 7.7502, Test Loss = 7.7842\n",
      "Epoch 297/1000: Train Loss = 7.7199, Test Loss = 7.7276\n",
      "Epoch 298/1000: Train Loss = 7.7402, Test Loss = 7.7741\n",
      "Epoch 299/1000: Train Loss = 7.7104, Test Loss = 7.7178\n",
      "Epoch 300/1000: Train Loss = 7.7305, Test Loss = 7.7642\n",
      "Epoch 301/1000: Train Loss = 7.7010, Test Loss = 7.7081\n",
      "Epoch 302/1000: Train Loss = 7.7209, Test Loss = 7.7545\n",
      "Epoch 303/1000: Train Loss = 7.6917, Test Loss = 7.6985\n",
      "Epoch 304/1000: Train Loss = 7.7114, Test Loss = 7.7449\n",
      "Epoch 305/1000: Train Loss = 7.6826, Test Loss = 7.6890\n",
      "Epoch 306/1000: Train Loss = 7.7020, Test Loss = 7.7354\n",
      "Epoch 307/1000: Train Loss = 7.6735, Test Loss = 7.6796\n",
      "Epoch 308/1000: Train Loss = 7.6928, Test Loss = 7.7261\n",
      "Epoch 309/1000: Train Loss = 7.6645, Test Loss = 7.6704\n",
      "Epoch 310/1000: Train Loss = 7.6838, Test Loss = 7.7169\n",
      "Epoch 311/1000: Train Loss = 7.6557, Test Loss = 7.6613\n",
      "Epoch 312/1000: Train Loss = 7.6749, Test Loss = 7.7078\n",
      "Epoch 313/1000: Train Loss = 7.6469, Test Loss = 7.6522\n",
      "Epoch 314/1000: Train Loss = 7.6660, Test Loss = 7.6989\n",
      "Epoch 315/1000: Train Loss = 7.6382, Test Loss = 7.6433\n",
      "Epoch 316/1000: Train Loss = 7.6573, Test Loss = 7.6901\n",
      "Epoch 317/1000: Train Loss = 7.6296, Test Loss = 7.6344\n",
      "Epoch 318/1000: Train Loss = 7.6487, Test Loss = 7.6814\n",
      "Epoch 319/1000: Train Loss = 7.6210, Test Loss = 7.6256\n",
      "Epoch 320/1000: Train Loss = 7.6403, Test Loss = 7.6728\n",
      "Epoch 321/1000: Train Loss = 7.6125, Test Loss = 7.6169\n",
      "Epoch 322/1000: Train Loss = 7.6319, Test Loss = 7.6642\n",
      "Epoch 323/1000: Train Loss = 7.6041, Test Loss = 7.6083\n",
      "Epoch 324/1000: Train Loss = 7.6236, Test Loss = 7.6558\n",
      "Epoch 325/1000: Train Loss = 7.5957, Test Loss = 7.5997\n",
      "Epoch 326/1000: Train Loss = 7.6153, Test Loss = 7.6475\n",
      "Epoch 327/1000: Train Loss = 7.5874, Test Loss = 7.5912\n",
      "Epoch 328/1000: Train Loss = 7.6072, Test Loss = 7.6392\n",
      "Epoch 329/1000: Train Loss = 7.5792, Test Loss = 7.5827\n",
      "Epoch 330/1000: Train Loss = 7.5991, Test Loss = 7.6311\n",
      "Epoch 331/1000: Train Loss = 7.5710, Test Loss = 7.5743\n",
      "Epoch 332/1000: Train Loss = 7.5911, Test Loss = 7.6230\n",
      "Epoch 333/1000: Train Loss = 7.5628, Test Loss = 7.5660\n",
      "Epoch 334/1000: Train Loss = 7.5832, Test Loss = 7.6150\n",
      "Epoch 335/1000: Train Loss = 7.5548, Test Loss = 7.5577\n",
      "Epoch 336/1000: Train Loss = 7.5753, Test Loss = 7.6070\n",
      "Epoch 337/1000: Train Loss = 7.5467, Test Loss = 7.5495\n",
      "Epoch 338/1000: Train Loss = 7.5675, Test Loss = 7.5991\n",
      "Epoch 339/1000: Train Loss = 7.5387, Test Loss = 7.5414\n",
      "Epoch 340/1000: Train Loss = 7.5597, Test Loss = 7.5913\n",
      "Epoch 341/1000: Train Loss = 7.5308, Test Loss = 7.5333\n",
      "Epoch 342/1000: Train Loss = 7.5520, Test Loss = 7.5835\n",
      "Epoch 343/1000: Train Loss = 7.5229, Test Loss = 7.5252\n",
      "Epoch 344/1000: Train Loss = 7.5444, Test Loss = 7.5758\n",
      "Epoch 345/1000: Train Loss = 7.5151, Test Loss = 7.5173\n",
      "Epoch 346/1000: Train Loss = 7.5368, Test Loss = 7.5682\n",
      "Epoch 347/1000: Train Loss = 7.5073, Test Loss = 7.5093\n",
      "Epoch 348/1000: Train Loss = 7.5293, Test Loss = 7.5606\n",
      "Epoch 349/1000: Train Loss = 7.4996, Test Loss = 7.5015\n",
      "Epoch 350/1000: Train Loss = 7.5219, Test Loss = 7.5531\n",
      "Epoch 351/1000: Train Loss = 7.4920, Test Loss = 7.4937\n",
      "Epoch 352/1000: Train Loss = 7.5145, Test Loss = 7.5456\n",
      "Epoch 353/1000: Train Loss = 7.4844, Test Loss = 7.4859\n",
      "Epoch 354/1000: Train Loss = 7.5072, Test Loss = 7.5382\n",
      "Epoch 355/1000: Train Loss = 7.4768, Test Loss = 7.4783\n",
      "Epoch 356/1000: Train Loss = 7.4999, Test Loss = 7.5309\n",
      "Epoch 357/1000: Train Loss = 7.4693, Test Loss = 7.4706\n",
      "Epoch 358/1000: Train Loss = 7.4927, Test Loss = 7.5236\n",
      "Epoch 359/1000: Train Loss = 7.4619, Test Loss = 7.4631\n",
      "Epoch 360/1000: Train Loss = 7.4855, Test Loss = 7.5164\n",
      "Epoch 361/1000: Train Loss = 7.4545, Test Loss = 7.4555\n",
      "Epoch 362/1000: Train Loss = 7.4784, Test Loss = 7.5092\n",
      "Epoch 363/1000: Train Loss = 7.4471, Test Loss = 7.4481\n",
      "Epoch 364/1000: Train Loss = 7.4714, Test Loss = 7.5021\n",
      "Epoch 365/1000: Train Loss = 7.4398, Test Loss = 7.4407\n",
      "Epoch 366/1000: Train Loss = 7.4644, Test Loss = 7.4951\n",
      "Epoch 367/1000: Train Loss = 7.4326, Test Loss = 7.4334\n",
      "Epoch 368/1000: Train Loss = 7.4574, Test Loss = 7.4881\n",
      "Epoch 369/1000: Train Loss = 7.4254, Test Loss = 7.4261\n",
      "Epoch 370/1000: Train Loss = 7.4505, Test Loss = 7.4812\n",
      "Epoch 371/1000: Train Loss = 7.4183, Test Loss = 7.4188\n",
      "Epoch 372/1000: Train Loss = 7.4437, Test Loss = 7.4743\n",
      "Epoch 373/1000: Train Loss = 7.4112, Test Loss = 7.4116\n",
      "Epoch 374/1000: Train Loss = 7.4369, Test Loss = 7.4675\n",
      "Epoch 375/1000: Train Loss = 7.4042, Test Loss = 7.4045\n",
      "Epoch 376/1000: Train Loss = 7.4302, Test Loss = 7.4608\n",
      "Epoch 377/1000: Train Loss = 7.3972, Test Loss = 7.3974\n",
      "Epoch 378/1000: Train Loss = 7.4235, Test Loss = 7.4541\n",
      "Epoch 379/1000: Train Loss = 7.3903, Test Loss = 7.3904\n",
      "Epoch 380/1000: Train Loss = 7.4169, Test Loss = 7.4474\n",
      "Epoch 381/1000: Train Loss = 7.3834, Test Loss = 7.3835\n",
      "Epoch 382/1000: Train Loss = 7.4103, Test Loss = 7.4409\n",
      "Epoch 383/1000: Train Loss = 7.3766, Test Loss = 7.3765\n",
      "Epoch 384/1000: Train Loss = 7.4038, Test Loss = 7.4343\n",
      "Epoch 385/1000: Train Loss = 7.3698, Test Loss = 7.3697\n",
      "Epoch 386/1000: Train Loss = 7.3973, Test Loss = 7.4278\n",
      "Epoch 387/1000: Train Loss = 7.3631, Test Loss = 7.3629\n",
      "Epoch 388/1000: Train Loss = 7.3909, Test Loss = 7.4214\n",
      "Epoch 389/1000: Train Loss = 7.3564, Test Loss = 7.3561\n",
      "Epoch 390/1000: Train Loss = 7.3845, Test Loss = 7.4150\n",
      "Epoch 391/1000: Train Loss = 7.3498, Test Loss = 7.3494\n",
      "Epoch 392/1000: Train Loss = 7.3781, Test Loss = 7.4087\n",
      "Epoch 393/1000: Train Loss = 7.3432, Test Loss = 7.3428\n",
      "Epoch 394/1000: Train Loss = 7.3718, Test Loss = 7.4024\n",
      "Epoch 395/1000: Train Loss = 7.3366, Test Loss = 7.3361\n",
      "Epoch 396/1000: Train Loss = 7.3656, Test Loss = 7.3961\n",
      "Epoch 397/1000: Train Loss = 7.3301, Test Loss = 7.3296\n",
      "Epoch 398/1000: Train Loss = 7.3594, Test Loss = 7.3899\n",
      "Epoch 399/1000: Train Loss = 7.3237, Test Loss = 7.3231\n",
      "Epoch 400/1000: Train Loss = 7.3532, Test Loss = 7.3838\n",
      "Epoch 401/1000: Train Loss = 7.3173, Test Loss = 7.3166\n",
      "Epoch 402/1000: Train Loss = 7.3471, Test Loss = 7.3777\n",
      "Epoch 403/1000: Train Loss = 7.3109, Test Loss = 7.3102\n",
      "Epoch 404/1000: Train Loss = 7.3410, Test Loss = 7.3716\n",
      "Epoch 405/1000: Train Loss = 7.3046, Test Loss = 7.3038\n",
      "Epoch 406/1000: Train Loss = 7.3350, Test Loss = 7.3656\n",
      "Epoch 407/1000: Train Loss = 7.2983, Test Loss = 7.2975\n",
      "Epoch 408/1000: Train Loss = 7.3290, Test Loss = 7.3596\n",
      "Epoch 409/1000: Train Loss = 7.2921, Test Loss = 7.2912\n",
      "Epoch 410/1000: Train Loss = 7.3231, Test Loss = 7.3537\n",
      "Epoch 411/1000: Train Loss = 7.2859, Test Loss = 7.2850\n",
      "Epoch 412/1000: Train Loss = 7.3172, Test Loss = 7.3478\n",
      "Epoch 413/1000: Train Loss = 7.2798, Test Loss = 7.2788\n",
      "Epoch 414/1000: Train Loss = 7.3113, Test Loss = 7.3419\n",
      "Epoch 415/1000: Train Loss = 7.2736, Test Loss = 7.2727\n",
      "Epoch 416/1000: Train Loss = 7.3055, Test Loss = 7.3361\n",
      "Epoch 417/1000: Train Loss = 7.2676, Test Loss = 7.2666\n",
      "Epoch 418/1000: Train Loss = 7.2997, Test Loss = 7.3304\n",
      "Epoch 419/1000: Train Loss = 7.2616, Test Loss = 7.2605\n",
      "Epoch 420/1000: Train Loss = 7.2939, Test Loss = 7.3246\n",
      "Epoch 421/1000: Train Loss = 7.2556, Test Loss = 7.2545\n",
      "Epoch 422/1000: Train Loss = 7.2882, Test Loss = 7.3189\n",
      "Epoch 423/1000: Train Loss = 7.2496, Test Loss = 7.2485\n",
      "Epoch 424/1000: Train Loss = 7.2825, Test Loss = 7.3133\n",
      "Epoch 425/1000: Train Loss = 7.2437, Test Loss = 7.2426\n",
      "Epoch 426/1000: Train Loss = 7.2769, Test Loss = 7.3077\n",
      "Epoch 427/1000: Train Loss = 7.2378, Test Loss = 7.2367\n",
      "Epoch 428/1000: Train Loss = 7.2712, Test Loss = 7.3021\n",
      "Epoch 429/1000: Train Loss = 7.2320, Test Loss = 7.2308\n",
      "Epoch 430/1000: Train Loss = 7.2657, Test Loss = 7.2966\n",
      "Epoch 431/1000: Train Loss = 7.2262, Test Loss = 7.2250\n",
      "Epoch 432/1000: Train Loss = 7.2601, Test Loss = 7.2911\n",
      "Epoch 433/1000: Train Loss = 7.2204, Test Loss = 7.2192\n",
      "Epoch 434/1000: Train Loss = 7.2546, Test Loss = 7.2856\n",
      "Epoch 435/1000: Train Loss = 7.2147, Test Loss = 7.2135\n",
      "Epoch 436/1000: Train Loss = 7.2491, Test Loss = 7.2802\n",
      "Epoch 437/1000: Train Loss = 7.2090, Test Loss = 7.2078\n",
      "Epoch 438/1000: Train Loss = 7.2437, Test Loss = 7.2748\n",
      "Epoch 439/1000: Train Loss = 7.2033, Test Loss = 7.2021\n",
      "Epoch 440/1000: Train Loss = 7.2383, Test Loss = 7.2695\n",
      "Epoch 441/1000: Train Loss = 7.1977, Test Loss = 7.1965\n",
      "Epoch 442/1000: Train Loss = 7.2329, Test Loss = 7.2641\n",
      "Epoch 443/1000: Train Loss = 7.1921, Test Loss = 7.1909\n",
      "Epoch 444/1000: Train Loss = 7.2276, Test Loss = 7.2588\n",
      "Epoch 445/1000: Train Loss = 7.1866, Test Loss = 7.1854\n",
      "Epoch 446/1000: Train Loss = 7.2223, Test Loss = 7.2536\n",
      "Epoch 447/1000: Train Loss = 7.1811, Test Loss = 7.1799\n",
      "Epoch 448/1000: Train Loss = 7.2170, Test Loss = 7.2483\n",
      "Epoch 449/1000: Train Loss = 7.1756, Test Loss = 7.1744\n",
      "Epoch 450/1000: Train Loss = 7.2117, Test Loss = 7.2431\n",
      "Epoch 451/1000: Train Loss = 7.1701, Test Loss = 7.1689\n",
      "Epoch 452/1000: Train Loss = 7.2065, Test Loss = 7.2380\n",
      "Epoch 453/1000: Train Loss = 7.1647, Test Loss = 7.1635\n",
      "Epoch 454/1000: Train Loss = 7.2013, Test Loss = 7.2328\n",
      "Epoch 455/1000: Train Loss = 7.1593, Test Loss = 7.1581\n",
      "Epoch 456/1000: Train Loss = 7.1962, Test Loss = 7.2277\n",
      "Epoch 457/1000: Train Loss = 7.1539, Test Loss = 7.1528\n",
      "Epoch 458/1000: Train Loss = 7.1911, Test Loss = 7.2227\n",
      "Epoch 459/1000: Train Loss = 7.1486, Test Loss = 7.1475\n",
      "Epoch 460/1000: Train Loss = 7.1860, Test Loss = 7.2176\n",
      "Epoch 461/1000: Train Loss = 7.1433, Test Loss = 7.1422\n",
      "Epoch 462/1000: Train Loss = 7.1809, Test Loss = 7.2126\n",
      "Epoch 463/1000: Train Loss = 7.1381, Test Loss = 7.1370\n",
      "Epoch 464/1000: Train Loss = 7.1758, Test Loss = 7.2076\n",
      "Epoch 465/1000: Train Loss = 7.1328, Test Loss = 7.1317\n",
      "Epoch 466/1000: Train Loss = 7.1708, Test Loss = 7.2027\n",
      "Epoch 467/1000: Train Loss = 7.1276, Test Loss = 7.1265\n",
      "Epoch 468/1000: Train Loss = 7.1658, Test Loss = 7.1978\n",
      "Epoch 469/1000: Train Loss = 7.1224, Test Loss = 7.1214\n",
      "Epoch 470/1000: Train Loss = 7.1609, Test Loss = 7.1929\n",
      "Epoch 471/1000: Train Loss = 7.1173, Test Loss = 7.1163\n",
      "Epoch 472/1000: Train Loss = 7.1560, Test Loss = 7.1880\n",
      "Epoch 473/1000: Train Loss = 7.1122, Test Loss = 7.1111\n",
      "Epoch 474/1000: Train Loss = 7.1511, Test Loss = 7.1832\n",
      "Epoch 475/1000: Train Loss = 7.1071, Test Loss = 7.1061\n",
      "Epoch 476/1000: Train Loss = 7.1462, Test Loss = 7.1784\n",
      "Epoch 477/1000: Train Loss = 7.1020, Test Loss = 7.1010\n",
      "Epoch 478/1000: Train Loss = 7.1413, Test Loss = 7.1736\n",
      "Epoch 479/1000: Train Loss = 7.0970, Test Loss = 7.0960\n",
      "Epoch 480/1000: Train Loss = 7.1365, Test Loss = 7.1688\n",
      "Epoch 481/1000: Train Loss = 7.0920, Test Loss = 7.0910\n",
      "Epoch 482/1000: Train Loss = 7.1317, Test Loss = 7.1641\n",
      "Epoch 483/1000: Train Loss = 7.0870, Test Loss = 7.0861\n",
      "Epoch 484/1000: Train Loss = 7.1269, Test Loss = 7.1594\n",
      "Epoch 485/1000: Train Loss = 7.0820, Test Loss = 7.0812\n",
      "Epoch 486/1000: Train Loss = 7.1222, Test Loss = 7.1547\n",
      "Epoch 487/1000: Train Loss = 7.0771, Test Loss = 7.0763\n",
      "Epoch 488/1000: Train Loss = 7.1175, Test Loss = 7.1501\n",
      "Epoch 489/1000: Train Loss = 7.0722, Test Loss = 7.0714\n",
      "Epoch 490/1000: Train Loss = 7.1128, Test Loss = 7.1455\n",
      "Epoch 491/1000: Train Loss = 7.0673, Test Loss = 7.0665\n",
      "Epoch 492/1000: Train Loss = 7.1081, Test Loss = 7.1409\n",
      "Epoch 493/1000: Train Loss = 7.0625, Test Loss = 7.0617\n",
      "Epoch 494/1000: Train Loss = 7.1034, Test Loss = 7.1363\n",
      "Epoch 495/1000: Train Loss = 7.0577, Test Loss = 7.0569\n",
      "Epoch 496/1000: Train Loss = 7.0988, Test Loss = 7.1318\n",
      "Epoch 497/1000: Train Loss = 7.0528, Test Loss = 7.0522\n",
      "Epoch 498/1000: Train Loss = 7.0942, Test Loss = 7.1273\n",
      "Epoch 499/1000: Train Loss = 7.0481, Test Loss = 7.0474\n",
      "Epoch 500/1000: Train Loss = 7.0896, Test Loss = 7.1228\n",
      "Epoch 501/1000: Train Loss = 7.0433, Test Loss = 7.0427\n",
      "Epoch 502/1000: Train Loss = 7.0851, Test Loss = 7.1183\n",
      "Epoch 503/1000: Train Loss = 7.0386, Test Loss = 7.0380\n",
      "Epoch 504/1000: Train Loss = 7.0805, Test Loss = 7.1138\n",
      "Epoch 505/1000: Train Loss = 7.0339, Test Loss = 7.0333\n",
      "Epoch 506/1000: Train Loss = 7.0760, Test Loss = 7.1094\n",
      "Epoch 507/1000: Train Loss = 7.0292, Test Loss = 7.0287\n",
      "Epoch 508/1000: Train Loss = 7.0715, Test Loss = 7.1050\n",
      "Epoch 509/1000: Train Loss = 7.0245, Test Loss = 7.0241\n",
      "Epoch 510/1000: Train Loss = 7.0671, Test Loss = 7.1006\n",
      "Epoch 511/1000: Train Loss = 7.0199, Test Loss = 7.0195\n",
      "Epoch 512/1000: Train Loss = 7.0626, Test Loss = 7.0963\n",
      "Epoch 513/1000: Train Loss = 7.0153, Test Loss = 7.0149\n",
      "Epoch 514/1000: Train Loss = 7.0582, Test Loss = 7.0919\n",
      "Epoch 515/1000: Train Loss = 7.0107, Test Loss = 7.0104\n",
      "Epoch 516/1000: Train Loss = 7.0538, Test Loss = 7.0876\n",
      "Epoch 517/1000: Train Loss = 7.0061, Test Loss = 7.0059\n",
      "Epoch 518/1000: Train Loss = 7.0494, Test Loss = 7.0833\n",
      "Epoch 519/1000: Train Loss = 7.0016, Test Loss = 7.0014\n",
      "Epoch 520/1000: Train Loss = 7.0451, Test Loss = 7.0791\n",
      "Epoch 521/1000: Train Loss = 6.9970, Test Loss = 6.9969\n",
      "Epoch 522/1000: Train Loss = 7.0407, Test Loss = 7.0748\n",
      "Epoch 523/1000: Train Loss = 6.9925, Test Loss = 6.9924\n",
      "Epoch 524/1000: Train Loss = 7.0364, Test Loss = 7.0706\n",
      "Epoch 525/1000: Train Loss = 6.9880, Test Loss = 6.9880\n",
      "Epoch 526/1000: Train Loss = 7.0321, Test Loss = 7.0664\n",
      "Epoch 527/1000: Train Loss = 6.9836, Test Loss = 6.9836\n",
      "Epoch 528/1000: Train Loss = 7.0279, Test Loss = 7.0622\n",
      "Epoch 529/1000: Train Loss = 6.9791, Test Loss = 6.9792\n",
      "Epoch 530/1000: Train Loss = 7.0236, Test Loss = 7.0581\n",
      "Epoch 531/1000: Train Loss = 6.9747, Test Loss = 6.9748\n",
      "Epoch 532/1000: Train Loss = 7.0194, Test Loss = 7.0539\n",
      "Epoch 533/1000: Train Loss = 6.9703, Test Loss = 6.9705\n",
      "Epoch 534/1000: Train Loss = 7.0152, Test Loss = 7.0498\n",
      "Epoch 535/1000: Train Loss = 6.9659, Test Loss = 6.9661\n",
      "Epoch 536/1000: Train Loss = 7.0110, Test Loss = 7.0457\n",
      "Epoch 537/1000: Train Loss = 6.9616, Test Loss = 6.9618\n",
      "Epoch 538/1000: Train Loss = 7.0068, Test Loss = 7.0416\n",
      "Epoch 539/1000: Train Loss = 6.9572, Test Loss = 6.9575\n",
      "Epoch 540/1000: Train Loss = 7.0027, Test Loss = 7.0376\n",
      "Epoch 541/1000: Train Loss = 6.9529, Test Loss = 6.9533\n",
      "Epoch 542/1000: Train Loss = 6.9985, Test Loss = 7.0335\n",
      "Epoch 543/1000: Train Loss = 6.9486, Test Loss = 6.9490\n",
      "Epoch 544/1000: Train Loss = 6.9944, Test Loss = 7.0295\n",
      "Epoch 545/1000: Train Loss = 6.9443, Test Loss = 6.9448\n",
      "Epoch 546/1000: Train Loss = 6.9903, Test Loss = 7.0255\n",
      "Epoch 547/1000: Train Loss = 6.9400, Test Loss = 6.9406\n",
      "Epoch 548/1000: Train Loss = 6.9863, Test Loss = 7.0216\n",
      "Epoch 549/1000: Train Loss = 6.9358, Test Loss = 6.9364\n",
      "Epoch 550/1000: Train Loss = 6.9822, Test Loss = 7.0176\n",
      "Epoch 551/1000: Train Loss = 6.9316, Test Loss = 6.9322\n",
      "Epoch 552/1000: Train Loss = 6.9782, Test Loss = 7.0137\n",
      "Epoch 553/1000: Train Loss = 6.9274, Test Loss = 6.9281\n",
      "Epoch 554/1000: Train Loss = 6.9742, Test Loss = 7.0097\n",
      "Epoch 555/1000: Train Loss = 6.9232, Test Loss = 6.9239\n",
      "Epoch 556/1000: Train Loss = 6.9702, Test Loss = 7.0058\n",
      "Epoch 557/1000: Train Loss = 6.9190, Test Loss = 6.9198\n",
      "Epoch 558/1000: Train Loss = 6.9662, Test Loss = 7.0020\n",
      "Epoch 559/1000: Train Loss = 6.9149, Test Loss = 6.9157\n",
      "Epoch 560/1000: Train Loss = 6.9622, Test Loss = 6.9981\n",
      "Epoch 561/1000: Train Loss = 6.9107, Test Loss = 6.9117\n",
      "Epoch 562/1000: Train Loss = 6.9583, Test Loss = 6.9943\n",
      "Epoch 563/1000: Train Loss = 6.9066, Test Loss = 6.9076\n",
      "Epoch 564/1000: Train Loss = 6.9544, Test Loss = 6.9904\n",
      "Epoch 565/1000: Train Loss = 6.9025, Test Loss = 6.9036\n",
      "Epoch 566/1000: Train Loss = 6.9505, Test Loss = 6.9866\n",
      "Epoch 567/1000: Train Loss = 6.8984, Test Loss = 6.8995\n",
      "Epoch 568/1000: Train Loss = 6.9466, Test Loss = 6.9829\n",
      "Epoch 569/1000: Train Loss = 6.8944, Test Loss = 6.8955\n",
      "Epoch 570/1000: Train Loss = 6.9427, Test Loss = 6.9791\n",
      "Epoch 571/1000: Train Loss = 6.8903, Test Loss = 6.8916\n",
      "Epoch 572/1000: Train Loss = 6.9388, Test Loss = 6.9754\n",
      "Epoch 573/1000: Train Loss = 6.8863, Test Loss = 6.8876\n",
      "Epoch 574/1000: Train Loss = 6.9350, Test Loss = 6.9716\n",
      "Epoch 575/1000: Train Loss = 6.8823, Test Loss = 6.8836\n",
      "Epoch 576/1000: Train Loss = 6.9312, Test Loss = 6.9679\n",
      "Epoch 577/1000: Train Loss = 6.8783, Test Loss = 6.8797\n",
      "Epoch 578/1000: Train Loss = 6.9274, Test Loss = 6.9642\n",
      "Epoch 579/1000: Train Loss = 6.8743, Test Loss = 6.8758\n",
      "Epoch 580/1000: Train Loss = 6.9236, Test Loss = 6.9605\n",
      "Epoch 581/1000: Train Loss = 6.8704, Test Loss = 6.8719\n",
      "Epoch 582/1000: Train Loss = 6.9198, Test Loss = 6.9569\n",
      "Epoch 583/1000: Train Loss = 6.8665, Test Loss = 6.8680\n",
      "Epoch 584/1000: Train Loss = 6.9161, Test Loss = 6.9532\n",
      "Epoch 585/1000: Train Loss = 6.8625, Test Loss = 6.8642\n",
      "Epoch 586/1000: Train Loss = 6.9123, Test Loss = 6.9496\n",
      "Epoch 587/1000: Train Loss = 6.8586, Test Loss = 6.8603\n",
      "Epoch 588/1000: Train Loss = 6.9086, Test Loss = 6.9460\n",
      "Epoch 589/1000: Train Loss = 6.8547, Test Loss = 6.8565\n",
      "Epoch 590/1000: Train Loss = 6.9049, Test Loss = 6.9424\n",
      "Epoch 591/1000: Train Loss = 6.8508, Test Loss = 6.8527\n",
      "Epoch 592/1000: Train Loss = 6.9012, Test Loss = 6.9388\n",
      "Epoch 593/1000: Train Loss = 6.8470, Test Loss = 6.8489\n",
      "Epoch 594/1000: Train Loss = 6.8975, Test Loss = 6.9353\n",
      "Epoch 595/1000: Train Loss = 6.8431, Test Loss = 6.8451\n",
      "Epoch 596/1000: Train Loss = 6.8939, Test Loss = 6.9317\n",
      "Epoch 597/1000: Train Loss = 6.8393, Test Loss = 6.8413\n",
      "Epoch 598/1000: Train Loss = 6.8902, Test Loss = 6.9282\n",
      "Epoch 599/1000: Train Loss = 6.8355, Test Loss = 6.8375\n",
      "Epoch 600/1000: Train Loss = 6.8866, Test Loss = 6.9247\n",
      "Epoch 601/1000: Train Loss = 6.8317, Test Loss = 6.8338\n",
      "Epoch 602/1000: Train Loss = 6.8830, Test Loss = 6.9212\n",
      "Epoch 603/1000: Train Loss = 6.8279, Test Loss = 6.8301\n",
      "Epoch 604/1000: Train Loss = 6.8794, Test Loss = 6.9177\n",
      "Epoch 605/1000: Train Loss = 6.8242, Test Loss = 6.8264\n",
      "Epoch 606/1000: Train Loss = 6.8758, Test Loss = 6.9143\n",
      "Epoch 607/1000: Train Loss = 6.8204, Test Loss = 6.8227\n",
      "Epoch 608/1000: Train Loss = 6.8723, Test Loss = 6.9108\n",
      "Epoch 609/1000: Train Loss = 6.8167, Test Loss = 6.8190\n",
      "Epoch 610/1000: Train Loss = 6.8687, Test Loss = 6.9074\n",
      "Epoch 611/1000: Train Loss = 6.8129, Test Loss = 6.8153\n",
      "Epoch 612/1000: Train Loss = 6.8652, Test Loss = 6.9039\n",
      "Epoch 613/1000: Train Loss = 6.8092, Test Loss = 6.8117\n",
      "Epoch 614/1000: Train Loss = 6.8616, Test Loss = 6.9005\n",
      "Epoch 615/1000: Train Loss = 6.8055, Test Loss = 6.8081\n",
      "Epoch 616/1000: Train Loss = 6.8581, Test Loss = 6.8971\n",
      "Epoch 617/1000: Train Loss = 6.8018, Test Loss = 6.8044\n",
      "Epoch 618/1000: Train Loss = 6.8546, Test Loss = 6.8938\n",
      "Epoch 619/1000: Train Loss = 6.7982, Test Loss = 6.8008\n",
      "Epoch 620/1000: Train Loss = 6.8512, Test Loss = 6.8904\n",
      "Epoch 621/1000: Train Loss = 6.7945, Test Loss = 6.7972\n",
      "Epoch 622/1000: Train Loss = 6.8477, Test Loss = 6.8871\n",
      "Epoch 623/1000: Train Loss = 6.7909, Test Loss = 6.7937\n",
      "Epoch 624/1000: Train Loss = 6.8442, Test Loss = 6.8837\n",
      "Epoch 625/1000: Train Loss = 6.7873, Test Loss = 6.7901\n",
      "Epoch 626/1000: Train Loss = 6.8408, Test Loss = 6.8804\n",
      "Epoch 627/1000: Train Loss = 6.7836, Test Loss = 6.7865\n",
      "Epoch 628/1000: Train Loss = 6.8374, Test Loss = 6.8771\n",
      "Epoch 629/1000: Train Loss = 6.7800, Test Loss = 6.7830\n",
      "Epoch 630/1000: Train Loss = 6.8339, Test Loss = 6.8738\n",
      "Epoch 631/1000: Train Loss = 6.7765, Test Loss = 6.7795\n",
      "Epoch 632/1000: Train Loss = 6.8305, Test Loss = 6.8705\n",
      "Epoch 633/1000: Train Loss = 6.7729, Test Loss = 6.7760\n",
      "Epoch 634/1000: Train Loss = 6.8272, Test Loss = 6.8673\n",
      "Epoch 635/1000: Train Loss = 6.7693, Test Loss = 6.7725\n",
      "Epoch 636/1000: Train Loss = 6.8238, Test Loss = 6.8640\n",
      "Epoch 637/1000: Train Loss = 6.7658, Test Loss = 6.7690\n",
      "Epoch 638/1000: Train Loss = 6.8204, Test Loss = 6.8608\n",
      "Epoch 639/1000: Train Loss = 6.7622, Test Loss = 6.7655\n",
      "Epoch 640/1000: Train Loss = 6.8171, Test Loss = 6.8576\n",
      "Epoch 641/1000: Train Loss = 6.7587, Test Loss = 6.7621\n",
      "Epoch 642/1000: Train Loss = 6.8137, Test Loss = 6.8544\n",
      "Epoch 643/1000: Train Loss = 6.7552, Test Loss = 6.7586\n",
      "Epoch 644/1000: Train Loss = 6.8104, Test Loss = 6.8512\n",
      "Epoch 645/1000: Train Loss = 6.7517, Test Loss = 6.7552\n",
      "Epoch 646/1000: Train Loss = 6.8071, Test Loss = 6.8480\n",
      "Epoch 647/1000: Train Loss = 6.7482, Test Loss = 6.7518\n",
      "Epoch 648/1000: Train Loss = 6.8038, Test Loss = 6.8448\n",
      "Epoch 649/1000: Train Loss = 6.7448, Test Loss = 6.7484\n",
      "Epoch 650/1000: Train Loss = 6.8005, Test Loss = 6.8417\n",
      "Epoch 651/1000: Train Loss = 6.7413, Test Loss = 6.7450\n",
      "Epoch 652/1000: Train Loss = 6.7972, Test Loss = 6.8385\n",
      "Epoch 653/1000: Train Loss = 6.7379, Test Loss = 6.7416\n",
      "Epoch 654/1000: Train Loss = 6.7940, Test Loss = 6.8354\n",
      "Epoch 655/1000: Train Loss = 6.7344, Test Loss = 6.7383\n",
      "Epoch 656/1000: Train Loss = 6.7907, Test Loss = 6.8323\n",
      "Epoch 657/1000: Train Loss = 6.7310, Test Loss = 6.7349\n",
      "Epoch 658/1000: Train Loss = 6.7875, Test Loss = 6.8292\n",
      "Epoch 659/1000: Train Loss = 6.7276, Test Loss = 6.7316\n",
      "Epoch 660/1000: Train Loss = 6.7843, Test Loss = 6.8261\n",
      "Epoch 661/1000: Train Loss = 6.7242, Test Loss = 6.7282\n",
      "Epoch 662/1000: Train Loss = 6.7810, Test Loss = 6.8230\n",
      "Epoch 663/1000: Train Loss = 6.7208, Test Loss = 6.7249\n",
      "Epoch 664/1000: Train Loss = 6.7778, Test Loss = 6.8199\n",
      "Epoch 665/1000: Train Loss = 6.7175, Test Loss = 6.7216\n",
      "Epoch 666/1000: Train Loss = 6.7746, Test Loss = 6.8168\n",
      "Epoch 667/1000: Train Loss = 6.7141, Test Loss = 6.7183\n",
      "Epoch 668/1000: Train Loss = 6.7715, Test Loss = 6.8138\n",
      "Epoch 669/1000: Train Loss = 6.7107, Test Loss = 6.7150\n",
      "Epoch 670/1000: Train Loss = 6.7683, Test Loss = 6.8107\n",
      "Epoch 671/1000: Train Loss = 6.7074, Test Loss = 6.7118\n",
      "Epoch 672/1000: Train Loss = 6.7651, Test Loss = 6.8077\n",
      "Epoch 673/1000: Train Loss = 6.7041, Test Loss = 6.7085\n",
      "Epoch 674/1000: Train Loss = 6.7620, Test Loss = 6.8047\n",
      "Epoch 675/1000: Train Loss = 6.7008, Test Loss = 6.7053\n",
      "Epoch 676/1000: Train Loss = 6.7589, Test Loss = 6.8017\n",
      "Epoch 677/1000: Train Loss = 6.6975, Test Loss = 6.7020\n",
      "Epoch 678/1000: Train Loss = 6.7557, Test Loss = 6.7987\n",
      "Epoch 679/1000: Train Loss = 6.6942, Test Loss = 6.6988\n",
      "Epoch 680/1000: Train Loss = 6.7526, Test Loss = 6.7957\n",
      "Epoch 681/1000: Train Loss = 6.6909, Test Loss = 6.6956\n",
      "Epoch 682/1000: Train Loss = 6.7495, Test Loss = 6.7927\n",
      "Epoch 683/1000: Train Loss = 6.6876, Test Loss = 6.6924\n",
      "Epoch 684/1000: Train Loss = 6.7464, Test Loss = 6.7897\n",
      "Epoch 685/1000: Train Loss = 6.6844, Test Loss = 6.6892\n",
      "Epoch 686/1000: Train Loss = 6.7433, Test Loss = 6.7868\n",
      "Epoch 687/1000: Train Loss = 6.6811, Test Loss = 6.6860\n",
      "Epoch 688/1000: Train Loss = 6.7403, Test Loss = 6.7838\n",
      "Epoch 689/1000: Train Loss = 6.6779, Test Loss = 6.6829\n",
      "Epoch 690/1000: Train Loss = 6.7372, Test Loss = 6.7809\n",
      "Epoch 691/1000: Train Loss = 6.6747, Test Loss = 6.6797\n",
      "Epoch 692/1000: Train Loss = 6.7342, Test Loss = 6.7780\n",
      "Epoch 693/1000: Train Loss = 6.6714, Test Loss = 6.6766\n",
      "Epoch 694/1000: Train Loss = 6.7311, Test Loss = 6.7751\n",
      "Epoch 695/1000: Train Loss = 6.6682, Test Loss = 6.6734\n",
      "Epoch 696/1000: Train Loss = 6.7281, Test Loss = 6.7722\n",
      "Epoch 697/1000: Train Loss = 6.6650, Test Loss = 6.6703\n",
      "Epoch 698/1000: Train Loss = 6.7251, Test Loss = 6.7693\n",
      "Epoch 699/1000: Train Loss = 6.6618, Test Loss = 6.6672\n",
      "Epoch 700/1000: Train Loss = 6.7220, Test Loss = 6.7664\n",
      "Epoch 701/1000: Train Loss = 6.6587, Test Loss = 6.6641\n",
      "Epoch 702/1000: Train Loss = 6.7190, Test Loss = 6.7635\n",
      "Epoch 703/1000: Train Loss = 6.6555, Test Loss = 6.6610\n",
      "Epoch 704/1000: Train Loss = 6.7161, Test Loss = 6.7607\n",
      "Epoch 705/1000: Train Loss = 6.6524, Test Loss = 6.6579\n",
      "Epoch 706/1000: Train Loss = 6.7131, Test Loss = 6.7578\n",
      "Epoch 707/1000: Train Loss = 6.6492, Test Loss = 6.6549\n",
      "Epoch 708/1000: Train Loss = 6.7101, Test Loss = 6.7550\n",
      "Epoch 709/1000: Train Loss = 6.6461, Test Loss = 6.6518\n",
      "Epoch 710/1000: Train Loss = 6.7071, Test Loss = 6.7522\n",
      "Epoch 711/1000: Train Loss = 6.6430, Test Loss = 6.6487\n",
      "Epoch 712/1000: Train Loss = 6.7042, Test Loss = 6.7493\n",
      "Epoch 713/1000: Train Loss = 6.6398, Test Loss = 6.6457\n",
      "Epoch 714/1000: Train Loss = 6.7012, Test Loss = 6.7465\n",
      "Epoch 715/1000: Train Loss = 6.6367, Test Loss = 6.6427\n",
      "Epoch 716/1000: Train Loss = 6.6983, Test Loss = 6.7437\n",
      "Epoch 717/1000: Train Loss = 6.6336, Test Loss = 6.6396\n",
      "Epoch 718/1000: Train Loss = 6.6954, Test Loss = 6.7409\n",
      "Epoch 719/1000: Train Loss = 6.6306, Test Loss = 6.6366\n",
      "Epoch 720/1000: Train Loss = 6.6925, Test Loss = 6.7381\n",
      "Epoch 721/1000: Train Loss = 6.6275, Test Loss = 6.6336\n",
      "Epoch 722/1000: Train Loss = 6.6896, Test Loss = 6.7354\n",
      "Epoch 723/1000: Train Loss = 6.6244, Test Loss = 6.6306\n",
      "Epoch 724/1000: Train Loss = 6.6867, Test Loss = 6.7326\n",
      "Epoch 725/1000: Train Loss = 6.6214, Test Loss = 6.6277\n",
      "Epoch 726/1000: Train Loss = 6.6838, Test Loss = 6.7299\n",
      "Epoch 727/1000: Train Loss = 6.6183, Test Loss = 6.6247\n",
      "Epoch 728/1000: Train Loss = 6.6809, Test Loss = 6.7271\n",
      "Epoch 729/1000: Train Loss = 6.6153, Test Loss = 6.6217\n",
      "Epoch 730/1000: Train Loss = 6.6780, Test Loss = 6.7244\n",
      "Epoch 731/1000: Train Loss = 6.6122, Test Loss = 6.6188\n",
      "Epoch 732/1000: Train Loss = 6.6752, Test Loss = 6.7217\n",
      "Epoch 733/1000: Train Loss = 6.6092, Test Loss = 6.6158\n",
      "Epoch 734/1000: Train Loss = 6.6723, Test Loss = 6.7189\n",
      "Epoch 735/1000: Train Loss = 6.6062, Test Loss = 6.6129\n",
      "Epoch 736/1000: Train Loss = 6.6695, Test Loss = 6.7162\n",
      "Epoch 737/1000: Train Loss = 6.6032, Test Loss = 6.6100\n",
      "Epoch 738/1000: Train Loss = 6.6667, Test Loss = 6.7135\n",
      "Epoch 739/1000: Train Loss = 6.6002, Test Loss = 6.6070\n",
      "Epoch 740/1000: Train Loss = 6.6638, Test Loss = 6.7108\n",
      "Epoch 741/1000: Train Loss = 6.5972, Test Loss = 6.6041\n",
      "Epoch 742/1000: Train Loss = 6.6610, Test Loss = 6.7082\n",
      "Epoch 743/1000: Train Loss = 6.5943, Test Loss = 6.6012\n",
      "Epoch 744/1000: Train Loss = 6.6582, Test Loss = 6.7055\n",
      "Epoch 745/1000: Train Loss = 6.5913, Test Loss = 6.5983\n",
      "Epoch 746/1000: Train Loss = 6.6554, Test Loss = 6.7028\n",
      "Epoch 747/1000: Train Loss = 6.5884, Test Loss = 6.5955\n",
      "Epoch 748/1000: Train Loss = 6.6526, Test Loss = 6.7002\n",
      "Epoch 749/1000: Train Loss = 6.5854, Test Loss = 6.5926\n",
      "Epoch 750/1000: Train Loss = 6.6498, Test Loss = 6.6975\n",
      "Epoch 751/1000: Train Loss = 6.5825, Test Loss = 6.5897\n",
      "Epoch 752/1000: Train Loss = 6.6471, Test Loss = 6.6949\n",
      "Epoch 753/1000: Train Loss = 6.5795, Test Loss = 6.5869\n",
      "Epoch 754/1000: Train Loss = 6.6443, Test Loss = 6.6922\n",
      "Epoch 755/1000: Train Loss = 6.5766, Test Loss = 6.5840\n",
      "Epoch 756/1000: Train Loss = 6.6415, Test Loss = 6.6896\n",
      "Epoch 757/1000: Train Loss = 6.5737, Test Loss = 6.5812\n",
      "Epoch 758/1000: Train Loss = 6.6388, Test Loss = 6.6870\n",
      "Epoch 759/1000: Train Loss = 6.5708, Test Loss = 6.5784\n",
      "Epoch 760/1000: Train Loss = 6.6361, Test Loss = 6.6844\n",
      "Epoch 761/1000: Train Loss = 6.5679, Test Loss = 6.5756\n",
      "Epoch 762/1000: Train Loss = 6.6333, Test Loss = 6.6818\n",
      "Epoch 763/1000: Train Loss = 6.5650, Test Loss = 6.5728\n",
      "Epoch 764/1000: Train Loss = 6.6306, Test Loss = 6.6792\n",
      "Epoch 765/1000: Train Loss = 6.5622, Test Loss = 6.5700\n",
      "Epoch 766/1000: Train Loss = 6.6279, Test Loss = 6.6766\n",
      "Epoch 767/1000: Train Loss = 6.5593, Test Loss = 6.5672\n",
      "Epoch 768/1000: Train Loss = 6.6252, Test Loss = 6.6741\n",
      "Epoch 769/1000: Train Loss = 6.5564, Test Loss = 6.5644\n",
      "Epoch 770/1000: Train Loss = 6.6225, Test Loss = 6.6715\n",
      "Epoch 771/1000: Train Loss = 6.5536, Test Loss = 6.5616\n",
      "Epoch 772/1000: Train Loss = 6.6198, Test Loss = 6.6690\n",
      "Epoch 773/1000: Train Loss = 6.5508, Test Loss = 6.5588\n",
      "Epoch 774/1000: Train Loss = 6.6171, Test Loss = 6.6664\n",
      "Epoch 775/1000: Train Loss = 6.5479, Test Loss = 6.5561\n",
      "Epoch 776/1000: Train Loss = 6.6144, Test Loss = 6.6639\n",
      "Epoch 777/1000: Train Loss = 6.5451, Test Loss = 6.5533\n",
      "Epoch 778/1000: Train Loss = 6.6118, Test Loss = 6.6613\n",
      "Epoch 779/1000: Train Loss = 6.5423, Test Loss = 6.5506\n",
      "Epoch 780/1000: Train Loss = 6.6091, Test Loss = 6.6588\n",
      "Epoch 781/1000: Train Loss = 6.5395, Test Loss = 6.5479\n",
      "Epoch 782/1000: Train Loss = 6.6064, Test Loss = 6.6563\n",
      "Epoch 783/1000: Train Loss = 6.5367, Test Loss = 6.5451\n",
      "Epoch 784/1000: Train Loss = 6.6038, Test Loss = 6.6538\n",
      "Epoch 785/1000: Train Loss = 6.5339, Test Loss = 6.5424\n",
      "Epoch 786/1000: Train Loss = 6.6012, Test Loss = 6.6513\n",
      "Epoch 787/1000: Train Loss = 6.5311, Test Loss = 6.5397\n",
      "Epoch 788/1000: Train Loss = 6.5985, Test Loss = 6.6488\n",
      "Epoch 789/1000: Train Loss = 6.5283, Test Loss = 6.5370\n",
      "Epoch 790/1000: Train Loss = 6.5959, Test Loss = 6.6463\n",
      "Epoch 791/1000: Train Loss = 6.5255, Test Loss = 6.5343\n",
      "Epoch 792/1000: Train Loss = 6.5933, Test Loss = 6.6438\n",
      "Epoch 793/1000: Train Loss = 6.5228, Test Loss = 6.5316\n",
      "Epoch 794/1000: Train Loss = 6.5907, Test Loss = 6.6413\n",
      "Epoch 795/1000: Train Loss = 6.5200, Test Loss = 6.5289\n",
      "Epoch 796/1000: Train Loss = 6.5881, Test Loss = 6.6389\n",
      "Epoch 797/1000: Train Loss = 6.5173, Test Loss = 6.5263\n",
      "Epoch 798/1000: Train Loss = 6.5855, Test Loss = 6.6364\n",
      "Epoch 799/1000: Train Loss = 6.5145, Test Loss = 6.5236\n",
      "Epoch 800/1000: Train Loss = 6.5829, Test Loss = 6.6340\n",
      "Epoch 801/1000: Train Loss = 6.5118, Test Loss = 6.5209\n",
      "Epoch 802/1000: Train Loss = 6.5803, Test Loss = 6.6315\n",
      "Epoch 803/1000: Train Loss = 6.5091, Test Loss = 6.5183\n",
      "Epoch 804/1000: Train Loss = 6.5778, Test Loss = 6.6291\n",
      "Epoch 805/1000: Train Loss = 6.5064, Test Loss = 6.5157\n",
      "Epoch 806/1000: Train Loss = 6.5752, Test Loss = 6.6266\n",
      "Epoch 807/1000: Train Loss = 6.5037, Test Loss = 6.5130\n",
      "Epoch 808/1000: Train Loss = 6.5726, Test Loss = 6.6242\n",
      "Epoch 809/1000: Train Loss = 6.5010, Test Loss = 6.5104\n",
      "Epoch 810/1000: Train Loss = 6.5701, Test Loss = 6.6218\n",
      "Epoch 811/1000: Train Loss = 6.4983, Test Loss = 6.5078\n",
      "Epoch 812/1000: Train Loss = 6.5675, Test Loss = 6.6194\n",
      "Epoch 813/1000: Train Loss = 6.4956, Test Loss = 6.5052\n",
      "Epoch 814/1000: Train Loss = 6.5650, Test Loss = 6.6170\n",
      "Epoch 815/1000: Train Loss = 6.4929, Test Loss = 6.5026\n",
      "Epoch 816/1000: Train Loss = 6.5625, Test Loss = 6.6146\n",
      "Epoch 817/1000: Train Loss = 6.4902, Test Loss = 6.5000\n",
      "Epoch 818/1000: Train Loss = 6.5599, Test Loss = 6.6122\n",
      "Epoch 819/1000: Train Loss = 6.4876, Test Loss = 6.4974\n",
      "Epoch 820/1000: Train Loss = 6.5574, Test Loss = 6.6098\n",
      "Epoch 821/1000: Train Loss = 6.4849, Test Loss = 6.4948\n",
      "Epoch 822/1000: Train Loss = 6.5549, Test Loss = 6.6074\n",
      "Epoch 823/1000: Train Loss = 6.4823, Test Loss = 6.4922\n",
      "Epoch 824/1000: Train Loss = 6.5524, Test Loss = 6.6051\n",
      "Epoch 825/1000: Train Loss = 6.4796, Test Loss = 6.4897\n",
      "Epoch 826/1000: Train Loss = 6.5499, Test Loss = 6.6027\n",
      "Epoch 827/1000: Train Loss = 6.4770, Test Loss = 6.4871\n",
      "Epoch 828/1000: Train Loss = 6.5474, Test Loss = 6.6004\n",
      "Epoch 829/1000: Train Loss = 6.4744, Test Loss = 6.4846\n",
      "Epoch 830/1000: Train Loss = 6.5450, Test Loss = 6.5980\n",
      "Epoch 831/1000: Train Loss = 6.4718, Test Loss = 6.4820\n",
      "Epoch 832/1000: Train Loss = 6.5425, Test Loss = 6.5957\n",
      "Epoch 833/1000: Train Loss = 6.4691, Test Loss = 6.4795\n",
      "Epoch 834/1000: Train Loss = 6.5400, Test Loss = 6.5933\n",
      "Epoch 835/1000: Train Loss = 6.4665, Test Loss = 6.4769\n",
      "Epoch 836/1000: Train Loss = 6.5376, Test Loss = 6.5910\n",
      "Epoch 837/1000: Train Loss = 6.4639, Test Loss = 6.4744\n",
      "Epoch 838/1000: Train Loss = 6.5351, Test Loss = 6.5887\n",
      "Epoch 839/1000: Train Loss = 6.4614, Test Loss = 6.4719\n",
      "Epoch 840/1000: Train Loss = 6.5326, Test Loss = 6.5864\n",
      "Epoch 841/1000: Train Loss = 6.4588, Test Loss = 6.4694\n",
      "Epoch 842/1000: Train Loss = 6.5302, Test Loss = 6.5841\n",
      "Epoch 843/1000: Train Loss = 6.4562, Test Loss = 6.4669\n",
      "Epoch 844/1000: Train Loss = 6.5278, Test Loss = 6.5817\n",
      "Epoch 845/1000: Train Loss = 6.4536, Test Loss = 6.4644\n",
      "Epoch 846/1000: Train Loss = 6.5253, Test Loss = 6.5795\n",
      "Epoch 847/1000: Train Loss = 6.4511, Test Loss = 6.4619\n",
      "Epoch 848/1000: Train Loss = 6.5229, Test Loss = 6.5772\n",
      "Epoch 849/1000: Train Loss = 6.4485, Test Loss = 6.4594\n",
      "Epoch 850/1000: Train Loss = 6.5205, Test Loss = 6.5749\n",
      "Epoch 851/1000: Train Loss = 6.4460, Test Loss = 6.4570\n",
      "Epoch 852/1000: Train Loss = 6.5181, Test Loss = 6.5726\n",
      "Epoch 853/1000: Train Loss = 6.4434, Test Loss = 6.4545\n",
      "Epoch 854/1000: Train Loss = 6.5157, Test Loss = 6.5703\n",
      "Epoch 855/1000: Train Loss = 6.4409, Test Loss = 6.4520\n",
      "Epoch 856/1000: Train Loss = 6.5133, Test Loss = 6.5681\n",
      "Epoch 857/1000: Train Loss = 6.4384, Test Loss = 6.4496\n",
      "Epoch 858/1000: Train Loss = 6.5109, Test Loss = 6.5658\n",
      "Epoch 859/1000: Train Loss = 6.4358, Test Loss = 6.4471\n",
      "Epoch 860/1000: Train Loss = 6.5085, Test Loss = 6.5636\n",
      "Epoch 861/1000: Train Loss = 6.4333, Test Loss = 6.4447\n",
      "Epoch 862/1000: Train Loss = 6.5061, Test Loss = 6.5613\n",
      "Epoch 863/1000: Train Loss = 6.4308, Test Loss = 6.4423\n",
      "Epoch 864/1000: Train Loss = 6.5038, Test Loss = 6.5591\n",
      "Epoch 865/1000: Train Loss = 6.4283, Test Loss = 6.4399\n",
      "Epoch 866/1000: Train Loss = 6.5014, Test Loss = 6.5568\n",
      "Epoch 867/1000: Train Loss = 6.4258, Test Loss = 6.4374\n",
      "Epoch 868/1000: Train Loss = 6.4990, Test Loss = 6.5546\n",
      "Epoch 869/1000: Train Loss = 6.4233, Test Loss = 6.4350\n",
      "Epoch 870/1000: Train Loss = 6.4967, Test Loss = 6.5524\n",
      "Epoch 871/1000: Train Loss = 6.4208, Test Loss = 6.4326\n",
      "Epoch 872/1000: Train Loss = 6.4943, Test Loss = 6.5502\n",
      "Epoch 873/1000: Train Loss = 6.4184, Test Loss = 6.4302\n",
      "Epoch 874/1000: Train Loss = 6.4920, Test Loss = 6.5480\n",
      "Epoch 875/1000: Train Loss = 6.4159, Test Loss = 6.4278\n",
      "Epoch 876/1000: Train Loss = 6.4897, Test Loss = 6.5458\n",
      "Epoch 877/1000: Train Loss = 6.4134, Test Loss = 6.4254\n",
      "Epoch 878/1000: Train Loss = 6.4873, Test Loss = 6.5436\n",
      "Epoch 879/1000: Train Loss = 6.4110, Test Loss = 6.4231\n",
      "Epoch 880/1000: Train Loss = 6.4850, Test Loss = 6.5414\n",
      "Epoch 881/1000: Train Loss = 6.4085, Test Loss = 6.4207\n",
      "Epoch 882/1000: Train Loss = 6.4827, Test Loss = 6.5392\n",
      "Epoch 883/1000: Train Loss = 6.4061, Test Loss = 6.4183\n",
      "Epoch 884/1000: Train Loss = 6.4804, Test Loss = 6.5370\n",
      "Epoch 885/1000: Train Loss = 6.4036, Test Loss = 6.4160\n",
      "Epoch 886/1000: Train Loss = 6.4781, Test Loss = 6.5348\n",
      "Epoch 887/1000: Train Loss = 6.4012, Test Loss = 6.4136\n",
      "Epoch 888/1000: Train Loss = 6.4758, Test Loss = 6.5327\n",
      "Epoch 889/1000: Train Loss = 6.3988, Test Loss = 6.4113\n",
      "Epoch 890/1000: Train Loss = 6.4735, Test Loss = 6.5305\n",
      "Epoch 891/1000: Train Loss = 6.3964, Test Loss = 6.4089\n",
      "Epoch 892/1000: Train Loss = 6.4712, Test Loss = 6.5283\n",
      "Epoch 893/1000: Train Loss = 6.3939, Test Loss = 6.4066\n",
      "Epoch 894/1000: Train Loss = 6.4689, Test Loss = 6.5262\n",
      "Epoch 895/1000: Train Loss = 6.3915, Test Loss = 6.4043\n",
      "Epoch 896/1000: Train Loss = 6.4666, Test Loss = 6.5240\n",
      "Epoch 897/1000: Train Loss = 6.3891, Test Loss = 6.4019\n",
      "Epoch 898/1000: Train Loss = 6.4643, Test Loss = 6.5219\n",
      "Epoch 899/1000: Train Loss = 6.3867, Test Loss = 6.3996\n",
      "Epoch 900/1000: Train Loss = 6.4621, Test Loss = 6.5198\n",
      "Epoch 901/1000: Train Loss = 6.3843, Test Loss = 6.3973\n",
      "Epoch 902/1000: Train Loss = 6.4598, Test Loss = 6.5176\n",
      "Epoch 903/1000: Train Loss = 6.3820, Test Loss = 6.3950\n",
      "Epoch 904/1000: Train Loss = 6.4575, Test Loss = 6.5155\n",
      "Epoch 905/1000: Train Loss = 6.3796, Test Loss = 6.3927\n",
      "Epoch 906/1000: Train Loss = 6.4553, Test Loss = 6.5134\n",
      "Epoch 907/1000: Train Loss = 6.3772, Test Loss = 6.3904\n",
      "Epoch 908/1000: Train Loss = 6.4530, Test Loss = 6.5113\n",
      "Epoch 909/1000: Train Loss = 6.3748, Test Loss = 6.3881\n",
      "Epoch 910/1000: Train Loss = 6.4508, Test Loss = 6.5092\n",
      "Epoch 911/1000: Train Loss = 6.3725, Test Loss = 6.3859\n",
      "Epoch 912/1000: Train Loss = 6.4486, Test Loss = 6.5071\n",
      "Epoch 913/1000: Train Loss = 6.3701, Test Loss = 6.3836\n",
      "Epoch 914/1000: Train Loss = 6.4463, Test Loss = 6.5050\n",
      "Epoch 915/1000: Train Loss = 6.3678, Test Loss = 6.3813\n",
      "Epoch 916/1000: Train Loss = 6.4441, Test Loss = 6.5029\n",
      "Epoch 917/1000: Train Loss = 6.3654, Test Loss = 6.3791\n",
      "Epoch 918/1000: Train Loss = 6.4419, Test Loss = 6.5008\n",
      "Epoch 919/1000: Train Loss = 6.3631, Test Loss = 6.3768\n",
      "Epoch 920/1000: Train Loss = 6.4397, Test Loss = 6.4987\n",
      "Epoch 921/1000: Train Loss = 6.3608, Test Loss = 6.3745\n",
      "Epoch 922/1000: Train Loss = 6.4375, Test Loss = 6.4966\n",
      "Epoch 923/1000: Train Loss = 6.3584, Test Loss = 6.3723\n",
      "Epoch 924/1000: Train Loss = 6.4353, Test Loss = 6.4946\n",
      "Epoch 925/1000: Train Loss = 6.3561, Test Loss = 6.3701\n",
      "Epoch 926/1000: Train Loss = 6.4331, Test Loss = 6.4925\n",
      "Epoch 927/1000: Train Loss = 6.3538, Test Loss = 6.3678\n",
      "Epoch 928/1000: Train Loss = 6.4309, Test Loss = 6.4905\n",
      "Epoch 929/1000: Train Loss = 6.3515, Test Loss = 6.3656\n",
      "Epoch 930/1000: Train Loss = 6.4287, Test Loss = 6.4884\n",
      "Epoch 931/1000: Train Loss = 6.3492, Test Loss = 6.3634\n",
      "Epoch 932/1000: Train Loss = 6.4265, Test Loss = 6.4863\n",
      "Epoch 933/1000: Train Loss = 6.3469, Test Loss = 6.3612\n",
      "Epoch 934/1000: Train Loss = 6.4243, Test Loss = 6.4843\n",
      "Epoch 935/1000: Train Loss = 6.3446, Test Loss = 6.3590\n",
      "Epoch 936/1000: Train Loss = 6.4222, Test Loss = 6.4823\n",
      "Epoch 937/1000: Train Loss = 6.3423, Test Loss = 6.3568\n",
      "Epoch 938/1000: Train Loss = 6.4200, Test Loss = 6.4802\n",
      "Epoch 939/1000: Train Loss = 6.3400, Test Loss = 6.3546\n",
      "Epoch 940/1000: Train Loss = 6.4178, Test Loss = 6.4782\n",
      "Epoch 941/1000: Train Loss = 6.3378, Test Loss = 6.3524\n",
      "Epoch 942/1000: Train Loss = 6.4157, Test Loss = 6.4762\n",
      "Epoch 943/1000: Train Loss = 6.3355, Test Loss = 6.3502\n",
      "Epoch 944/1000: Train Loss = 6.4135, Test Loss = 6.4742\n",
      "Epoch 945/1000: Train Loss = 6.3332, Test Loss = 6.3480\n",
      "Epoch 946/1000: Train Loss = 6.4114, Test Loss = 6.4721\n",
      "Epoch 947/1000: Train Loss = 6.3310, Test Loss = 6.3458\n",
      "Epoch 948/1000: Train Loss = 6.4092, Test Loss = 6.4701\n",
      "Epoch 949/1000: Train Loss = 6.3287, Test Loss = 6.3436\n",
      "Epoch 950/1000: Train Loss = 6.4071, Test Loss = 6.4681\n",
      "Epoch 951/1000: Train Loss = 6.3265, Test Loss = 6.3415\n",
      "Epoch 952/1000: Train Loss = 6.4050, Test Loss = 6.4661\n",
      "Epoch 953/1000: Train Loss = 6.3242, Test Loss = 6.3393\n",
      "Epoch 954/1000: Train Loss = 6.4029, Test Loss = 6.4641\n",
      "Epoch 955/1000: Train Loss = 6.3220, Test Loss = 6.3372\n",
      "Epoch 956/1000: Train Loss = 6.4007, Test Loss = 6.4621\n",
      "Epoch 957/1000: Train Loss = 6.3198, Test Loss = 6.3350\n",
      "Epoch 958/1000: Train Loss = 6.3986, Test Loss = 6.4601\n",
      "Epoch 959/1000: Train Loss = 6.3175, Test Loss = 6.3329\n",
      "Epoch 960/1000: Train Loss = 6.3965, Test Loss = 6.4582\n",
      "Epoch 961/1000: Train Loss = 6.3153, Test Loss = 6.3307\n",
      "Epoch 962/1000: Train Loss = 6.3944, Test Loss = 6.4562\n",
      "Epoch 963/1000: Train Loss = 6.3131, Test Loss = 6.3286\n",
      "Epoch 964/1000: Train Loss = 6.3923, Test Loss = 6.4542\n",
      "Epoch 965/1000: Train Loss = 6.3109, Test Loss = 6.3265\n",
      "Epoch 966/1000: Train Loss = 6.3902, Test Loss = 6.4522\n",
      "Epoch 967/1000: Train Loss = 6.3087, Test Loss = 6.3243\n",
      "Epoch 968/1000: Train Loss = 6.3881, Test Loss = 6.4503\n",
      "Epoch 969/1000: Train Loss = 6.3065, Test Loss = 6.3222\n",
      "Epoch 970/1000: Train Loss = 6.3860, Test Loss = 6.4483\n",
      "Epoch 971/1000: Train Loss = 6.3043, Test Loss = 6.3201\n",
      "Epoch 972/1000: Train Loss = 6.3839, Test Loss = 6.4463\n",
      "Epoch 973/1000: Train Loss = 6.3021, Test Loss = 6.3180\n",
      "Epoch 974/1000: Train Loss = 6.3819, Test Loss = 6.4444\n",
      "Epoch 975/1000: Train Loss = 6.2999, Test Loss = 6.3159\n",
      "Epoch 976/1000: Train Loss = 6.3798, Test Loss = 6.4424\n",
      "Epoch 977/1000: Train Loss = 6.2977, Test Loss = 6.3138\n",
      "Epoch 978/1000: Train Loss = 6.3777, Test Loss = 6.4405\n",
      "Epoch 979/1000: Train Loss = 6.2955, Test Loss = 6.3117\n",
      "Epoch 980/1000: Train Loss = 6.3757, Test Loss = 6.4386\n",
      "Epoch 981/1000: Train Loss = 6.2934, Test Loss = 6.3096\n",
      "Epoch 982/1000: Train Loss = 6.3736, Test Loss = 6.4366\n",
      "Epoch 983/1000: Train Loss = 6.2912, Test Loss = 6.3075\n",
      "Epoch 984/1000: Train Loss = 6.3715, Test Loss = 6.4347\n",
      "Epoch 985/1000: Train Loss = 6.2890, Test Loss = 6.3055\n",
      "Epoch 986/1000: Train Loss = 6.3695, Test Loss = 6.4328\n",
      "Epoch 987/1000: Train Loss = 6.2869, Test Loss = 6.3034\n",
      "Epoch 988/1000: Train Loss = 6.3674, Test Loss = 6.4309\n",
      "Epoch 989/1000: Train Loss = 6.2847, Test Loss = 6.3013\n",
      "Epoch 990/1000: Train Loss = 6.3654, Test Loss = 6.4289\n",
      "Epoch 991/1000: Train Loss = 6.2826, Test Loss = 6.2993\n",
      "Epoch 992/1000: Train Loss = 6.3634, Test Loss = 6.4270\n",
      "Epoch 993/1000: Train Loss = 6.2804, Test Loss = 6.2972\n",
      "Epoch 994/1000: Train Loss = 6.3613, Test Loss = 6.4251\n",
      "Epoch 995/1000: Train Loss = 6.2783, Test Loss = 6.2951\n",
      "Epoch 996/1000: Train Loss = 6.3593, Test Loss = 6.4232\n",
      "Epoch 997/1000: Train Loss = 6.2761, Test Loss = 6.2931\n",
      "Epoch 998/1000: Train Loss = 6.3573, Test Loss = 6.4213\n",
      "Epoch 999/1000: Train Loss = 6.2740, Test Loss = 6.2910\n",
      "Epoch 1000/1000: Train Loss = 6.3553, Test Loss = 6.4194\n"
     ]
    }
   ],
   "source": [
    "lr = logistic_regression(learning_rate=0.1, epochs=1000)\n",
    "lr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[ 234  518  917   69   95  542   98  658 1717  152]\n",
      " [  70 1821  411  126  113  489  199  494  830  447]\n",
      " [  59  304 1583   95  208  862  355  809  643   82]\n",
      " [  65  539  810  244  198 1310  416  733  520  165]\n",
      " [  37  297 1329  103  324  894  485 1116  314  101]\n",
      " [  47  399  898  202  177 1645  324  731  479   98]\n",
      " [  23  379  982  189  261  993  987  755  290  141]\n",
      " [  49  387  834  136  239  719  308 1727  400  201]\n",
      " [ 104  639  513   54   54  393   76  352 2599  216]\n",
      " [  93 1191  434  107  100  347  214  582 1071  861]]\n",
      "Accuracy: 0.2405\n",
      "Precision: 0.2475\n",
      "Recall: 0.2405\n",
      "F1 Score: 0.2440\n",
      "\n",
      "Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[ 43  94 178  11  12 111  22 136 368  25]\n",
      " [ 17 355  96  16  20  91  45  90 185  85]\n",
      " [ 10  55 315  22  42 152  85 174 128  17]\n",
      " [  7 107 185  52  36 277  77 136  91  32]\n",
      " [  6  57 275  23  59 188  91 216  62  23]\n",
      " [ 10  90 207  32  34 317  65 135  83  27]\n",
      " [  1  77 192  38  44 191 220 166  49  22]\n",
      " [ 11  82 177  33  42 142  64 308  92  49]\n",
      " [ 15 140 114  12   8  90  13  63 502  43]\n",
      " [ 18 215  94  25  15  71  48 110 234 170]]\n",
      "Accuracy: 0.2341\n",
      "Precision: 0.2470\n",
      "Recall: 0.2341\n",
      "F1 Score: 0.2404\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for train and test sets\n",
    "train_pred = lr.predict(X_train)\n",
    "\n",
    "# Calculate training metrics\n",
    "train_cm = confusion_matrix(y_train, train_pred)\n",
    "train_accuracy = accuracy(y_train, train_pred)\n",
    "train_precision = precision(train_cm)\n",
    "train_recall = recall(train_cm)\n",
    "train_f1 = f1_score(train_cm)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nTraining Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm)\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1 Score: {train_f1:.4f}\")\n",
    "\n",
    "\n",
    "test_pred = lr.predict(X_test)\n",
    "\n",
    "test_cm = confusion_matrix(y_test, test_pred)\n",
    "test_accuracy = accuracy(y_test, test_pred)\n",
    "test_precision = precision(test_cm)\n",
    "test_recall = recall(test_cm)\n",
    "test_f1 = f1_score(test_cm)\n",
    "\n",
    "print(\"\\nValidation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm)\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch vs log-loss\n",
    "epochs = list(range(1, len(lr.train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, lr.train_losses, label='Training Loss')\n",
    "plt.plot(epochs, lr.test_losses, label='Validation Loss')\n",
    "\n",
    "plt.title('Epoch vs Mean Log-Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Log-Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.01, lambda_param=0.01, n_epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weights = None\n",
    "        self.accuracy_per_epoch = []\n",
    "\n",
    "    def fit(self, X, y, num_classes, x_test = None, y_test = None):\n",
    "        _, n_features = X.shape\n",
    "        self.weights = np.zeros((num_classes, n_features))\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for c in range(num_classes):\n",
    "                y_binary = np.where(y == c, 1, -1)\n",
    "                w = self.weights[c,:]\n",
    "                \n",
    "                for idx, x_i in enumerate(X):\n",
    "                    condition = y_binary[idx] * (np.dot(x_i, w)) # Check if current sample passes margin condition\n",
    "                    if condition >= 1:\n",
    "                        w -= self.learning_rate * (2 * self.lambda_param * w)\n",
    "                    else:\n",
    "                        w -= self.learning_rate * (2 * self.lambda_param * w - x_i * y_binary[idx])\n",
    "                            \n",
    "                self.weights[c, :] = w\n",
    "            \n",
    "            # Evaluate after each epoch\n",
    "            if x_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(x_test)\n",
    "                accuracy = np.mean(y_pred == y_test)\n",
    "                self.accuracy_per_epoch.append(accuracy)\n",
    "                print(f\"Epoch {epoch + 1}/{self.n_epochs} - Accuracy: {accuracy * 100:.2f}%\")\n",
    "                         \n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights.T)\n",
    "        return np.argmax(linear_output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n",
      "Epoch 1/100 - Accuracy: 25.69%\n",
      "Epoch 2/100 - Accuracy: 29.49%\n",
      "Epoch 3/100 - Accuracy: 31.20%\n",
      "Epoch 4/100 - Accuracy: 32.18%\n",
      "Epoch 5/100 - Accuracy: 32.86%\n",
      "Epoch 6/100 - Accuracy: 33.52%\n",
      "Epoch 7/100 - Accuracy: 33.91%\n",
      "Epoch 8/100 - Accuracy: 34.37%\n",
      "Epoch 9/100 - Accuracy: 34.57%\n",
      "Epoch 10/100 - Accuracy: 34.92%\n",
      "Epoch 11/100 - Accuracy: 35.05%\n",
      "Epoch 12/100 - Accuracy: 35.14%\n",
      "Epoch 13/100 - Accuracy: 35.47%\n",
      "Epoch 14/100 - Accuracy: 35.42%\n",
      "Epoch 15/100 - Accuracy: 35.70%\n",
      "Epoch 16/100 - Accuracy: 35.84%\n",
      "Epoch 17/100 - Accuracy: 36.08%\n",
      "Epoch 18/100 - Accuracy: 36.38%\n",
      "Epoch 19/100 - Accuracy: 36.44%\n",
      "Epoch 20/100 - Accuracy: 36.48%\n",
      "Epoch 21/100 - Accuracy: 36.44%\n",
      "Epoch 22/100 - Accuracy: 36.51%\n",
      "Epoch 23/100 - Accuracy: 36.58%\n",
      "Epoch 24/100 - Accuracy: 36.68%\n",
      "Epoch 25/100 - Accuracy: 36.78%\n",
      "Epoch 26/100 - Accuracy: 36.91%\n",
      "Epoch 27/100 - Accuracy: 36.93%\n",
      "Epoch 28/100 - Accuracy: 37.10%\n",
      "Epoch 29/100 - Accuracy: 37.20%\n",
      "Epoch 30/100 - Accuracy: 37.31%\n",
      "Epoch 31/100 - Accuracy: 37.36%\n",
      "Epoch 32/100 - Accuracy: 37.44%\n",
      "Epoch 33/100 - Accuracy: 37.55%\n",
      "Epoch 34/100 - Accuracy: 37.64%\n",
      "Epoch 35/100 - Accuracy: 37.77%\n",
      "Epoch 36/100 - Accuracy: 37.86%\n",
      "Epoch 37/100 - Accuracy: 37.95%\n",
      "Epoch 38/100 - Accuracy: 37.99%\n",
      "Epoch 39/100 - Accuracy: 38.07%\n",
      "Epoch 40/100 - Accuracy: 38.06%\n",
      "Epoch 41/100 - Accuracy: 38.10%\n",
      "Epoch 42/100 - Accuracy: 38.15%\n",
      "Epoch 43/100 - Accuracy: 38.13%\n",
      "Epoch 44/100 - Accuracy: 38.18%\n",
      "Epoch 45/100 - Accuracy: 38.21%\n",
      "Epoch 46/100 - Accuracy: 38.19%\n",
      "Epoch 47/100 - Accuracy: 38.25%\n",
      "Epoch 48/100 - Accuracy: 38.23%\n",
      "Epoch 49/100 - Accuracy: 38.18%\n",
      "Epoch 50/100 - Accuracy: 38.19%\n",
      "Epoch 51/100 - Accuracy: 38.25%\n",
      "Epoch 52/100 - Accuracy: 38.31%\n",
      "Epoch 53/100 - Accuracy: 38.34%\n",
      "Epoch 54/100 - Accuracy: 38.35%\n",
      "Epoch 55/100 - Accuracy: 38.39%\n",
      "Epoch 56/100 - Accuracy: 38.34%\n",
      "Epoch 57/100 - Accuracy: 38.41%\n",
      "Epoch 58/100 - Accuracy: 38.45%\n",
      "Epoch 59/100 - Accuracy: 38.47%\n",
      "Epoch 60/100 - Accuracy: 38.52%\n",
      "Epoch 61/100 - Accuracy: 38.53%\n",
      "Epoch 62/100 - Accuracy: 38.46%\n",
      "Epoch 63/100 - Accuracy: 38.50%\n",
      "Epoch 64/100 - Accuracy: 38.55%\n",
      "Epoch 65/100 - Accuracy: 38.54%\n",
      "Epoch 66/100 - Accuracy: 38.53%\n",
      "Epoch 67/100 - Accuracy: 38.57%\n",
      "Epoch 68/100 - Accuracy: 38.59%\n",
      "Epoch 69/100 - Accuracy: 38.62%\n",
      "Epoch 70/100 - Accuracy: 38.70%\n",
      "Epoch 71/100 - Accuracy: 38.74%\n",
      "Epoch 72/100 - Accuracy: 38.80%\n",
      "Epoch 73/100 - Accuracy: 38.87%\n",
      "Epoch 74/100 - Accuracy: 38.86%\n",
      "Epoch 75/100 - Accuracy: 38.91%\n",
      "Epoch 76/100 - Accuracy: 38.99%\n",
      "Epoch 77/100 - Accuracy: 39.04%\n",
      "Epoch 78/100 - Accuracy: 39.03%\n",
      "Epoch 79/100 - Accuracy: 39.06%\n",
      "Epoch 80/100 - Accuracy: 39.05%\n",
      "Epoch 81/100 - Accuracy: 39.04%\n",
      "Epoch 82/100 - Accuracy: 39.11%\n",
      "Epoch 83/100 - Accuracy: 39.10%\n",
      "Epoch 84/100 - Accuracy: 39.06%\n",
      "Epoch 85/100 - Accuracy: 39.07%\n",
      "Epoch 86/100 - Accuracy: 39.12%\n",
      "Epoch 87/100 - Accuracy: 39.12%\n",
      "Epoch 88/100 - Accuracy: 39.14%\n",
      "Epoch 89/100 - Accuracy: 39.19%\n",
      "Epoch 90/100 - Accuracy: 39.21%\n",
      "Epoch 91/100 - Accuracy: 39.24%\n",
      "Epoch 92/100 - Accuracy: 39.24%\n",
      "Epoch 93/100 - Accuracy: 39.21%\n",
      "Epoch 94/100 - Accuracy: 39.19%\n",
      "Epoch 95/100 - Accuracy: 39.15%\n",
      "Epoch 96/100 - Accuracy: 39.16%\n",
      "Epoch 97/100 - Accuracy: 39.18%\n",
      "Epoch 98/100 - Accuracy: 39.18%\n",
      "Epoch 99/100 - Accuracy: 39.19%\n",
      "Epoch 100/100 - Accuracy: 39.20%\n",
      "Total runtime: 402.67 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the SVM\n",
    "print(\"Training SVM...\")\n",
    "svm = SVM(learning_rate=1e-6, lambda_param=0.01, n_epochs=100)\n",
    "\n",
    "# Decode labels from one-hot encoding to integers\n",
    "y_train_decoded = np.argmax(y_train, axis=1)\n",
    "y_test_decoded = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Fit the SVM model\n",
    "svm.fit(X_train, y_train_decoded, num_classes, x_test=X_test, y_test=y_test_decoded)\n",
    "\n",
    "# End timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total runtime: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVM...\n",
      "\n",
      "SVM Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[2381  325   53  185   52  194  130  302  926  452]\n",
      " [ 300 2575   27  130   58  197  219  200  438  856]\n",
      " [ 703  283  662  499  423  610  836  511  303  170]\n",
      " [ 326  407  138 1359  122 1093  610  286  312  347]\n",
      " [ 426  196  306  346 1199  532  930  658  197  210]\n",
      " [ 214  307  227  821  187 1938  515  357  262  172]\n",
      " [ 132  287  123  638  318  470 2479  209  140  204]\n",
      " [ 240  327  110  341  343  451  276 2241  204  467]\n",
      " [ 808  441   22  101   21  218   59  109 2687  534]\n",
      " [ 287  918   25  110   34  156  181  198  502 2589]]\n",
      "Accuracy: 0.4022\n",
      "Precision: 0.4011\n",
      "Recall: 0.4022\n",
      "F1 Score: 0.4017\n",
      "\n",
      "SVM Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[498  66  15  40  12  34  25  48 174  88]\n",
      " [ 62 497   6  29   8  43  40  42 107 166]\n",
      " [148  54 123 123  88 108 178  92  59  27]\n",
      " [ 65  80  38 255  25 229 111  62  52  83]\n",
      " [ 73  37  67  77 216 105 211 142  42  30]\n",
      " [ 48  57  48 164  42 369  93  86  64  29]\n",
      " [ 20  55  31 128  57  79 522  38  26  44]\n",
      " [ 49  68  32  65  61  88  47 435  47 108]\n",
      " [152 100   3  23   3  56  14  21 515 113]\n",
      " [ 71 177   3  30   9  25  41  41 113 490]]\n",
      "Accuracy: 0.3920\n",
      "Precision: 0.3870\n",
      "Recall: 0.3920\n",
      "F1 Score: 0.3895\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating SVM...\")\n",
    "y_pred_train = svm.predict(X_train)\n",
    "y_pred_test = svm.predict(X_test)\n",
    "\n",
    "# Training Metrics\n",
    "train_cm_svm = confusion_matrix(y_train, y_pred_train)\n",
    "train_accuracy_svm = accuracy(y_train, y_pred_train)\n",
    "train_precision_svm = precision(train_cm_svm)\n",
    "train_recall_svm = recall(train_cm_svm)\n",
    "train_f1_svm = f1_score(train_cm_svm)\n",
    "\n",
    "print(\"\\nSVM Training Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm_svm)\n",
    "print(f\"Accuracy: {train_accuracy_svm:.4f}\")\n",
    "print(f\"Precision: {train_precision_svm:.4f}\")\n",
    "print(f\"Recall: {train_recall_svm:.4f}\")\n",
    "print(f\"F1 Score: {train_f1_svm:.4f}\")\n",
    "\n",
    "# Test Metrics\n",
    "test_cm_svm = confusion_matrix(y_test, y_pred_test)\n",
    "test_accuracy_svm = accuracy(y_test, y_pred_test)\n",
    "test_precision_svm = precision(test_cm_svm)\n",
    "test_recall_svm = recall(test_cm_svm)\n",
    "test_f1_svm = f1_score(test_cm_svm)\n",
    "\n",
    "print(\"\\nSVM Validation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm_svm)\n",
    "print(f\"Accuracy: {test_accuracy_svm:.4f}\")\n",
    "print(f\"Precision: {test_precision_svm:.4f}\")\n",
    "print(f\"Recall: {test_recall_svm:.4f}\")\n",
    "print(f\"F1 Score: {test_f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
