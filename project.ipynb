{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS613 Final Project: Cross-Language Image Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='bytes')\n",
    "    \n",
    "def load_cifar10(file_path):\n",
    "    train_data, train_labels = [] , []\n",
    "    for i in range(1,6):\n",
    "        batch = unpickle(f\"{file_path}\\\\data_batch_{i}\")\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    # Load test batch\n",
    "    test_batch = unpickle(f\"{file_path}\\\\test_batch\")\n",
    "    test_data = np.array(test_batch[b'data'])\n",
    "    test_labels = np.array(test_batch[b'labels'])\n",
    "\n",
    "    # Reshape the data to (N, 32, 32, 3)\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, labels, file_path):\n",
    "    # Combine labels and data\n",
    "    combined = np.column_stack((labels, data))\n",
    "    np.savetxt(file_path, combined, delimiter=\",\", fmt=\"%f\")\n",
    "    print(f\"Saved {file_path} successfully!\")\n",
    "    \n",
    "# Prepare data\n",
    "def normalize_images(data):\n",
    "    return data / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot = np.zeros((labels.size, num_classes))\n",
    "    one_hot[np.arange(labels.size),labels] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10 dataset...\n",
      "Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "file_path = r\"cifar-10-python\\\\cifar-10-batches-py\"\n",
    "x_train, y_train, x_test, y_test = load_cifar10(file_path)\n",
    "\n",
    "# Preprocess data\n",
    "print(\"Preprocessing data...\")\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0],-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = one_hot_encode(y_train, num_classes)\n",
    "y_test = one_hot_encode(y_test, num_classes)\n",
    "\n",
    "# Add biases to X\n",
    "X_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "X_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to CSV...\n",
      "Saved train.csv successfully!\n",
      "Saved test.csv successfully!\n",
      "CSV files created.\n"
     ]
    }
   ],
   "source": [
    "# Save csv files if needed\n",
    "print(\"Saving to CSV...\")\n",
    "save_to_csv(x_train, y_train, \"train.csv\")\n",
    "save_to_csv(x_test, y_test, \"test.csv\")\n",
    "print(\"CSV files created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    num_classes = y_true.shape[1]\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    y_true_indices = np.argmax(y_true, axis=1)\n",
    "    for t, p in zip(y_true_indices, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true_label = np.argmax(y_true, axis=1)  #Converting one-hot encoded back to label encoded\n",
    "    return np.sum(y_true_label == y_pred) / len(y_true_label)\n",
    "\n",
    "\n",
    "def precision(cm):\n",
    "    precisions = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        if tp + fp > 0:\n",
    "            precisions.append(tp / (tp + fp))\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "def recall(cm):\n",
    "    recalls = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        if tp + fn > 0:\n",
    "            recalls.append(tp / (tp + fn))\n",
    "        else:\n",
    "            recalls.append(0)\n",
    "    return np.mean(recalls)\n",
    "\n",
    "def f1_score(cm):\n",
    "    precisions = precision(cm)\n",
    "    recalls = recall(cm)\n",
    "    if (precisions + recalls) > 0:\n",
    "        return 2 * (precisions * recalls) / (precisions + recalls) \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        #Weight Initalization\n",
    "        self.num_features = None\n",
    "        self.num_classes = None\n",
    "        self.weights = None\n",
    "\n",
    "        # Initialize lists for tracking losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    # Logistic Regression Functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predictProb(self, X):\n",
    "        y_pred = self.softmax(np.dot(X, self.weights))\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, y_pred):\n",
    "        return np.argmax(self.predictProb(y_pred), axis=1)\n",
    "\n",
    "    def log_loss(self, y, y_pred, epsilon=1e-15):\n",
    "        return -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
    "    \n",
    "    def categorical_crossentropy(self, y, y_pred, epsilon=1e-15):\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon) \n",
    "        return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  \n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def gradient_descent(self, X, y, weights, lr):\n",
    "        y_pred = self.softmax(np.dot(X, weights))\n",
    "        error = y_pred - y\n",
    "        gradient = np.dot(X.T, error) / len(y)\n",
    "        return weights - lr * gradient\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test=None, y_test=None):\n",
    "        # Normalize data\n",
    "        X_train, X_test = normalize_images(X_train), normalize_images(X_test)\n",
    "        self.num_features = X_train.shape[1]\n",
    "        self.num_classes = y_train.shape[1]\n",
    "        self.weights = np.random.randn(self.num_features, self.num_classes)\n",
    "    \n",
    "        for epoch in range(self.epochs):\n",
    "            # Update Weights\n",
    "            self.weights = self.gradient_descent(X_train, y_train, self.weights, self.learning_rate)\n",
    "\n",
    "            train_pred = self.softmax(np.dot(X_train, self.weights))\n",
    "            train_loss = self.categorical_crossentropy(y_train, train_pred)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_pred = self.softmax(np.dot(X_test, self.weights))\n",
    "                test_loss = self.categorical_crossentropy(y_test, test_pred)\n",
    "                self.test_losses.append(test_loss)\n",
    "            else:\n",
    "                test_loss = None\n",
    "\n",
    "            # if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Train Loss = 23.6701, Test Loss = 23.7231\n",
      "Epoch 2/1000: Train Loss = 20.9292, Test Loss = 20.8743\n",
      "Epoch 3/1000: Train Loss = 19.5619, Test Loss = 19.5982\n",
      "Epoch 4/1000: Train Loss = 19.0507, Test Loss = 19.0567\n",
      "Epoch 5/1000: Train Loss = 18.4171, Test Loss = 18.4268\n",
      "Epoch 6/1000: Train Loss = 17.5497, Test Loss = 17.5607\n",
      "Epoch 7/1000: Train Loss = 16.7117, Test Loss = 16.7431\n",
      "Epoch 8/1000: Train Loss = 16.2427, Test Loss = 16.2649\n",
      "Epoch 9/1000: Train Loss = 16.0514, Test Loss = 16.0587\n",
      "Epoch 10/1000: Train Loss = 15.8709, Test Loss = 15.8763\n",
      "Epoch 11/1000: Train Loss = 15.7119, Test Loss = 15.7175\n",
      "Epoch 12/1000: Train Loss = 15.5435, Test Loss = 15.5527\n",
      "Epoch 13/1000: Train Loss = 15.3880, Test Loss = 15.3978\n",
      "Epoch 14/1000: Train Loss = 15.2274, Test Loss = 15.2408\n",
      "Epoch 15/1000: Train Loss = 15.0765, Test Loss = 15.0902\n",
      "Epoch 16/1000: Train Loss = 14.9231, Test Loss = 14.9396\n",
      "Epoch 17/1000: Train Loss = 14.7780, Test Loss = 14.7940\n",
      "Epoch 18/1000: Train Loss = 14.6317, Test Loss = 14.6504\n",
      "Epoch 19/1000: Train Loss = 14.4925, Test Loss = 14.5108\n",
      "Epoch 20/1000: Train Loss = 14.3533, Test Loss = 14.3734\n",
      "Epoch 21/1000: Train Loss = 14.2200, Test Loss = 14.2395\n",
      "Epoch 22/1000: Train Loss = 14.0876, Test Loss = 14.1087\n",
      "Epoch 23/1000: Train Loss = 13.9608, Test Loss = 13.9814\n",
      "Epoch 24/1000: Train Loss = 13.8354, Test Loss = 13.8575\n",
      "Epoch 25/1000: Train Loss = 13.7153, Test Loss = 13.7370\n",
      "Epoch 26/1000: Train Loss = 13.5967, Test Loss = 13.6202\n",
      "Epoch 27/1000: Train Loss = 13.4831, Test Loss = 13.5065\n",
      "Epoch 28/1000: Train Loss = 13.3712, Test Loss = 13.3962\n",
      "Epoch 29/1000: Train Loss = 13.2640, Test Loss = 13.2887\n",
      "Epoch 30/1000: Train Loss = 13.1586, Test Loss = 13.1850\n",
      "Epoch 31/1000: Train Loss = 13.0577, Test Loss = 13.0843\n",
      "Epoch 32/1000: Train Loss = 12.9584, Test Loss = 12.9868\n",
      "Epoch 33/1000: Train Loss = 12.8634, Test Loss = 12.8923\n",
      "Epoch 34/1000: Train Loss = 12.7700, Test Loss = 12.8009\n",
      "Epoch 35/1000: Train Loss = 12.6806, Test Loss = 12.7120\n",
      "Epoch 36/1000: Train Loss = 12.5928, Test Loss = 12.6261\n",
      "Epoch 37/1000: Train Loss = 12.5089, Test Loss = 12.5427\n",
      "Epoch 38/1000: Train Loss = 12.4264, Test Loss = 12.4622\n",
      "Epoch 39/1000: Train Loss = 12.3476, Test Loss = 12.3841\n",
      "Epoch 40/1000: Train Loss = 12.2701, Test Loss = 12.3085\n",
      "Epoch 41/1000: Train Loss = 12.1960, Test Loss = 12.2352\n",
      "Epoch 42/1000: Train Loss = 12.1231, Test Loss = 12.1644\n",
      "Epoch 43/1000: Train Loss = 12.0535, Test Loss = 12.0956\n",
      "Epoch 44/1000: Train Loss = 11.9850, Test Loss = 12.0292\n",
      "Epoch 45/1000: Train Loss = 11.9195, Test Loss = 11.9647\n",
      "Epoch 46/1000: Train Loss = 11.8551, Test Loss = 11.9023\n",
      "Epoch 47/1000: Train Loss = 11.7935, Test Loss = 11.8416\n",
      "Epoch 48/1000: Train Loss = 11.7328, Test Loss = 11.7829\n",
      "Epoch 49/1000: Train Loss = 11.6749, Test Loss = 11.7259\n",
      "Epoch 50/1000: Train Loss = 11.6175, Test Loss = 11.6706\n",
      "Epoch 51/1000: Train Loss = 11.5631, Test Loss = 11.6172\n",
      "Epoch 52/1000: Train Loss = 11.5089, Test Loss = 11.5649\n",
      "Epoch 53/1000: Train Loss = 11.4577, Test Loss = 11.5150\n",
      "Epoch 54/1000: Train Loss = 11.4064, Test Loss = 11.4653\n",
      "Epoch 55/1000: Train Loss = 11.3584, Test Loss = 11.4188\n",
      "Epoch 56/1000: Train Loss = 11.3098, Test Loss = 11.3714\n",
      "Epoch 57/1000: Train Loss = 11.2648, Test Loss = 11.3285\n",
      "Epoch 58/1000: Train Loss = 11.2187, Test Loss = 11.2831\n",
      "Epoch 59/1000: Train Loss = 11.1771, Test Loss = 11.2437\n",
      "Epoch 60/1000: Train Loss = 11.1332, Test Loss = 11.2001\n",
      "Epoch 61/1000: Train Loss = 11.0951, Test Loss = 11.1645\n",
      "Epoch 62/1000: Train Loss = 11.0530, Test Loss = 11.1226\n",
      "Epoch 63/1000: Train Loss = 11.0190, Test Loss = 11.0913\n",
      "Epoch 64/1000: Train Loss = 10.9783, Test Loss = 11.0505\n",
      "Epoch 65/1000: Train Loss = 10.9487, Test Loss = 11.0239\n",
      "Epoch 66/1000: Train Loss = 10.9085, Test Loss = 10.9834\n",
      "Epoch 67/1000: Train Loss = 10.8838, Test Loss = 10.9620\n",
      "Epoch 68/1000: Train Loss = 10.8428, Test Loss = 10.9206\n",
      "Epoch 69/1000: Train Loss = 10.8232, Test Loss = 10.9044\n",
      "Epoch 70/1000: Train Loss = 10.7802, Test Loss = 10.8608\n",
      "Epoch 71/1000: Train Loss = 10.7652, Test Loss = 10.8492\n",
      "Epoch 72/1000: Train Loss = 10.7194, Test Loss = 10.8027\n",
      "Epoch 73/1000: Train Loss = 10.7082, Test Loss = 10.7949\n",
      "Epoch 74/1000: Train Loss = 10.6596, Test Loss = 10.7456\n",
      "Epoch 75/1000: Train Loss = 10.6512, Test Loss = 10.7403\n",
      "Epoch 76/1000: Train Loss = 10.6005, Test Loss = 10.6890\n",
      "Epoch 77/1000: Train Loss = 10.5942, Test Loss = 10.6857\n",
      "Epoch 78/1000: Train Loss = 10.5423, Test Loss = 10.6330\n",
      "Epoch 79/1000: Train Loss = 10.5375, Test Loss = 10.6313\n",
      "Epoch 80/1000: Train Loss = 10.4852, Test Loss = 10.5779\n",
      "Epoch 81/1000: Train Loss = 10.4817, Test Loss = 10.5775\n",
      "Epoch 82/1000: Train Loss = 10.4294, Test Loss = 10.5241\n",
      "Epoch 83/1000: Train Loss = 10.4269, Test Loss = 10.5247\n",
      "Epoch 84/1000: Train Loss = 10.3749, Test Loss = 10.4715\n",
      "Epoch 85/1000: Train Loss = 10.3734, Test Loss = 10.4730\n",
      "Epoch 86/1000: Train Loss = 10.3218, Test Loss = 10.4203\n",
      "Epoch 87/1000: Train Loss = 10.3213, Test Loss = 10.4227\n",
      "Epoch 88/1000: Train Loss = 10.2702, Test Loss = 10.3703\n",
      "Epoch 89/1000: Train Loss = 10.2705, Test Loss = 10.3737\n",
      "Epoch 90/1000: Train Loss = 10.2199, Test Loss = 10.3217\n",
      "Epoch 91/1000: Train Loss = 10.2211, Test Loss = 10.3260\n",
      "Epoch 92/1000: Train Loss = 10.1709, Test Loss = 10.2743\n",
      "Epoch 93/1000: Train Loss = 10.1729, Test Loss = 10.2796\n",
      "Epoch 94/1000: Train Loss = 10.1232, Test Loss = 10.2281\n",
      "Epoch 95/1000: Train Loss = 10.1260, Test Loss = 10.2343\n",
      "Epoch 96/1000: Train Loss = 10.0768, Test Loss = 10.1831\n",
      "Epoch 97/1000: Train Loss = 10.0803, Test Loss = 10.1902\n",
      "Epoch 98/1000: Train Loss = 10.0315, Test Loss = 10.1391\n",
      "Epoch 99/1000: Train Loss = 10.0357, Test Loss = 10.1471\n",
      "Epoch 100/1000: Train Loss = 9.9874, Test Loss = 10.0962\n",
      "Epoch 101/1000: Train Loss = 9.9922, Test Loss = 10.1051\n",
      "Epoch 102/1000: Train Loss = 9.9443, Test Loss = 10.0543\n",
      "Epoch 103/1000: Train Loss = 9.9497, Test Loss = 10.0641\n",
      "Epoch 104/1000: Train Loss = 9.9022, Test Loss = 10.0133\n",
      "Epoch 105/1000: Train Loss = 9.9082, Test Loss = 10.0240\n",
      "Epoch 106/1000: Train Loss = 9.8611, Test Loss = 9.9733\n",
      "Epoch 107/1000: Train Loss = 9.8677, Test Loss = 9.9849\n",
      "Epoch 108/1000: Train Loss = 9.8208, Test Loss = 9.9341\n",
      "Epoch 109/1000: Train Loss = 9.8280, Test Loss = 9.9466\n",
      "Epoch 110/1000: Train Loss = 9.7815, Test Loss = 9.8958\n",
      "Epoch 111/1000: Train Loss = 9.7893, Test Loss = 9.9092\n",
      "Epoch 112/1000: Train Loss = 9.7430, Test Loss = 9.8583\n",
      "Epoch 113/1000: Train Loss = 9.7514, Test Loss = 9.8725\n",
      "Epoch 114/1000: Train Loss = 9.7053, Test Loss = 9.8215\n",
      "Epoch 115/1000: Train Loss = 9.7143, Test Loss = 9.8366\n",
      "Epoch 116/1000: Train Loss = 9.6684, Test Loss = 9.7856\n",
      "Epoch 117/1000: Train Loss = 9.6780, Test Loss = 9.8014\n",
      "Epoch 118/1000: Train Loss = 9.6323, Test Loss = 9.7503\n",
      "Epoch 119/1000: Train Loss = 9.6425, Test Loss = 9.7670\n",
      "Epoch 120/1000: Train Loss = 9.5969, Test Loss = 9.7157\n",
      "Epoch 121/1000: Train Loss = 9.6077, Test Loss = 9.7333\n",
      "Epoch 122/1000: Train Loss = 9.5622, Test Loss = 9.6817\n",
      "Epoch 123/1000: Train Loss = 9.5736, Test Loss = 9.7003\n",
      "Epoch 124/1000: Train Loss = 9.5281, Test Loss = 9.6484\n",
      "Epoch 125/1000: Train Loss = 9.5401, Test Loss = 9.6680\n",
      "Epoch 126/1000: Train Loss = 9.4947, Test Loss = 9.6157\n",
      "Epoch 127/1000: Train Loss = 9.5073, Test Loss = 9.6362\n",
      "Epoch 128/1000: Train Loss = 9.4620, Test Loss = 9.5836\n",
      "Epoch 129/1000: Train Loss = 9.4751, Test Loss = 9.6051\n",
      "Epoch 130/1000: Train Loss = 9.4299, Test Loss = 9.5521\n",
      "Epoch 131/1000: Train Loss = 9.4435, Test Loss = 9.5746\n",
      "Epoch 132/1000: Train Loss = 9.3984, Test Loss = 9.5212\n",
      "Epoch 133/1000: Train Loss = 9.4124, Test Loss = 9.5446\n",
      "Epoch 134/1000: Train Loss = 9.3674, Test Loss = 9.4909\n",
      "Epoch 135/1000: Train Loss = 9.3819, Test Loss = 9.5151\n",
      "Epoch 136/1000: Train Loss = 9.3370, Test Loss = 9.4610\n",
      "Epoch 137/1000: Train Loss = 9.3520, Test Loss = 9.4861\n",
      "Epoch 138/1000: Train Loss = 9.3072, Test Loss = 9.4317\n",
      "Epoch 139/1000: Train Loss = 9.3226, Test Loss = 9.4576\n",
      "Epoch 140/1000: Train Loss = 9.2779, Test Loss = 9.4029\n",
      "Epoch 141/1000: Train Loss = 9.2937, Test Loss = 9.4296\n",
      "Epoch 142/1000: Train Loss = 9.2491, Test Loss = 9.3746\n",
      "Epoch 143/1000: Train Loss = 9.2653, Test Loss = 9.4021\n",
      "Epoch 144/1000: Train Loss = 9.2208, Test Loss = 9.3468\n",
      "Epoch 145/1000: Train Loss = 9.2374, Test Loss = 9.3751\n",
      "Epoch 146/1000: Train Loss = 9.1930, Test Loss = 9.3194\n",
      "Epoch 147/1000: Train Loss = 9.2099, Test Loss = 9.3485\n",
      "Epoch 148/1000: Train Loss = 9.1656, Test Loss = 9.2925\n",
      "Epoch 149/1000: Train Loss = 9.1830, Test Loss = 9.3224\n",
      "Epoch 150/1000: Train Loss = 9.1387, Test Loss = 9.2661\n",
      "Epoch 151/1000: Train Loss = 9.1564, Test Loss = 9.2966\n",
      "Epoch 152/1000: Train Loss = 9.1122, Test Loss = 9.2401\n",
      "Epoch 153/1000: Train Loss = 9.1304, Test Loss = 9.2713\n",
      "Epoch 154/1000: Train Loss = 9.0862, Test Loss = 9.2144\n",
      "Epoch 155/1000: Train Loss = 9.1047, Test Loss = 9.2464\n",
      "Epoch 156/1000: Train Loss = 9.0606, Test Loss = 9.1892\n",
      "Epoch 157/1000: Train Loss = 9.0795, Test Loss = 9.2219\n",
      "Epoch 158/1000: Train Loss = 9.0354, Test Loss = 9.1644\n",
      "Epoch 159/1000: Train Loss = 9.0546, Test Loss = 9.1977\n",
      "Epoch 160/1000: Train Loss = 9.0106, Test Loss = 9.1399\n",
      "Epoch 161/1000: Train Loss = 9.0302, Test Loss = 9.1740\n",
      "Epoch 162/1000: Train Loss = 8.9862, Test Loss = 9.1158\n",
      "Epoch 163/1000: Train Loss = 9.0061, Test Loss = 9.1507\n",
      "Epoch 164/1000: Train Loss = 8.9622, Test Loss = 9.0922\n",
      "Epoch 165/1000: Train Loss = 8.9825, Test Loss = 9.1276\n",
      "Epoch 166/1000: Train Loss = 8.9385, Test Loss = 9.0689\n",
      "Epoch 167/1000: Train Loss = 8.9591, Test Loss = 9.1049\n",
      "Epoch 168/1000: Train Loss = 8.9152, Test Loss = 9.0459\n",
      "Epoch 169/1000: Train Loss = 8.9362, Test Loss = 9.0826\n",
      "Epoch 170/1000: Train Loss = 8.8923, Test Loss = 9.0233\n",
      "Epoch 171/1000: Train Loss = 8.9136, Test Loss = 9.0606\n",
      "Epoch 172/1000: Train Loss = 8.8697, Test Loss = 9.0011\n",
      "Epoch 173/1000: Train Loss = 8.8913, Test Loss = 9.0389\n",
      "Epoch 174/1000: Train Loss = 8.8475, Test Loss = 8.9791\n",
      "Epoch 175/1000: Train Loss = 8.8693, Test Loss = 9.0175\n",
      "Epoch 176/1000: Train Loss = 8.8255, Test Loss = 8.9575\n",
      "Epoch 177/1000: Train Loss = 8.8477, Test Loss = 8.9964\n",
      "Epoch 178/1000: Train Loss = 8.8039, Test Loss = 8.9362\n",
      "Epoch 179/1000: Train Loss = 8.8264, Test Loss = 8.9755\n",
      "Epoch 180/1000: Train Loss = 8.7826, Test Loss = 8.9151\n",
      "Epoch 181/1000: Train Loss = 8.8053, Test Loss = 8.9550\n",
      "Epoch 182/1000: Train Loss = 8.7616, Test Loss = 8.8944\n",
      "Epoch 183/1000: Train Loss = 8.7846, Test Loss = 8.9347\n",
      "Epoch 184/1000: Train Loss = 8.7409, Test Loss = 8.8739\n",
      "Epoch 185/1000: Train Loss = 8.7642, Test Loss = 8.9147\n",
      "Epoch 186/1000: Train Loss = 8.7204, Test Loss = 8.8536\n",
      "Epoch 187/1000: Train Loss = 8.7440, Test Loss = 8.8950\n",
      "Epoch 188/1000: Train Loss = 8.7003, Test Loss = 8.8337\n",
      "Epoch 189/1000: Train Loss = 8.7241, Test Loss = 8.8755\n",
      "Epoch 190/1000: Train Loss = 8.6804, Test Loss = 8.8139\n",
      "Epoch 191/1000: Train Loss = 8.7045, Test Loss = 8.8562\n",
      "Epoch 192/1000: Train Loss = 8.6608, Test Loss = 8.7945\n",
      "Epoch 193/1000: Train Loss = 8.6851, Test Loss = 8.8372\n",
      "Epoch 194/1000: Train Loss = 8.6414, Test Loss = 8.7752\n",
      "Epoch 195/1000: Train Loss = 8.6660, Test Loss = 8.8184\n",
      "Epoch 196/1000: Train Loss = 8.6222, Test Loss = 8.7563\n",
      "Epoch 197/1000: Train Loss = 8.6471, Test Loss = 8.7999\n",
      "Epoch 198/1000: Train Loss = 8.6033, Test Loss = 8.7375\n",
      "Epoch 199/1000: Train Loss = 8.6284, Test Loss = 8.7815\n",
      "Epoch 200/1000: Train Loss = 8.5847, Test Loss = 8.7189\n",
      "Epoch 201/1000: Train Loss = 8.6099, Test Loss = 8.7634\n",
      "Epoch 202/1000: Train Loss = 8.5662, Test Loss = 8.7006\n",
      "Epoch 203/1000: Train Loss = 8.5917, Test Loss = 8.7455\n",
      "Epoch 204/1000: Train Loss = 8.5480, Test Loss = 8.6825\n",
      "Epoch 205/1000: Train Loss = 8.5737, Test Loss = 8.7278\n",
      "Epoch 206/1000: Train Loss = 8.5300, Test Loss = 8.6645\n",
      "Epoch 207/1000: Train Loss = 8.5559, Test Loss = 8.7103\n",
      "Epoch 208/1000: Train Loss = 8.5122, Test Loss = 8.6468\n",
      "Epoch 209/1000: Train Loss = 8.5383, Test Loss = 8.6929\n",
      "Epoch 210/1000: Train Loss = 8.4946, Test Loss = 8.6293\n",
      "Epoch 211/1000: Train Loss = 8.5209, Test Loss = 8.6758\n",
      "Epoch 212/1000: Train Loss = 8.4772, Test Loss = 8.6120\n",
      "Epoch 213/1000: Train Loss = 8.5036, Test Loss = 8.6589\n",
      "Epoch 214/1000: Train Loss = 8.4600, Test Loss = 8.5949\n",
      "Epoch 215/1000: Train Loss = 8.4866, Test Loss = 8.6421\n",
      "Epoch 216/1000: Train Loss = 8.4430, Test Loss = 8.5779\n",
      "Epoch 217/1000: Train Loss = 8.4698, Test Loss = 8.6255\n",
      "Epoch 218/1000: Train Loss = 8.4262, Test Loss = 8.5612\n",
      "Epoch 219/1000: Train Loss = 8.4531, Test Loss = 8.6091\n",
      "Epoch 220/1000: Train Loss = 8.4096, Test Loss = 8.5446\n",
      "Epoch 221/1000: Train Loss = 8.4367, Test Loss = 8.5929\n",
      "Epoch 222/1000: Train Loss = 8.3931, Test Loss = 8.5282\n",
      "Epoch 223/1000: Train Loss = 8.4203, Test Loss = 8.5768\n",
      "Epoch 224/1000: Train Loss = 8.3768, Test Loss = 8.5120\n",
      "Epoch 225/1000: Train Loss = 8.4042, Test Loss = 8.5609\n",
      "Epoch 226/1000: Train Loss = 8.3607, Test Loss = 8.4959\n",
      "Epoch 227/1000: Train Loss = 8.3883, Test Loss = 8.5452\n",
      "Epoch 228/1000: Train Loss = 8.3448, Test Loss = 8.4801\n",
      "Epoch 229/1000: Train Loss = 8.3725, Test Loss = 8.5296\n",
      "Epoch 230/1000: Train Loss = 8.3290, Test Loss = 8.4643\n",
      "Epoch 231/1000: Train Loss = 8.3568, Test Loss = 8.5142\n",
      "Epoch 232/1000: Train Loss = 8.3134, Test Loss = 8.4488\n",
      "Epoch 233/1000: Train Loss = 8.3414, Test Loss = 8.4989\n",
      "Epoch 234/1000: Train Loss = 8.2980, Test Loss = 8.4334\n",
      "Epoch 235/1000: Train Loss = 8.3261, Test Loss = 8.4838\n",
      "Epoch 236/1000: Train Loss = 8.2827, Test Loss = 8.4181\n",
      "Epoch 237/1000: Train Loss = 8.3109, Test Loss = 8.4688\n",
      "Epoch 238/1000: Train Loss = 8.2676, Test Loss = 8.4030\n",
      "Epoch 239/1000: Train Loss = 8.2959, Test Loss = 8.4540\n",
      "Epoch 240/1000: Train Loss = 8.2526, Test Loss = 8.3880\n",
      "Epoch 241/1000: Train Loss = 8.2811, Test Loss = 8.4393\n",
      "Epoch 242/1000: Train Loss = 8.2378, Test Loss = 8.3732\n",
      "Epoch 243/1000: Train Loss = 8.2663, Test Loss = 8.4248\n",
      "Epoch 244/1000: Train Loss = 8.2231, Test Loss = 8.3586\n",
      "Epoch 245/1000: Train Loss = 8.2518, Test Loss = 8.4103\n",
      "Epoch 246/1000: Train Loss = 8.2086, Test Loss = 8.3441\n",
      "Epoch 247/1000: Train Loss = 8.2374, Test Loss = 8.3961\n",
      "Epoch 248/1000: Train Loss = 8.1943, Test Loss = 8.3297\n",
      "Epoch 249/1000: Train Loss = 8.2231, Test Loss = 8.3819\n",
      "Epoch 250/1000: Train Loss = 8.1801, Test Loss = 8.3155\n",
      "Epoch 251/1000: Train Loss = 8.2090, Test Loss = 8.3679\n",
      "Epoch 252/1000: Train Loss = 8.1660, Test Loss = 8.3014\n",
      "Epoch 253/1000: Train Loss = 8.1950, Test Loss = 8.3540\n",
      "Epoch 254/1000: Train Loss = 8.1520, Test Loss = 8.2874\n",
      "Epoch 255/1000: Train Loss = 8.1811, Test Loss = 8.3403\n",
      "Epoch 256/1000: Train Loss = 8.1382, Test Loss = 8.2736\n",
      "Epoch 257/1000: Train Loss = 8.1674, Test Loss = 8.3266\n",
      "Epoch 258/1000: Train Loss = 8.1246, Test Loss = 8.2599\n",
      "Epoch 259/1000: Train Loss = 8.1538, Test Loss = 8.3131\n",
      "Epoch 260/1000: Train Loss = 8.1110, Test Loss = 8.2463\n",
      "Epoch 261/1000: Train Loss = 8.1403, Test Loss = 8.2997\n",
      "Epoch 262/1000: Train Loss = 8.0976, Test Loss = 8.2329\n",
      "Epoch 263/1000: Train Loss = 8.1269, Test Loss = 8.2864\n",
      "Epoch 264/1000: Train Loss = 8.0843, Test Loss = 8.2196\n",
      "Epoch 265/1000: Train Loss = 8.1137, Test Loss = 8.2733\n",
      "Epoch 266/1000: Train Loss = 8.0712, Test Loss = 8.2064\n",
      "Epoch 267/1000: Train Loss = 8.1006, Test Loss = 8.2602\n",
      "Epoch 268/1000: Train Loss = 8.0582, Test Loss = 8.1933\n",
      "Epoch 269/1000: Train Loss = 8.0876, Test Loss = 8.2473\n",
      "Epoch 270/1000: Train Loss = 8.0453, Test Loss = 8.1804\n",
      "Epoch 271/1000: Train Loss = 8.0747, Test Loss = 8.2344\n",
      "Epoch 272/1000: Train Loss = 8.0325, Test Loss = 8.1675\n",
      "Epoch 273/1000: Train Loss = 8.0619, Test Loss = 8.2217\n",
      "Epoch 274/1000: Train Loss = 8.0198, Test Loss = 8.1548\n",
      "Epoch 275/1000: Train Loss = 8.0493, Test Loss = 8.2092\n",
      "Epoch 276/1000: Train Loss = 8.0072, Test Loss = 8.1422\n",
      "Epoch 277/1000: Train Loss = 8.0368, Test Loss = 8.1967\n",
      "Epoch 278/1000: Train Loss = 7.9948, Test Loss = 8.1297\n",
      "Epoch 279/1000: Train Loss = 8.0243, Test Loss = 8.1843\n",
      "Epoch 280/1000: Train Loss = 7.9824, Test Loss = 8.1173\n",
      "Epoch 281/1000: Train Loss = 8.0120, Test Loss = 8.1720\n",
      "Epoch 282/1000: Train Loss = 7.9702, Test Loss = 8.1050\n",
      "Epoch 283/1000: Train Loss = 7.9998, Test Loss = 8.1598\n",
      "Epoch 284/1000: Train Loss = 7.9581, Test Loss = 8.0929\n",
      "Epoch 285/1000: Train Loss = 7.9877, Test Loss = 8.1477\n",
      "Epoch 286/1000: Train Loss = 7.9460, Test Loss = 8.0808\n",
      "Epoch 287/1000: Train Loss = 7.9757, Test Loss = 8.1357\n",
      "Epoch 288/1000: Train Loss = 7.9341, Test Loss = 8.0688\n",
      "Epoch 289/1000: Train Loss = 7.9638, Test Loss = 8.1238\n",
      "Epoch 290/1000: Train Loss = 7.9223, Test Loss = 8.0570\n",
      "Epoch 291/1000: Train Loss = 7.9520, Test Loss = 8.1120\n",
      "Epoch 292/1000: Train Loss = 7.9106, Test Loss = 8.0452\n",
      "Epoch 293/1000: Train Loss = 7.9403, Test Loss = 8.1002\n",
      "Epoch 294/1000: Train Loss = 7.8990, Test Loss = 8.0336\n",
      "Epoch 295/1000: Train Loss = 7.9287, Test Loss = 8.0886\n",
      "Epoch 296/1000: Train Loss = 7.8875, Test Loss = 8.0220\n",
      "Epoch 297/1000: Train Loss = 7.9172, Test Loss = 8.0771\n",
      "Epoch 298/1000: Train Loss = 7.8761, Test Loss = 8.0106\n",
      "Epoch 299/1000: Train Loss = 7.9058, Test Loss = 8.0657\n",
      "Epoch 300/1000: Train Loss = 7.8648, Test Loss = 7.9993\n",
      "Epoch 301/1000: Train Loss = 7.8945, Test Loss = 8.0543\n",
      "Epoch 302/1000: Train Loss = 7.8536, Test Loss = 7.9880\n",
      "Epoch 303/1000: Train Loss = 7.8832, Test Loss = 8.0431\n",
      "Epoch 304/1000: Train Loss = 7.8425, Test Loss = 7.9769\n",
      "Epoch 305/1000: Train Loss = 7.8721, Test Loss = 8.0319\n",
      "Epoch 306/1000: Train Loss = 7.8315, Test Loss = 7.9658\n",
      "Epoch 307/1000: Train Loss = 7.8610, Test Loss = 8.0208\n",
      "Epoch 308/1000: Train Loss = 7.8206, Test Loss = 7.9549\n",
      "Epoch 309/1000: Train Loss = 7.8501, Test Loss = 8.0098\n",
      "Epoch 310/1000: Train Loss = 7.8097, Test Loss = 7.9440\n",
      "Epoch 311/1000: Train Loss = 7.8392, Test Loss = 7.9989\n",
      "Epoch 312/1000: Train Loss = 7.7990, Test Loss = 7.9332\n",
      "Epoch 313/1000: Train Loss = 7.8284, Test Loss = 7.9881\n",
      "Epoch 314/1000: Train Loss = 7.7884, Test Loss = 7.9226\n",
      "Epoch 315/1000: Train Loss = 7.8178, Test Loss = 7.9774\n",
      "Epoch 316/1000: Train Loss = 7.7778, Test Loss = 7.9120\n",
      "Epoch 317/1000: Train Loss = 7.8071, Test Loss = 7.9667\n",
      "Epoch 318/1000: Train Loss = 7.7673, Test Loss = 7.9015\n",
      "Epoch 319/1000: Train Loss = 7.7966, Test Loss = 7.9561\n",
      "Epoch 320/1000: Train Loss = 7.7570, Test Loss = 7.8911\n",
      "Epoch 321/1000: Train Loss = 7.7862, Test Loss = 7.9457\n",
      "Epoch 322/1000: Train Loss = 7.7467, Test Loss = 7.8807\n",
      "Epoch 323/1000: Train Loss = 7.7758, Test Loss = 7.9352\n",
      "Epoch 324/1000: Train Loss = 7.7364, Test Loss = 7.8705\n",
      "Epoch 325/1000: Train Loss = 7.7655, Test Loss = 7.9249\n",
      "Epoch 326/1000: Train Loss = 7.7263, Test Loss = 7.8604\n",
      "Epoch 327/1000: Train Loss = 7.7553, Test Loss = 7.9147\n",
      "Epoch 328/1000: Train Loss = 7.7163, Test Loss = 7.8503\n",
      "Epoch 329/1000: Train Loss = 7.7452, Test Loss = 7.9045\n",
      "Epoch 330/1000: Train Loss = 7.7063, Test Loss = 7.8403\n",
      "Epoch 331/1000: Train Loss = 7.7351, Test Loss = 7.8944\n",
      "Epoch 332/1000: Train Loss = 7.6964, Test Loss = 7.8304\n",
      "Epoch 333/1000: Train Loss = 7.7251, Test Loss = 7.8844\n",
      "Epoch 334/1000: Train Loss = 7.6866, Test Loss = 7.8206\n",
      "Epoch 335/1000: Train Loss = 7.7152, Test Loss = 7.8744\n",
      "Epoch 336/1000: Train Loss = 7.6769, Test Loss = 7.8109\n",
      "Epoch 337/1000: Train Loss = 7.7054, Test Loss = 7.8646\n",
      "Epoch 338/1000: Train Loss = 7.6673, Test Loss = 7.8012\n",
      "Epoch 339/1000: Train Loss = 7.6956, Test Loss = 7.8548\n",
      "Epoch 340/1000: Train Loss = 7.6577, Test Loss = 7.7916\n",
      "Epoch 341/1000: Train Loss = 7.6860, Test Loss = 7.8450\n",
      "Epoch 342/1000: Train Loss = 7.6482, Test Loss = 7.7821\n",
      "Epoch 343/1000: Train Loss = 7.6763, Test Loss = 7.8354\n",
      "Epoch 344/1000: Train Loss = 7.6388, Test Loss = 7.7727\n",
      "Epoch 345/1000: Train Loss = 7.6668, Test Loss = 7.8258\n",
      "Epoch 346/1000: Train Loss = 7.6295, Test Loss = 7.7633\n",
      "Epoch 347/1000: Train Loss = 7.6573, Test Loss = 7.8163\n",
      "Epoch 348/1000: Train Loss = 7.6202, Test Loss = 7.7540\n",
      "Epoch 349/1000: Train Loss = 7.6479, Test Loss = 7.8069\n",
      "Epoch 350/1000: Train Loss = 7.6110, Test Loss = 7.7448\n",
      "Epoch 351/1000: Train Loss = 7.6385, Test Loss = 7.7975\n",
      "Epoch 352/1000: Train Loss = 7.6019, Test Loss = 7.7357\n",
      "Epoch 353/1000: Train Loss = 7.6293, Test Loss = 7.7882\n",
      "Epoch 354/1000: Train Loss = 7.5929, Test Loss = 7.7267\n",
      "Epoch 355/1000: Train Loss = 7.6200, Test Loss = 7.7789\n",
      "Epoch 356/1000: Train Loss = 7.5839, Test Loss = 7.7177\n",
      "Epoch 357/1000: Train Loss = 7.6109, Test Loss = 7.7697\n",
      "Epoch 358/1000: Train Loss = 7.5750, Test Loss = 7.7088\n",
      "Epoch 359/1000: Train Loss = 7.6018, Test Loss = 7.7606\n",
      "Epoch 360/1000: Train Loss = 7.5662, Test Loss = 7.6999\n",
      "Epoch 361/1000: Train Loss = 7.5928, Test Loss = 7.7516\n",
      "Epoch 362/1000: Train Loss = 7.5574, Test Loss = 7.6912\n",
      "Epoch 363/1000: Train Loss = 7.5838, Test Loss = 7.7426\n",
      "Epoch 364/1000: Train Loss = 7.5487, Test Loss = 7.6825\n",
      "Epoch 365/1000: Train Loss = 7.5749, Test Loss = 7.7337\n",
      "Epoch 366/1000: Train Loss = 7.5401, Test Loss = 7.6738\n",
      "Epoch 367/1000: Train Loss = 7.5661, Test Loss = 7.7248\n",
      "Epoch 368/1000: Train Loss = 7.5316, Test Loss = 7.6653\n",
      "Epoch 369/1000: Train Loss = 7.5573, Test Loss = 7.7160\n",
      "Epoch 370/1000: Train Loss = 7.5231, Test Loss = 7.6568\n",
      "Epoch 371/1000: Train Loss = 7.5486, Test Loss = 7.7073\n",
      "Epoch 372/1000: Train Loss = 7.5147, Test Loss = 7.6484\n",
      "Epoch 373/1000: Train Loss = 7.5399, Test Loss = 7.6986\n",
      "Epoch 374/1000: Train Loss = 7.5063, Test Loss = 7.6400\n",
      "Epoch 375/1000: Train Loss = 7.5313, Test Loss = 7.6900\n",
      "Epoch 376/1000: Train Loss = 7.4981, Test Loss = 7.6317\n",
      "Epoch 377/1000: Train Loss = 7.5228, Test Loss = 7.6814\n",
      "Epoch 378/1000: Train Loss = 7.4899, Test Loss = 7.6235\n",
      "Epoch 379/1000: Train Loss = 7.5143, Test Loss = 7.6729\n",
      "Epoch 380/1000: Train Loss = 7.4817, Test Loss = 7.6154\n",
      "Epoch 381/1000: Train Loss = 7.5058, Test Loss = 7.6645\n",
      "Epoch 382/1000: Train Loss = 7.4736, Test Loss = 7.6073\n",
      "Epoch 383/1000: Train Loss = 7.4975, Test Loss = 7.6561\n",
      "Epoch 384/1000: Train Loss = 7.4656, Test Loss = 7.5993\n",
      "Epoch 385/1000: Train Loss = 7.4891, Test Loss = 7.6478\n",
      "Epoch 386/1000: Train Loss = 7.4576, Test Loss = 7.5913\n",
      "Epoch 387/1000: Train Loss = 7.4809, Test Loss = 7.6395\n",
      "Epoch 388/1000: Train Loss = 7.4497, Test Loss = 7.5834\n",
      "Epoch 389/1000: Train Loss = 7.4727, Test Loss = 7.6313\n",
      "Epoch 390/1000: Train Loss = 7.4419, Test Loss = 7.5756\n",
      "Epoch 391/1000: Train Loss = 7.4645, Test Loss = 7.6231\n",
      "Epoch 392/1000: Train Loss = 7.4341, Test Loss = 7.5678\n",
      "Epoch 393/1000: Train Loss = 7.4564, Test Loss = 7.6150\n",
      "Epoch 394/1000: Train Loss = 7.4264, Test Loss = 7.5601\n",
      "Epoch 395/1000: Train Loss = 7.4484, Test Loss = 7.6069\n",
      "Epoch 396/1000: Train Loss = 7.4187, Test Loss = 7.5525\n",
      "Epoch 397/1000: Train Loss = 7.4404, Test Loss = 7.5989\n",
      "Epoch 398/1000: Train Loss = 7.4111, Test Loss = 7.5449\n",
      "Epoch 399/1000: Train Loss = 7.4324, Test Loss = 7.5910\n",
      "Epoch 400/1000: Train Loss = 7.4036, Test Loss = 7.5373\n",
      "Epoch 401/1000: Train Loss = 7.4245, Test Loss = 7.5831\n",
      "Epoch 402/1000: Train Loss = 7.3961, Test Loss = 7.5299\n",
      "Epoch 403/1000: Train Loss = 7.4167, Test Loss = 7.5752\n",
      "Epoch 404/1000: Train Loss = 7.3887, Test Loss = 7.5225\n",
      "Epoch 405/1000: Train Loss = 7.4089, Test Loss = 7.5674\n",
      "Epoch 406/1000: Train Loss = 7.3813, Test Loss = 7.5151\n",
      "Epoch 407/1000: Train Loss = 7.4011, Test Loss = 7.5596\n",
      "Epoch 408/1000: Train Loss = 7.3740, Test Loss = 7.5078\n",
      "Epoch 409/1000: Train Loss = 7.3934, Test Loss = 7.5519\n",
      "Epoch 410/1000: Train Loss = 7.3667, Test Loss = 7.5005\n",
      "Epoch 411/1000: Train Loss = 7.3858, Test Loss = 7.5443\n",
      "Epoch 412/1000: Train Loss = 7.3595, Test Loss = 7.4933\n",
      "Epoch 413/1000: Train Loss = 7.3782, Test Loss = 7.5366\n",
      "Epoch 414/1000: Train Loss = 7.3523, Test Loss = 7.4862\n",
      "Epoch 415/1000: Train Loss = 7.3706, Test Loss = 7.5291\n",
      "Epoch 416/1000: Train Loss = 7.3452, Test Loss = 7.4791\n",
      "Epoch 417/1000: Train Loss = 7.3631, Test Loss = 7.5215\n",
      "Epoch 418/1000: Train Loss = 7.3382, Test Loss = 7.4721\n",
      "Epoch 419/1000: Train Loss = 7.3556, Test Loss = 7.5140\n",
      "Epoch 420/1000: Train Loss = 7.3312, Test Loss = 7.4651\n",
      "Epoch 421/1000: Train Loss = 7.3482, Test Loss = 7.5066\n",
      "Epoch 422/1000: Train Loss = 7.3242, Test Loss = 7.4582\n",
      "Epoch 423/1000: Train Loss = 7.3409, Test Loss = 7.4992\n",
      "Epoch 424/1000: Train Loss = 7.3173, Test Loss = 7.4513\n",
      "Epoch 425/1000: Train Loss = 7.3335, Test Loss = 7.4919\n",
      "Epoch 426/1000: Train Loss = 7.3105, Test Loss = 7.4445\n",
      "Epoch 427/1000: Train Loss = 7.3262, Test Loss = 7.4845\n",
      "Epoch 428/1000: Train Loss = 7.3036, Test Loss = 7.4377\n",
      "Epoch 429/1000: Train Loss = 7.3190, Test Loss = 7.4773\n",
      "Epoch 430/1000: Train Loss = 7.2969, Test Loss = 7.4309\n",
      "Epoch 431/1000: Train Loss = 7.3118, Test Loss = 7.4701\n",
      "Epoch 432/1000: Train Loss = 7.2902, Test Loss = 7.4242\n",
      "Epoch 433/1000: Train Loss = 7.3047, Test Loss = 7.4629\n",
      "Epoch 434/1000: Train Loss = 7.2835, Test Loss = 7.4176\n",
      "Epoch 435/1000: Train Loss = 7.2976, Test Loss = 7.4557\n",
      "Epoch 436/1000: Train Loss = 7.2769, Test Loss = 7.4110\n",
      "Epoch 437/1000: Train Loss = 7.2905, Test Loss = 7.4487\n",
      "Epoch 438/1000: Train Loss = 7.2703, Test Loss = 7.4044\n",
      "Epoch 439/1000: Train Loss = 7.2835, Test Loss = 7.4416\n",
      "Epoch 440/1000: Train Loss = 7.2638, Test Loss = 7.3979\n",
      "Epoch 441/1000: Train Loss = 7.2765, Test Loss = 7.4346\n",
      "Epoch 442/1000: Train Loss = 7.2573, Test Loss = 7.3915\n",
      "Epoch 443/1000: Train Loss = 7.2696, Test Loss = 7.4276\n",
      "Epoch 444/1000: Train Loss = 7.2509, Test Loss = 7.3851\n",
      "Epoch 445/1000: Train Loss = 7.2627, Test Loss = 7.4207\n",
      "Epoch 446/1000: Train Loss = 7.2445, Test Loss = 7.3787\n",
      "Epoch 447/1000: Train Loss = 7.2559, Test Loss = 7.4138\n",
      "Epoch 448/1000: Train Loss = 7.2382, Test Loss = 7.3724\n",
      "Epoch 449/1000: Train Loss = 7.2490, Test Loss = 7.4070\n",
      "Epoch 450/1000: Train Loss = 7.2318, Test Loss = 7.3661\n",
      "Epoch 451/1000: Train Loss = 7.2423, Test Loss = 7.4002\n",
      "Epoch 452/1000: Train Loss = 7.2256, Test Loss = 7.3598\n",
      "Epoch 453/1000: Train Loss = 7.2356, Test Loss = 7.3934\n",
      "Epoch 454/1000: Train Loss = 7.2194, Test Loss = 7.3536\n",
      "Epoch 455/1000: Train Loss = 7.2289, Test Loss = 7.3867\n",
      "Epoch 456/1000: Train Loss = 7.2132, Test Loss = 7.3475\n",
      "Epoch 457/1000: Train Loss = 7.2222, Test Loss = 7.3801\n",
      "Epoch 458/1000: Train Loss = 7.2070, Test Loss = 7.3413\n",
      "Epoch 459/1000: Train Loss = 7.2156, Test Loss = 7.3734\n",
      "Epoch 460/1000: Train Loss = 7.2009, Test Loss = 7.3353\n",
      "Epoch 461/1000: Train Loss = 7.2090, Test Loss = 7.3668\n",
      "Epoch 462/1000: Train Loss = 7.1949, Test Loss = 7.3292\n",
      "Epoch 463/1000: Train Loss = 7.2025, Test Loss = 7.3603\n",
      "Epoch 464/1000: Train Loss = 7.1888, Test Loss = 7.3232\n",
      "Epoch 465/1000: Train Loss = 7.1960, Test Loss = 7.3537\n",
      "Epoch 466/1000: Train Loss = 7.1829, Test Loss = 7.3173\n",
      "Epoch 467/1000: Train Loss = 7.1896, Test Loss = 7.3473\n",
      "Epoch 468/1000: Train Loss = 7.1769, Test Loss = 7.3114\n",
      "Epoch 469/1000: Train Loss = 7.1832, Test Loss = 7.3408\n",
      "Epoch 470/1000: Train Loss = 7.1710, Test Loss = 7.3055\n",
      "Epoch 471/1000: Train Loss = 7.1768, Test Loss = 7.3344\n",
      "Epoch 472/1000: Train Loss = 7.1651, Test Loss = 7.2997\n",
      "Epoch 473/1000: Train Loss = 7.1705, Test Loss = 7.3281\n",
      "Epoch 474/1000: Train Loss = 7.1593, Test Loss = 7.2939\n",
      "Epoch 475/1000: Train Loss = 7.1642, Test Loss = 7.3217\n",
      "Epoch 476/1000: Train Loss = 7.1535, Test Loss = 7.2881\n",
      "Epoch 477/1000: Train Loss = 7.1579, Test Loss = 7.3154\n",
      "Epoch 478/1000: Train Loss = 7.1478, Test Loss = 7.2824\n",
      "Epoch 479/1000: Train Loss = 7.1517, Test Loss = 7.3092\n",
      "Epoch 480/1000: Train Loss = 7.1420, Test Loss = 7.2767\n",
      "Epoch 481/1000: Train Loss = 7.1455, Test Loss = 7.3030\n",
      "Epoch 482/1000: Train Loss = 7.1363, Test Loss = 7.2710\n",
      "Epoch 483/1000: Train Loss = 7.1393, Test Loss = 7.2968\n",
      "Epoch 484/1000: Train Loss = 7.1307, Test Loss = 7.2654\n",
      "Epoch 485/1000: Train Loss = 7.1332, Test Loss = 7.2906\n",
      "Epoch 486/1000: Train Loss = 7.1251, Test Loss = 7.2598\n",
      "Epoch 487/1000: Train Loss = 7.1271, Test Loss = 7.2845\n",
      "Epoch 488/1000: Train Loss = 7.1195, Test Loss = 7.2543\n",
      "Epoch 489/1000: Train Loss = 7.1211, Test Loss = 7.2784\n",
      "Epoch 490/1000: Train Loss = 7.1139, Test Loss = 7.2488\n",
      "Epoch 491/1000: Train Loss = 7.1151, Test Loss = 7.2724\n",
      "Epoch 492/1000: Train Loss = 7.1084, Test Loss = 7.2433\n",
      "Epoch 493/1000: Train Loss = 7.1091, Test Loss = 7.2664\n",
      "Epoch 494/1000: Train Loss = 7.1029, Test Loss = 7.2378\n",
      "Epoch 495/1000: Train Loss = 7.1032, Test Loss = 7.2604\n",
      "Epoch 496/1000: Train Loss = 7.0975, Test Loss = 7.2324\n",
      "Epoch 497/1000: Train Loss = 7.0973, Test Loss = 7.2545\n",
      "Epoch 498/1000: Train Loss = 7.0921, Test Loss = 7.2271\n",
      "Epoch 499/1000: Train Loss = 7.0914, Test Loss = 7.2486\n",
      "Epoch 500/1000: Train Loss = 7.0867, Test Loss = 7.2217\n",
      "Epoch 501/1000: Train Loss = 7.0856, Test Loss = 7.2427\n",
      "Epoch 502/1000: Train Loss = 7.0813, Test Loss = 7.2164\n",
      "Epoch 503/1000: Train Loss = 7.0798, Test Loss = 7.2369\n",
      "Epoch 504/1000: Train Loss = 7.0760, Test Loss = 7.2111\n",
      "Epoch 505/1000: Train Loss = 7.0740, Test Loss = 7.2311\n",
      "Epoch 506/1000: Train Loss = 7.0707, Test Loss = 7.2058\n",
      "Epoch 507/1000: Train Loss = 7.0683, Test Loss = 7.2253\n",
      "Epoch 508/1000: Train Loss = 7.0654, Test Loss = 7.2006\n",
      "Epoch 509/1000: Train Loss = 7.0626, Test Loss = 7.2196\n",
      "Epoch 510/1000: Train Loss = 7.0602, Test Loss = 7.1954\n",
      "Epoch 511/1000: Train Loss = 7.0570, Test Loss = 7.2139\n",
      "Epoch 512/1000: Train Loss = 7.0550, Test Loss = 7.1902\n",
      "Epoch 513/1000: Train Loss = 7.0513, Test Loss = 7.2082\n",
      "Epoch 514/1000: Train Loss = 7.0498, Test Loss = 7.1851\n",
      "Epoch 515/1000: Train Loss = 7.0457, Test Loss = 7.2026\n",
      "Epoch 516/1000: Train Loss = 7.0446, Test Loss = 7.1800\n",
      "Epoch 517/1000: Train Loss = 7.0402, Test Loss = 7.1970\n",
      "Epoch 518/1000: Train Loss = 7.0395, Test Loss = 7.1749\n",
      "Epoch 519/1000: Train Loss = 7.0347, Test Loss = 7.1914\n",
      "Epoch 520/1000: Train Loss = 7.0344, Test Loss = 7.1698\n",
      "Epoch 521/1000: Train Loss = 7.0292, Test Loss = 7.1859\n",
      "Epoch 522/1000: Train Loss = 7.0293, Test Loss = 7.1648\n",
      "Epoch 523/1000: Train Loss = 7.0237, Test Loss = 7.1804\n",
      "Epoch 524/1000: Train Loss = 7.0243, Test Loss = 7.1598\n",
      "Epoch 525/1000: Train Loss = 7.0182, Test Loss = 7.1749\n",
      "Epoch 526/1000: Train Loss = 7.0192, Test Loss = 7.1548\n",
      "Epoch 527/1000: Train Loss = 7.0128, Test Loss = 7.1694\n",
      "Epoch 528/1000: Train Loss = 7.0142, Test Loss = 7.1499\n",
      "Epoch 529/1000: Train Loss = 7.0075, Test Loss = 7.1640\n",
      "Epoch 530/1000: Train Loss = 7.0093, Test Loss = 7.1449\n",
      "Epoch 531/1000: Train Loss = 7.0021, Test Loss = 7.1586\n",
      "Epoch 532/1000: Train Loss = 7.0043, Test Loss = 7.1400\n",
      "Epoch 533/1000: Train Loss = 6.9968, Test Loss = 7.1533\n",
      "Epoch 534/1000: Train Loss = 6.9994, Test Loss = 7.1352\n",
      "Epoch 535/1000: Train Loss = 6.9915, Test Loss = 7.1480\n",
      "Epoch 536/1000: Train Loss = 6.9945, Test Loss = 7.1303\n",
      "Epoch 537/1000: Train Loss = 6.9862, Test Loss = 7.1427\n",
      "Epoch 538/1000: Train Loss = 6.9896, Test Loss = 7.1255\n",
      "Epoch 539/1000: Train Loss = 6.9810, Test Loss = 7.1374\n",
      "Epoch 540/1000: Train Loss = 6.9848, Test Loss = 7.1207\n",
      "Epoch 541/1000: Train Loss = 6.9758, Test Loss = 7.1322\n",
      "Epoch 542/1000: Train Loss = 6.9799, Test Loss = 7.1159\n",
      "Epoch 543/1000: Train Loss = 6.9707, Test Loss = 7.1270\n",
      "Epoch 544/1000: Train Loss = 6.9751, Test Loss = 7.1111\n",
      "Epoch 545/1000: Train Loss = 6.9655, Test Loss = 7.1218\n",
      "Epoch 546/1000: Train Loss = 6.9704, Test Loss = 7.1064\n",
      "Epoch 547/1000: Train Loss = 6.9604, Test Loss = 7.1167\n",
      "Epoch 548/1000: Train Loss = 6.9656, Test Loss = 7.1017\n",
      "Epoch 549/1000: Train Loss = 6.9553, Test Loss = 7.1115\n",
      "Epoch 550/1000: Train Loss = 6.9609, Test Loss = 7.0970\n",
      "Epoch 551/1000: Train Loss = 6.9503, Test Loss = 7.1065\n",
      "Epoch 552/1000: Train Loss = 6.9562, Test Loss = 7.0923\n",
      "Epoch 553/1000: Train Loss = 6.9452, Test Loss = 7.1014\n",
      "Epoch 554/1000: Train Loss = 6.9515, Test Loss = 7.0877\n",
      "Epoch 555/1000: Train Loss = 6.9402, Test Loss = 7.0964\n",
      "Epoch 556/1000: Train Loss = 6.9468, Test Loss = 7.0831\n",
      "Epoch 557/1000: Train Loss = 6.9353, Test Loss = 7.0914\n",
      "Epoch 558/1000: Train Loss = 6.9421, Test Loss = 7.0784\n",
      "Epoch 559/1000: Train Loss = 6.9303, Test Loss = 7.0864\n",
      "Epoch 560/1000: Train Loss = 6.9375, Test Loss = 7.0739\n",
      "Epoch 561/1000: Train Loss = 6.9254, Test Loss = 7.0814\n",
      "Epoch 562/1000: Train Loss = 6.9329, Test Loss = 7.0693\n",
      "Epoch 563/1000: Train Loss = 6.9205, Test Loss = 7.0765\n",
      "Epoch 564/1000: Train Loss = 6.9283, Test Loss = 7.0647\n",
      "Epoch 565/1000: Train Loss = 6.9156, Test Loss = 7.0716\n",
      "Epoch 566/1000: Train Loss = 6.9238, Test Loss = 7.0602\n",
      "Epoch 567/1000: Train Loss = 6.9108, Test Loss = 7.0667\n",
      "Epoch 568/1000: Train Loss = 6.9192, Test Loss = 7.0557\n",
      "Epoch 569/1000: Train Loss = 6.9060, Test Loss = 7.0619\n",
      "Epoch 570/1000: Train Loss = 6.9147, Test Loss = 7.0512\n",
      "Epoch 571/1000: Train Loss = 6.9012, Test Loss = 7.0571\n",
      "Epoch 572/1000: Train Loss = 6.9102, Test Loss = 7.0468\n",
      "Epoch 573/1000: Train Loss = 6.8964, Test Loss = 7.0523\n",
      "Epoch 574/1000: Train Loss = 6.9057, Test Loss = 7.0423\n",
      "Epoch 575/1000: Train Loss = 6.8917, Test Loss = 7.0475\n",
      "Epoch 576/1000: Train Loss = 6.9012, Test Loss = 7.0379\n",
      "Epoch 577/1000: Train Loss = 6.8870, Test Loss = 7.0427\n",
      "Epoch 578/1000: Train Loss = 6.8968, Test Loss = 7.0335\n",
      "Epoch 579/1000: Train Loss = 6.8823, Test Loss = 7.0380\n",
      "Epoch 580/1000: Train Loss = 6.8924, Test Loss = 7.0292\n",
      "Epoch 581/1000: Train Loss = 6.8776, Test Loss = 7.0333\n",
      "Epoch 582/1000: Train Loss = 6.8880, Test Loss = 7.0248\n",
      "Epoch 583/1000: Train Loss = 6.8729, Test Loss = 7.0286\n",
      "Epoch 584/1000: Train Loss = 6.8836, Test Loss = 7.0205\n",
      "Epoch 585/1000: Train Loss = 6.8683, Test Loss = 7.0240\n",
      "Epoch 586/1000: Train Loss = 6.8792, Test Loss = 7.0161\n",
      "Epoch 587/1000: Train Loss = 6.8637, Test Loss = 7.0194\n",
      "Epoch 588/1000: Train Loss = 6.8749, Test Loss = 7.0118\n",
      "Epoch 589/1000: Train Loss = 6.8592, Test Loss = 7.0148\n",
      "Epoch 590/1000: Train Loss = 6.8705, Test Loss = 7.0076\n",
      "Epoch 591/1000: Train Loss = 6.8546, Test Loss = 7.0102\n",
      "Epoch 592/1000: Train Loss = 6.8662, Test Loss = 7.0033\n",
      "Epoch 593/1000: Train Loss = 6.8501, Test Loss = 7.0056\n",
      "Epoch 594/1000: Train Loss = 6.8619, Test Loss = 6.9990\n",
      "Epoch 595/1000: Train Loss = 6.8456, Test Loss = 7.0011\n",
      "Epoch 596/1000: Train Loss = 6.8576, Test Loss = 6.9948\n",
      "Epoch 597/1000: Train Loss = 6.8411, Test Loss = 6.9966\n",
      "Epoch 598/1000: Train Loss = 6.8534, Test Loss = 6.9906\n",
      "Epoch 599/1000: Train Loss = 6.8366, Test Loss = 6.9921\n",
      "Epoch 600/1000: Train Loss = 6.8491, Test Loss = 6.9864\n",
      "Epoch 601/1000: Train Loss = 6.8322, Test Loss = 6.9876\n",
      "Epoch 602/1000: Train Loss = 6.8449, Test Loss = 6.9822\n",
      "Epoch 603/1000: Train Loss = 6.8278, Test Loss = 6.9832\n",
      "Epoch 604/1000: Train Loss = 6.8407, Test Loss = 6.9781\n",
      "Epoch 605/1000: Train Loss = 6.8234, Test Loss = 6.9788\n",
      "Epoch 606/1000: Train Loss = 6.8365, Test Loss = 6.9739\n",
      "Epoch 607/1000: Train Loss = 6.8190, Test Loss = 6.9744\n",
      "Epoch 608/1000: Train Loss = 6.8324, Test Loss = 6.9698\n",
      "Epoch 609/1000: Train Loss = 6.8147, Test Loss = 6.9700\n",
      "Epoch 610/1000: Train Loss = 6.8282, Test Loss = 6.9657\n",
      "Epoch 611/1000: Train Loss = 6.8104, Test Loss = 6.9656\n",
      "Epoch 612/1000: Train Loss = 6.8241, Test Loss = 6.9616\n",
      "Epoch 613/1000: Train Loss = 6.8061, Test Loss = 6.9613\n",
      "Epoch 614/1000: Train Loss = 6.8199, Test Loss = 6.9576\n",
      "Epoch 615/1000: Train Loss = 6.8018, Test Loss = 6.9570\n",
      "Epoch 616/1000: Train Loss = 6.8158, Test Loss = 6.9535\n",
      "Epoch 617/1000: Train Loss = 6.7975, Test Loss = 6.9527\n",
      "Epoch 618/1000: Train Loss = 6.8117, Test Loss = 6.9495\n",
      "Epoch 619/1000: Train Loss = 6.7933, Test Loss = 6.9484\n",
      "Epoch 620/1000: Train Loss = 6.8077, Test Loss = 6.9455\n",
      "Epoch 621/1000: Train Loss = 6.7891, Test Loss = 6.9442\n",
      "Epoch 622/1000: Train Loss = 6.8036, Test Loss = 6.9415\n",
      "Epoch 623/1000: Train Loss = 6.7849, Test Loss = 6.9400\n",
      "Epoch 624/1000: Train Loss = 6.7996, Test Loss = 6.9375\n",
      "Epoch 625/1000: Train Loss = 6.7807, Test Loss = 6.9358\n",
      "Epoch 626/1000: Train Loss = 6.7955, Test Loss = 6.9335\n",
      "Epoch 627/1000: Train Loss = 6.7766, Test Loss = 6.9316\n",
      "Epoch 628/1000: Train Loss = 6.7915, Test Loss = 6.9296\n",
      "Epoch 629/1000: Train Loss = 6.7724, Test Loss = 6.9275\n",
      "Epoch 630/1000: Train Loss = 6.7875, Test Loss = 6.9256\n",
      "Epoch 631/1000: Train Loss = 6.7683, Test Loss = 6.9233\n",
      "Epoch 632/1000: Train Loss = 6.7836, Test Loss = 6.9217\n",
      "Epoch 633/1000: Train Loss = 6.7642, Test Loss = 6.9192\n",
      "Epoch 634/1000: Train Loss = 6.7796, Test Loss = 6.9178\n",
      "Epoch 635/1000: Train Loss = 6.7602, Test Loss = 6.9151\n",
      "Epoch 636/1000: Train Loss = 6.7757, Test Loss = 6.9139\n",
      "Epoch 637/1000: Train Loss = 6.7561, Test Loss = 6.9110\n",
      "Epoch 638/1000: Train Loss = 6.7717, Test Loss = 6.9100\n",
      "Epoch 639/1000: Train Loss = 6.7521, Test Loss = 6.9070\n",
      "Epoch 640/1000: Train Loss = 6.7678, Test Loss = 6.9062\n",
      "Epoch 641/1000: Train Loss = 6.7481, Test Loss = 6.9030\n",
      "Epoch 642/1000: Train Loss = 6.7639, Test Loss = 6.9023\n",
      "Epoch 643/1000: Train Loss = 6.7441, Test Loss = 6.8990\n",
      "Epoch 644/1000: Train Loss = 6.7600, Test Loss = 6.8985\n",
      "Epoch 645/1000: Train Loss = 6.7401, Test Loss = 6.8950\n",
      "Epoch 646/1000: Train Loss = 6.7561, Test Loss = 6.8947\n",
      "Epoch 647/1000: Train Loss = 6.7362, Test Loss = 6.8910\n",
      "Epoch 648/1000: Train Loss = 6.7523, Test Loss = 6.8909\n",
      "Epoch 649/1000: Train Loss = 6.7322, Test Loss = 6.8870\n",
      "Epoch 650/1000: Train Loss = 6.7484, Test Loss = 6.8871\n",
      "Epoch 651/1000: Train Loss = 6.7283, Test Loss = 6.8831\n",
      "Epoch 652/1000: Train Loss = 6.7446, Test Loss = 6.8833\n",
      "Epoch 653/1000: Train Loss = 6.7244, Test Loss = 6.8792\n",
      "Epoch 654/1000: Train Loss = 6.7408, Test Loss = 6.8796\n",
      "Epoch 655/1000: Train Loss = 6.7205, Test Loss = 6.8753\n",
      "Epoch 656/1000: Train Loss = 6.7369, Test Loss = 6.8758\n",
      "Epoch 657/1000: Train Loss = 6.7167, Test Loss = 6.8714\n",
      "Epoch 658/1000: Train Loss = 6.7332, Test Loss = 6.8721\n",
      "Epoch 659/1000: Train Loss = 6.7128, Test Loss = 6.8676\n",
      "Epoch 660/1000: Train Loss = 6.7294, Test Loss = 6.8684\n",
      "Epoch 661/1000: Train Loss = 6.7090, Test Loss = 6.8637\n",
      "Epoch 662/1000: Train Loss = 6.7256, Test Loss = 6.8647\n",
      "Epoch 663/1000: Train Loss = 6.7052, Test Loss = 6.8599\n",
      "Epoch 664/1000: Train Loss = 6.7219, Test Loss = 6.8610\n",
      "Epoch 665/1000: Train Loss = 6.7014, Test Loss = 6.8561\n",
      "Epoch 666/1000: Train Loss = 6.7181, Test Loss = 6.8573\n",
      "Epoch 667/1000: Train Loss = 6.6977, Test Loss = 6.8523\n",
      "Epoch 668/1000: Train Loss = 6.7144, Test Loss = 6.8537\n",
      "Epoch 669/1000: Train Loss = 6.6939, Test Loss = 6.8485\n",
      "Epoch 670/1000: Train Loss = 6.7107, Test Loss = 6.8500\n",
      "Epoch 671/1000: Train Loss = 6.6902, Test Loss = 6.8448\n",
      "Epoch 672/1000: Train Loss = 6.7070, Test Loss = 6.8464\n",
      "Epoch 673/1000: Train Loss = 6.6865, Test Loss = 6.8410\n",
      "Epoch 674/1000: Train Loss = 6.7033, Test Loss = 6.8428\n",
      "Epoch 675/1000: Train Loss = 6.6828, Test Loss = 6.8373\n",
      "Epoch 676/1000: Train Loss = 6.6996, Test Loss = 6.8392\n",
      "Epoch 677/1000: Train Loss = 6.6791, Test Loss = 6.8336\n",
      "Epoch 678/1000: Train Loss = 6.6960, Test Loss = 6.8356\n",
      "Epoch 679/1000: Train Loss = 6.6754, Test Loss = 6.8299\n",
      "Epoch 680/1000: Train Loss = 6.6923, Test Loss = 6.8320\n",
      "Epoch 681/1000: Train Loss = 6.6718, Test Loss = 6.8263\n",
      "Epoch 682/1000: Train Loss = 6.6887, Test Loss = 6.8284\n",
      "Epoch 683/1000: Train Loss = 6.6682, Test Loss = 6.8226\n",
      "Epoch 684/1000: Train Loss = 6.6850, Test Loss = 6.8249\n",
      "Epoch 685/1000: Train Loss = 6.6645, Test Loss = 6.8190\n",
      "Epoch 686/1000: Train Loss = 6.6814, Test Loss = 6.8213\n",
      "Epoch 687/1000: Train Loss = 6.6610, Test Loss = 6.8153\n",
      "Epoch 688/1000: Train Loss = 6.6778, Test Loss = 6.8178\n",
      "Epoch 689/1000: Train Loss = 6.6574, Test Loss = 6.8117\n",
      "Epoch 690/1000: Train Loss = 6.6742, Test Loss = 6.8143\n",
      "Epoch 691/1000: Train Loss = 6.6538, Test Loss = 6.8082\n",
      "Epoch 692/1000: Train Loss = 6.6707, Test Loss = 6.8108\n",
      "Epoch 693/1000: Train Loss = 6.6503, Test Loss = 6.8046\n",
      "Epoch 694/1000: Train Loss = 6.6671, Test Loss = 6.8073\n",
      "Epoch 695/1000: Train Loss = 6.6467, Test Loss = 6.8010\n",
      "Epoch 696/1000: Train Loss = 6.6636, Test Loss = 6.8038\n",
      "Epoch 697/1000: Train Loss = 6.6432, Test Loss = 6.7975\n",
      "Epoch 698/1000: Train Loss = 6.6600, Test Loss = 6.8004\n",
      "Epoch 699/1000: Train Loss = 6.6397, Test Loss = 6.7940\n",
      "Epoch 700/1000: Train Loss = 6.6565, Test Loss = 6.7969\n",
      "Epoch 701/1000: Train Loss = 6.6362, Test Loss = 6.7905\n",
      "Epoch 702/1000: Train Loss = 6.6530, Test Loss = 6.7935\n",
      "Epoch 703/1000: Train Loss = 6.6327, Test Loss = 6.7870\n",
      "Epoch 704/1000: Train Loss = 6.6495, Test Loss = 6.7901\n",
      "Epoch 705/1000: Train Loss = 6.6293, Test Loss = 6.7835\n",
      "Epoch 706/1000: Train Loss = 6.6460, Test Loss = 6.7866\n",
      "Epoch 707/1000: Train Loss = 6.6258, Test Loss = 6.7800\n",
      "Epoch 708/1000: Train Loss = 6.6425, Test Loss = 6.7832\n",
      "Epoch 709/1000: Train Loss = 6.6224, Test Loss = 6.7766\n",
      "Epoch 710/1000: Train Loss = 6.6390, Test Loss = 6.7799\n",
      "Epoch 711/1000: Train Loss = 6.6190, Test Loss = 6.7732\n",
      "Epoch 712/1000: Train Loss = 6.6356, Test Loss = 6.7765\n",
      "Epoch 713/1000: Train Loss = 6.6156, Test Loss = 6.7698\n",
      "Epoch 714/1000: Train Loss = 6.6321, Test Loss = 6.7731\n",
      "Epoch 715/1000: Train Loss = 6.6122, Test Loss = 6.7664\n",
      "Epoch 716/1000: Train Loss = 6.6287, Test Loss = 6.7698\n",
      "Epoch 717/1000: Train Loss = 6.6088, Test Loss = 6.7630\n",
      "Epoch 718/1000: Train Loss = 6.6253, Test Loss = 6.7664\n",
      "Epoch 719/1000: Train Loss = 6.6055, Test Loss = 6.7596\n",
      "Epoch 720/1000: Train Loss = 6.6219, Test Loss = 6.7631\n",
      "Epoch 721/1000: Train Loss = 6.6021, Test Loss = 6.7562\n",
      "Epoch 722/1000: Train Loss = 6.6185, Test Loss = 6.7598\n",
      "Epoch 723/1000: Train Loss = 6.5988, Test Loss = 6.7529\n",
      "Epoch 724/1000: Train Loss = 6.6151, Test Loss = 6.7564\n",
      "Epoch 725/1000: Train Loss = 6.5955, Test Loss = 6.7496\n",
      "Epoch 726/1000: Train Loss = 6.6117, Test Loss = 6.7531\n",
      "Epoch 727/1000: Train Loss = 6.5922, Test Loss = 6.7462\n",
      "Epoch 728/1000: Train Loss = 6.6084, Test Loss = 6.7499\n",
      "Epoch 729/1000: Train Loss = 6.5889, Test Loss = 6.7429\n",
      "Epoch 730/1000: Train Loss = 6.6050, Test Loss = 6.7466\n",
      "Epoch 731/1000: Train Loss = 6.5856, Test Loss = 6.7396\n",
      "Epoch 732/1000: Train Loss = 6.6017, Test Loss = 6.7433\n",
      "Epoch 733/1000: Train Loss = 6.5824, Test Loss = 6.7364\n",
      "Epoch 734/1000: Train Loss = 6.5983, Test Loss = 6.7401\n",
      "Epoch 735/1000: Train Loss = 6.5791, Test Loss = 6.7331\n",
      "Epoch 736/1000: Train Loss = 6.5950, Test Loss = 6.7368\n",
      "Epoch 737/1000: Train Loss = 6.5759, Test Loss = 6.7298\n",
      "Epoch 738/1000: Train Loss = 6.5917, Test Loss = 6.7336\n",
      "Epoch 739/1000: Train Loss = 6.5726, Test Loss = 6.7266\n",
      "Epoch 740/1000: Train Loss = 6.5884, Test Loss = 6.7304\n",
      "Epoch 741/1000: Train Loss = 6.5694, Test Loss = 6.7234\n",
      "Epoch 742/1000: Train Loss = 6.5851, Test Loss = 6.7272\n",
      "Epoch 743/1000: Train Loss = 6.5662, Test Loss = 6.7202\n",
      "Epoch 744/1000: Train Loss = 6.5818, Test Loss = 6.7240\n",
      "Epoch 745/1000: Train Loss = 6.5630, Test Loss = 6.7170\n",
      "Epoch 746/1000: Train Loss = 6.5786, Test Loss = 6.7208\n",
      "Epoch 747/1000: Train Loss = 6.5599, Test Loss = 6.7138\n",
      "Epoch 748/1000: Train Loss = 6.5753, Test Loss = 6.7176\n",
      "Epoch 749/1000: Train Loss = 6.5567, Test Loss = 6.7106\n",
      "Epoch 750/1000: Train Loss = 6.5721, Test Loss = 6.7144\n",
      "Epoch 751/1000: Train Loss = 6.5535, Test Loss = 6.7074\n",
      "Epoch 752/1000: Train Loss = 6.5688, Test Loss = 6.7113\n",
      "Epoch 753/1000: Train Loss = 6.5504, Test Loss = 6.7043\n",
      "Epoch 754/1000: Train Loss = 6.5656, Test Loss = 6.7081\n",
      "Epoch 755/1000: Train Loss = 6.5473, Test Loss = 6.7011\n",
      "Epoch 756/1000: Train Loss = 6.5624, Test Loss = 6.7050\n",
      "Epoch 757/1000: Train Loss = 6.5441, Test Loss = 6.6980\n",
      "Epoch 758/1000: Train Loss = 6.5592, Test Loss = 6.7019\n",
      "Epoch 759/1000: Train Loss = 6.5410, Test Loss = 6.6949\n",
      "Epoch 760/1000: Train Loss = 6.5560, Test Loss = 6.6988\n",
      "Epoch 761/1000: Train Loss = 6.5379, Test Loss = 6.6918\n",
      "Epoch 762/1000: Train Loss = 6.5528, Test Loss = 6.6957\n",
      "Epoch 763/1000: Train Loss = 6.5348, Test Loss = 6.6887\n",
      "Epoch 764/1000: Train Loss = 6.5497, Test Loss = 6.6926\n",
      "Epoch 765/1000: Train Loss = 6.5318, Test Loss = 6.6856\n",
      "Epoch 766/1000: Train Loss = 6.5465, Test Loss = 6.6895\n",
      "Epoch 767/1000: Train Loss = 6.5287, Test Loss = 6.6825\n",
      "Epoch 768/1000: Train Loss = 6.5433, Test Loss = 6.6864\n",
      "Epoch 769/1000: Train Loss = 6.5256, Test Loss = 6.6795\n",
      "Epoch 770/1000: Train Loss = 6.5402, Test Loss = 6.6834\n",
      "Epoch 771/1000: Train Loss = 6.5226, Test Loss = 6.6764\n",
      "Epoch 772/1000: Train Loss = 6.5371, Test Loss = 6.6803\n",
      "Epoch 773/1000: Train Loss = 6.5195, Test Loss = 6.6734\n",
      "Epoch 774/1000: Train Loss = 6.5340, Test Loss = 6.6773\n",
      "Epoch 775/1000: Train Loss = 6.5165, Test Loss = 6.6704\n",
      "Epoch 776/1000: Train Loss = 6.5308, Test Loss = 6.6742\n",
      "Epoch 777/1000: Train Loss = 6.5135, Test Loss = 6.6673\n",
      "Epoch 778/1000: Train Loss = 6.5277, Test Loss = 6.6712\n",
      "Epoch 779/1000: Train Loss = 6.5105, Test Loss = 6.6643\n",
      "Epoch 780/1000: Train Loss = 6.5247, Test Loss = 6.6682\n",
      "Epoch 781/1000: Train Loss = 6.5075, Test Loss = 6.6613\n",
      "Epoch 782/1000: Train Loss = 6.5216, Test Loss = 6.6652\n",
      "Epoch 783/1000: Train Loss = 6.5045, Test Loss = 6.6583\n",
      "Epoch 784/1000: Train Loss = 6.5185, Test Loss = 6.6622\n",
      "Epoch 785/1000: Train Loss = 6.5015, Test Loss = 6.6554\n",
      "Epoch 786/1000: Train Loss = 6.5154, Test Loss = 6.6592\n",
      "Epoch 787/1000: Train Loss = 6.4986, Test Loss = 6.6524\n",
      "Epoch 788/1000: Train Loss = 6.5124, Test Loss = 6.6563\n",
      "Epoch 789/1000: Train Loss = 6.4956, Test Loss = 6.6494\n",
      "Epoch 790/1000: Train Loss = 6.5094, Test Loss = 6.6533\n",
      "Epoch 791/1000: Train Loss = 6.4926, Test Loss = 6.6465\n",
      "Epoch 792/1000: Train Loss = 6.5063, Test Loss = 6.6504\n",
      "Epoch 793/1000: Train Loss = 6.4897, Test Loss = 6.6435\n",
      "Epoch 794/1000: Train Loss = 6.5033, Test Loss = 6.6474\n",
      "Epoch 795/1000: Train Loss = 6.4868, Test Loss = 6.6406\n",
      "Epoch 796/1000: Train Loss = 6.5003, Test Loss = 6.6445\n",
      "Epoch 797/1000: Train Loss = 6.4839, Test Loss = 6.6377\n",
      "Epoch 798/1000: Train Loss = 6.4973, Test Loss = 6.6416\n",
      "Epoch 799/1000: Train Loss = 6.4809, Test Loss = 6.6348\n",
      "Epoch 800/1000: Train Loss = 6.4943, Test Loss = 6.6387\n",
      "Epoch 801/1000: Train Loss = 6.4780, Test Loss = 6.6319\n",
      "Epoch 802/1000: Train Loss = 6.4913, Test Loss = 6.6358\n",
      "Epoch 803/1000: Train Loss = 6.4751, Test Loss = 6.6290\n",
      "Epoch 804/1000: Train Loss = 6.4884, Test Loss = 6.6329\n",
      "Epoch 805/1000: Train Loss = 6.4723, Test Loss = 6.6261\n",
      "Epoch 806/1000: Train Loss = 6.4854, Test Loss = 6.6300\n",
      "Epoch 807/1000: Train Loss = 6.4694, Test Loss = 6.6232\n",
      "Epoch 808/1000: Train Loss = 6.4824, Test Loss = 6.6271\n",
      "Epoch 809/1000: Train Loss = 6.4665, Test Loss = 6.6204\n",
      "Epoch 810/1000: Train Loss = 6.4795, Test Loss = 6.6242\n",
      "Epoch 811/1000: Train Loss = 6.4636, Test Loss = 6.6175\n",
      "Epoch 812/1000: Train Loss = 6.4766, Test Loss = 6.6214\n",
      "Epoch 813/1000: Train Loss = 6.4608, Test Loss = 6.6147\n",
      "Epoch 814/1000: Train Loss = 6.4736, Test Loss = 6.6185\n",
      "Epoch 815/1000: Train Loss = 6.4580, Test Loss = 6.6118\n",
      "Epoch 816/1000: Train Loss = 6.4707, Test Loss = 6.6157\n",
      "Epoch 817/1000: Train Loss = 6.4551, Test Loss = 6.6090\n",
      "Epoch 818/1000: Train Loss = 6.4678, Test Loss = 6.6129\n",
      "Epoch 819/1000: Train Loss = 6.4523, Test Loss = 6.6062\n",
      "Epoch 820/1000: Train Loss = 6.4649, Test Loss = 6.6101\n",
      "Epoch 821/1000: Train Loss = 6.4495, Test Loss = 6.6034\n",
      "Epoch 822/1000: Train Loss = 6.4620, Test Loss = 6.6072\n",
      "Epoch 823/1000: Train Loss = 6.4467, Test Loss = 6.6006\n",
      "Epoch 824/1000: Train Loss = 6.4591, Test Loss = 6.6044\n",
      "Epoch 825/1000: Train Loss = 6.4439, Test Loss = 6.5978\n",
      "Epoch 826/1000: Train Loss = 6.4563, Test Loss = 6.6017\n",
      "Epoch 827/1000: Train Loss = 6.4411, Test Loss = 6.5950\n",
      "Epoch 828/1000: Train Loss = 6.4534, Test Loss = 6.5989\n",
      "Epoch 829/1000: Train Loss = 6.4383, Test Loss = 6.5922\n",
      "Epoch 830/1000: Train Loss = 6.4506, Test Loss = 6.5961\n",
      "Epoch 831/1000: Train Loss = 6.4355, Test Loss = 6.5895\n",
      "Epoch 832/1000: Train Loss = 6.4477, Test Loss = 6.5933\n",
      "Epoch 833/1000: Train Loss = 6.4327, Test Loss = 6.5867\n",
      "Epoch 834/1000: Train Loss = 6.4449, Test Loss = 6.5906\n",
      "Epoch 835/1000: Train Loss = 6.4300, Test Loss = 6.5840\n",
      "Epoch 836/1000: Train Loss = 6.4421, Test Loss = 6.5878\n",
      "Epoch 837/1000: Train Loss = 6.4272, Test Loss = 6.5812\n",
      "Epoch 838/1000: Train Loss = 6.4392, Test Loss = 6.5851\n",
      "Epoch 839/1000: Train Loss = 6.4245, Test Loss = 6.5785\n",
      "Epoch 840/1000: Train Loss = 6.4364, Test Loss = 6.5824\n",
      "Epoch 841/1000: Train Loss = 6.4217, Test Loss = 6.5758\n",
      "Epoch 842/1000: Train Loss = 6.4336, Test Loss = 6.5796\n",
      "Epoch 843/1000: Train Loss = 6.4190, Test Loss = 6.5731\n",
      "Epoch 844/1000: Train Loss = 6.4308, Test Loss = 6.5769\n",
      "Epoch 845/1000: Train Loss = 6.4163, Test Loss = 6.5704\n",
      "Epoch 846/1000: Train Loss = 6.4280, Test Loss = 6.5742\n",
      "Epoch 847/1000: Train Loss = 6.4136, Test Loss = 6.5677\n",
      "Epoch 848/1000: Train Loss = 6.4253, Test Loss = 6.5715\n",
      "Epoch 849/1000: Train Loss = 6.4109, Test Loss = 6.5650\n",
      "Epoch 850/1000: Train Loss = 6.4225, Test Loss = 6.5688\n",
      "Epoch 851/1000: Train Loss = 6.4082, Test Loss = 6.5623\n",
      "Epoch 852/1000: Train Loss = 6.4198, Test Loss = 6.5661\n",
      "Epoch 853/1000: Train Loss = 6.4055, Test Loss = 6.5596\n",
      "Epoch 854/1000: Train Loss = 6.4170, Test Loss = 6.5635\n",
      "Epoch 855/1000: Train Loss = 6.4028, Test Loss = 6.5569\n",
      "Epoch 856/1000: Train Loss = 6.4143, Test Loss = 6.5608\n",
      "Epoch 857/1000: Train Loss = 6.4001, Test Loss = 6.5543\n",
      "Epoch 858/1000: Train Loss = 6.4115, Test Loss = 6.5581\n",
      "Epoch 859/1000: Train Loss = 6.3975, Test Loss = 6.5516\n",
      "Epoch 860/1000: Train Loss = 6.4088, Test Loss = 6.5555\n",
      "Epoch 861/1000: Train Loss = 6.3948, Test Loss = 6.5490\n",
      "Epoch 862/1000: Train Loss = 6.4061, Test Loss = 6.5528\n",
      "Epoch 863/1000: Train Loss = 6.3921, Test Loss = 6.5464\n",
      "Epoch 864/1000: Train Loss = 6.4034, Test Loss = 6.5502\n",
      "Epoch 865/1000: Train Loss = 6.3895, Test Loss = 6.5437\n",
      "Epoch 866/1000: Train Loss = 6.4007, Test Loss = 6.5476\n",
      "Epoch 867/1000: Train Loss = 6.3869, Test Loss = 6.5411\n",
      "Epoch 868/1000: Train Loss = 6.3980, Test Loss = 6.5450\n",
      "Epoch 869/1000: Train Loss = 6.3842, Test Loss = 6.5385\n",
      "Epoch 870/1000: Train Loss = 6.3953, Test Loss = 6.5424\n",
      "Epoch 871/1000: Train Loss = 6.3816, Test Loss = 6.5359\n",
      "Epoch 872/1000: Train Loss = 6.3926, Test Loss = 6.5398\n",
      "Epoch 873/1000: Train Loss = 6.3790, Test Loss = 6.5333\n",
      "Epoch 874/1000: Train Loss = 6.3900, Test Loss = 6.5372\n",
      "Epoch 875/1000: Train Loss = 6.3764, Test Loss = 6.5307\n",
      "Epoch 876/1000: Train Loss = 6.3873, Test Loss = 6.5346\n",
      "Epoch 877/1000: Train Loss = 6.3738, Test Loss = 6.5282\n",
      "Epoch 878/1000: Train Loss = 6.3847, Test Loss = 6.5320\n",
      "Epoch 879/1000: Train Loss = 6.3712, Test Loss = 6.5256\n",
      "Epoch 880/1000: Train Loss = 6.3820, Test Loss = 6.5294\n",
      "Epoch 881/1000: Train Loss = 6.3686, Test Loss = 6.5230\n",
      "Epoch 882/1000: Train Loss = 6.3794, Test Loss = 6.5269\n",
      "Epoch 883/1000: Train Loss = 6.3660, Test Loss = 6.5205\n",
      "Epoch 884/1000: Train Loss = 6.3768, Test Loss = 6.5243\n",
      "Epoch 885/1000: Train Loss = 6.3635, Test Loss = 6.5179\n",
      "Epoch 886/1000: Train Loss = 6.3742, Test Loss = 6.5217\n",
      "Epoch 887/1000: Train Loss = 6.3609, Test Loss = 6.5154\n",
      "Epoch 888/1000: Train Loss = 6.3715, Test Loss = 6.5192\n",
      "Epoch 889/1000: Train Loss = 6.3583, Test Loss = 6.5128\n",
      "Epoch 890/1000: Train Loss = 6.3689, Test Loss = 6.5167\n",
      "Epoch 891/1000: Train Loss = 6.3558, Test Loss = 6.5103\n",
      "Epoch 892/1000: Train Loss = 6.3664, Test Loss = 6.5142\n",
      "Epoch 893/1000: Train Loss = 6.3532, Test Loss = 6.5078\n",
      "Epoch 894/1000: Train Loss = 6.3638, Test Loss = 6.5116\n",
      "Epoch 895/1000: Train Loss = 6.3507, Test Loss = 6.5053\n",
      "Epoch 896/1000: Train Loss = 6.3612, Test Loss = 6.5091\n",
      "Epoch 897/1000: Train Loss = 6.3482, Test Loss = 6.5028\n",
      "Epoch 898/1000: Train Loss = 6.3586, Test Loss = 6.5066\n",
      "Epoch 899/1000: Train Loss = 6.3456, Test Loss = 6.5003\n",
      "Epoch 900/1000: Train Loss = 6.3561, Test Loss = 6.5041\n",
      "Epoch 901/1000: Train Loss = 6.3431, Test Loss = 6.4978\n",
      "Epoch 902/1000: Train Loss = 6.3535, Test Loss = 6.5016\n",
      "Epoch 903/1000: Train Loss = 6.3406, Test Loss = 6.4953\n",
      "Epoch 904/1000: Train Loss = 6.3510, Test Loss = 6.4992\n",
      "Epoch 905/1000: Train Loss = 6.3381, Test Loss = 6.4928\n",
      "Epoch 906/1000: Train Loss = 6.3484, Test Loss = 6.4967\n",
      "Epoch 907/1000: Train Loss = 6.3356, Test Loss = 6.4904\n",
      "Epoch 908/1000: Train Loss = 6.3459, Test Loss = 6.4942\n",
      "Epoch 909/1000: Train Loss = 6.3331, Test Loss = 6.4879\n",
      "Epoch 910/1000: Train Loss = 6.3434, Test Loss = 6.4918\n",
      "Epoch 911/1000: Train Loss = 6.3306, Test Loss = 6.4855\n",
      "Epoch 912/1000: Train Loss = 6.3408, Test Loss = 6.4893\n",
      "Epoch 913/1000: Train Loss = 6.3281, Test Loss = 6.4830\n",
      "Epoch 914/1000: Train Loss = 6.3383, Test Loss = 6.4869\n",
      "Epoch 915/1000: Train Loss = 6.3257, Test Loss = 6.4806\n",
      "Epoch 916/1000: Train Loss = 6.3358, Test Loss = 6.4845\n",
      "Epoch 917/1000: Train Loss = 6.3232, Test Loss = 6.4781\n",
      "Epoch 918/1000: Train Loss = 6.3333, Test Loss = 6.4820\n",
      "Epoch 919/1000: Train Loss = 6.3207, Test Loss = 6.4757\n",
      "Epoch 920/1000: Train Loss = 6.3309, Test Loss = 6.4796\n",
      "Epoch 921/1000: Train Loss = 6.3183, Test Loss = 6.4733\n",
      "Epoch 922/1000: Train Loss = 6.3284, Test Loss = 6.4772\n",
      "Epoch 923/1000: Train Loss = 6.3158, Test Loss = 6.4709\n",
      "Epoch 924/1000: Train Loss = 6.3259, Test Loss = 6.4748\n",
      "Epoch 925/1000: Train Loss = 6.3134, Test Loss = 6.4685\n",
      "Epoch 926/1000: Train Loss = 6.3234, Test Loss = 6.4724\n",
      "Epoch 927/1000: Train Loss = 6.3110, Test Loss = 6.4661\n",
      "Epoch 928/1000: Train Loss = 6.3210, Test Loss = 6.4700\n",
      "Epoch 929/1000: Train Loss = 6.3085, Test Loss = 6.4637\n",
      "Epoch 930/1000: Train Loss = 6.3185, Test Loss = 6.4676\n",
      "Epoch 931/1000: Train Loss = 6.3061, Test Loss = 6.4613\n",
      "Epoch 932/1000: Train Loss = 6.3161, Test Loss = 6.4652\n",
      "Epoch 933/1000: Train Loss = 6.3037, Test Loss = 6.4589\n",
      "Epoch 934/1000: Train Loss = 6.3137, Test Loss = 6.4629\n",
      "Epoch 935/1000: Train Loss = 6.3013, Test Loss = 6.4565\n",
      "Epoch 936/1000: Train Loss = 6.3112, Test Loss = 6.4605\n",
      "Epoch 937/1000: Train Loss = 6.2989, Test Loss = 6.4541\n",
      "Epoch 938/1000: Train Loss = 6.3088, Test Loss = 6.4582\n",
      "Epoch 939/1000: Train Loss = 6.2965, Test Loss = 6.4518\n",
      "Epoch 940/1000: Train Loss = 6.3064, Test Loss = 6.4558\n",
      "Epoch 941/1000: Train Loss = 6.2941, Test Loss = 6.4494\n",
      "Epoch 942/1000: Train Loss = 6.3040, Test Loss = 6.4535\n",
      "Epoch 943/1000: Train Loss = 6.2917, Test Loss = 6.4471\n",
      "Epoch 944/1000: Train Loss = 6.3016, Test Loss = 6.4511\n",
      "Epoch 945/1000: Train Loss = 6.2893, Test Loss = 6.4447\n",
      "Epoch 946/1000: Train Loss = 6.2992, Test Loss = 6.4488\n",
      "Epoch 947/1000: Train Loss = 6.2869, Test Loss = 6.4424\n",
      "Epoch 948/1000: Train Loss = 6.2968, Test Loss = 6.4465\n",
      "Epoch 949/1000: Train Loss = 6.2846, Test Loss = 6.4401\n",
      "Epoch 950/1000: Train Loss = 6.2944, Test Loss = 6.4442\n",
      "Epoch 951/1000: Train Loss = 6.2822, Test Loss = 6.4377\n",
      "Epoch 952/1000: Train Loss = 6.2921, Test Loss = 6.4419\n",
      "Epoch 953/1000: Train Loss = 6.2799, Test Loss = 6.4354\n",
      "Epoch 954/1000: Train Loss = 6.2897, Test Loss = 6.4396\n",
      "Epoch 955/1000: Train Loss = 6.2775, Test Loss = 6.4331\n",
      "Epoch 956/1000: Train Loss = 6.2873, Test Loss = 6.4373\n",
      "Epoch 957/1000: Train Loss = 6.2752, Test Loss = 6.4308\n",
      "Epoch 958/1000: Train Loss = 6.2850, Test Loss = 6.4350\n",
      "Epoch 959/1000: Train Loss = 6.2728, Test Loss = 6.4285\n",
      "Epoch 960/1000: Train Loss = 6.2826, Test Loss = 6.4327\n",
      "Epoch 961/1000: Train Loss = 6.2705, Test Loss = 6.4262\n",
      "Epoch 962/1000: Train Loss = 6.2803, Test Loss = 6.4304\n",
      "Epoch 963/1000: Train Loss = 6.2681, Test Loss = 6.4239\n",
      "Epoch 964/1000: Train Loss = 6.2780, Test Loss = 6.4282\n",
      "Epoch 965/1000: Train Loss = 6.2658, Test Loss = 6.4216\n",
      "Epoch 966/1000: Train Loss = 6.2756, Test Loss = 6.4259\n",
      "Epoch 967/1000: Train Loss = 6.2635, Test Loss = 6.4193\n",
      "Epoch 968/1000: Train Loss = 6.2733, Test Loss = 6.4236\n",
      "Epoch 969/1000: Train Loss = 6.2612, Test Loss = 6.4171\n",
      "Epoch 970/1000: Train Loss = 6.2710, Test Loss = 6.4214\n",
      "Epoch 971/1000: Train Loss = 6.2589, Test Loss = 6.4148\n",
      "Epoch 972/1000: Train Loss = 6.2687, Test Loss = 6.4192\n",
      "Epoch 973/1000: Train Loss = 6.2566, Test Loss = 6.4125\n",
      "Epoch 974/1000: Train Loss = 6.2664, Test Loss = 6.4169\n",
      "Epoch 975/1000: Train Loss = 6.2543, Test Loss = 6.4103\n",
      "Epoch 976/1000: Train Loss = 6.2641, Test Loss = 6.4147\n",
      "Epoch 977/1000: Train Loss = 6.2520, Test Loss = 6.4080\n",
      "Epoch 978/1000: Train Loss = 6.2618, Test Loss = 6.4125\n",
      "Epoch 979/1000: Train Loss = 6.2497, Test Loss = 6.4058\n",
      "Epoch 980/1000: Train Loss = 6.2595, Test Loss = 6.4103\n",
      "Epoch 981/1000: Train Loss = 6.2474, Test Loss = 6.4036\n",
      "Epoch 982/1000: Train Loss = 6.2573, Test Loss = 6.4081\n",
      "Epoch 983/1000: Train Loss = 6.2452, Test Loss = 6.4013\n",
      "Epoch 984/1000: Train Loss = 6.2550, Test Loss = 6.4058\n",
      "Epoch 985/1000: Train Loss = 6.2429, Test Loss = 6.3991\n",
      "Epoch 986/1000: Train Loss = 6.2527, Test Loss = 6.4037\n",
      "Epoch 987/1000: Train Loss = 6.2406, Test Loss = 6.3969\n",
      "Epoch 988/1000: Train Loss = 6.2505, Test Loss = 6.4015\n",
      "Epoch 989/1000: Train Loss = 6.2384, Test Loss = 6.3947\n",
      "Epoch 990/1000: Train Loss = 6.2482, Test Loss = 6.3993\n",
      "Epoch 991/1000: Train Loss = 6.2361, Test Loss = 6.3925\n",
      "Epoch 992/1000: Train Loss = 6.2460, Test Loss = 6.3971\n",
      "Epoch 993/1000: Train Loss = 6.2339, Test Loss = 6.3903\n",
      "Epoch 994/1000: Train Loss = 6.2438, Test Loss = 6.3949\n",
      "Epoch 995/1000: Train Loss = 6.2316, Test Loss = 6.3881\n",
      "Epoch 996/1000: Train Loss = 6.2415, Test Loss = 6.3928\n",
      "Epoch 997/1000: Train Loss = 6.2294, Test Loss = 6.3859\n",
      "Epoch 998/1000: Train Loss = 6.2393, Test Loss = 6.3906\n",
      "Epoch 999/1000: Train Loss = 6.2271, Test Loss = 6.3837\n",
      "Epoch 1000/1000: Train Loss = 6.2371, Test Loss = 6.3884\n",
      "Total runtime: 331.70 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = logistic_regression(learning_rate=0.1, epochs=1000)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the LR model\n",
    "lr.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# End timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total runtime: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[2505  562  652   51   63  425  165  164  159  254]\n",
      " [ 785 1990  428   90   84  412  308  179  127  597]\n",
      " [ 882  363 1534  107  222  807  668  232   41  144]\n",
      " [ 634  609  786  278  158 1294  706  250   66  219]\n",
      " [ 644  322 1290  106  388  863  835  378   23  151]\n",
      " [ 617  477  859  220  173 1590  628  256   47  133]\n",
      " [ 364  396  879  231  303  949 1430  235   44  169]\n",
      " [ 695  468  902  156  257  792  529  822   50  329]\n",
      " [1921  961  526   52   28  429  142  115  386  440]\n",
      " [ 873 1375  423   84   78  373  330  202  149 1113]]\n",
      "Accuracy: 0.2407\n",
      "Precision: 0.2532\n",
      "Recall: 0.2407\n",
      "F1 Score: 0.2468\n",
      "\n",
      "Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[507 109 142  12   8  92  26  30  29  45]\n",
      " [150 371  91  20  25  95  61  32  34 121]\n",
      " [200  70 280  26  56 169 121  39  12  27]\n",
      " [137 108 168  56  43 263 124  46  12  43]\n",
      " [138  64 258  26  77 177 171  61   8  20]\n",
      " [134  80 180  25  35 323 126  53  10  34]\n",
      " [ 70  92 189  42  60 165 292  49   8  33]\n",
      " [150  95 166  18  54 165  94 173  13  72]\n",
      " [361 188 115  14   9  92  30  18  77  96]\n",
      " [177 293  80  17  12  84  63  43  34 197]]\n",
      "Accuracy: 0.2353\n",
      "Precision: 0.2484\n",
      "Recall: 0.2353\n",
      "F1 Score: 0.2417\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for train and test sets\n",
    "train_pred = lr.predict(X_train)\n",
    "\n",
    "# Calculate training metrics\n",
    "train_cm = confusion_matrix(y_train, train_pred)\n",
    "train_accuracy = accuracy(y_train, train_pred)\n",
    "train_precision = precision(train_cm)\n",
    "train_recall = recall(train_cm)\n",
    "train_f1 = f1_score(train_cm)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nTraining Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm)\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1 Score: {train_f1:.4f}\")\n",
    "\n",
    "\n",
    "test_pred = lr.predict(X_test)\n",
    "\n",
    "test_cm = confusion_matrix(y_test, test_pred)\n",
    "test_accuracy = accuracy(y_test, test_pred)\n",
    "test_precision = precision(test_cm)\n",
    "test_recall = recall(test_cm)\n",
    "test_f1 = f1_score(test_cm)\n",
    "\n",
    "print(\"\\nValidation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm)\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch vs log-loss\n",
    "epochs = list(range(1, len(lr.train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, lr.train_losses, label='Training Loss')\n",
    "plt.plot(epochs, lr.test_losses, label='Validation Loss')\n",
    "\n",
    "plt.title('Epoch vs Mean Log-Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Log-Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.01, lambda_param=0.01, n_epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weights = None\n",
    "        self.accuracy_per_epoch = []\n",
    "\n",
    "    def fit(self, X, y, num_classes, x_test = None, y_test = None):\n",
    "        _, n_features = X.shape\n",
    "        self.weights = np.zeros((num_classes, n_features))\n",
    "        X, x_test= normalize_images(X), normalize_images(x_test)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for c in range(num_classes):\n",
    "                y_binary = np.where(y == c, 1, -1)\n",
    "                w = self.weights[c,:]\n",
    "                \n",
    "                for idx, x_i in enumerate(X):\n",
    "                    condition = y_binary[idx] * (np.dot(x_i, w)) # Check if current sample passes margin condition\n",
    "                    if condition >= 1:\n",
    "                        w -= self.learning_rate * (2 * self.lambda_param * w)\n",
    "                    else:\n",
    "                        w -= self.learning_rate * (2 * self.lambda_param * w - x_i * y_binary[idx])\n",
    "                            \n",
    "                self.weights[c, :] = w\n",
    "            \n",
    "            # Evaluate after each epoch\n",
    "            if x_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(x_test)\n",
    "                accuracy = np.mean(y_pred == y_test)\n",
    "                self.accuracy_per_epoch.append(accuracy)\n",
    "                print(f\"Epoch {epoch + 1}/{self.n_epochs} - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    def predictProb(self, X):\n",
    "        linear_output = np.dot(X, self.weights.T)\n",
    "        return linear_output\n",
    "    \n",
    "    def predict(self, linear_output):\n",
    "        return np.argmax(self.predictProb(linear_output), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n",
      "Epoch 1/20 - Accuracy: 32.37%\n",
      "Epoch 2/20 - Accuracy: 34.07%\n",
      "Epoch 3/20 - Accuracy: 34.87%\n",
      "Epoch 4/20 - Accuracy: 35.68%\n",
      "Epoch 5/20 - Accuracy: 35.79%\n",
      "Epoch 6/20 - Accuracy: 36.03%\n",
      "Epoch 7/20 - Accuracy: 36.21%\n",
      "Epoch 8/20 - Accuracy: 36.37%\n",
      "Epoch 9/20 - Accuracy: 36.52%\n",
      "Epoch 10/20 - Accuracy: 36.79%\n",
      "Epoch 11/20 - Accuracy: 36.69%\n",
      "Epoch 12/20 - Accuracy: 36.92%\n",
      "Epoch 13/20 - Accuracy: 36.84%\n",
      "Epoch 14/20 - Accuracy: 36.93%\n",
      "Epoch 15/20 - Accuracy: 36.94%\n",
      "Epoch 16/20 - Accuracy: 37.01%\n",
      "Epoch 17/20 - Accuracy: 36.96%\n",
      "Epoch 18/20 - Accuracy: 36.95%\n",
      "Epoch 19/20 - Accuracy: 36.96%\n",
      "Epoch 20/20 - Accuracy: 36.95%\n",
      "Total runtime: 84.82 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM\n",
    "print(\"Training SVM...\")\n",
    "svm = SVM(learning_rate=1e-5, lambda_param=0.1, n_epochs=20)\n",
    "\n",
    "# Decode labels from one-hot encoding to integers\n",
    "y_train_decoded = np.argmax(y_train, axis=1)\n",
    "y_test_decoded = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the SVM model\n",
    "svm.fit(X_train, y_train_decoded, num_classes, x_test=X_test, y_test=y_test_decoded)\n",
    "\n",
    "# End timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total runtime: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVM...\n",
      "\n",
      "SVM Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[1516  351   18  599   26   99  118  335 1527  411]\n",
      " [ 118 2532    9  316   22  162  186  200  666  789]\n",
      " [ 322  314  331 1223  197  428  822  659  566  138]\n",
      " [ 119  422   66 2090   48  759  451  306  431  308]\n",
      " [ 194  239  152 1026  681  322  990  843  368  185]\n",
      " [  62  307  124 1536   79 1523  422  396  398  153]\n",
      " [  44  317   66 1215  155  324 2223  248  224  184]\n",
      " [  82  353   49  728  163  298  260 2282  339  446]\n",
      " [ 287  440    7  275    8  150   43  106 3220  464]\n",
      " [ 131  979   10  245   13  108  148  210  784 2372]]\n",
      "Accuracy: 0.3754\n",
      "Precision: 0.4024\n",
      "Recall: 0.3754\n",
      "F1 Score: 0.3884\n",
      "\n",
      "SVM Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[297  73   4 118   4  18  25  69 313  79]\n",
      " [ 25 485   3  64   4  35  35  41 155 153]\n",
      " [ 74  61  66 237  45  81 179 115 118  24]\n",
      " [ 24  81  19 418   8 159  89  62  75  65]\n",
      " [ 40  47  34 208 133  66 212 160  71  29]\n",
      " [ 14  60  24 275  21 311  84  94  94  23]\n",
      " [  4  68  18 244  24  55 466  39  41  41]\n",
      " [ 16  75   7 150  26  58  49 433  85 101]\n",
      " [ 56 100   0  61   0  31   9  20 628  95]\n",
      " [ 29 186   1  56   3  20  38  45 164 458]]\n",
      "Accuracy: 0.3695\n",
      "Precision: 0.3961\n",
      "Recall: 0.3695\n",
      "F1 Score: 0.3824\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating SVM...\")\n",
    "y_pred_train = svm.predict(X_train)\n",
    "y_pred_test = svm.predict(X_test)\n",
    "\n",
    "# Training Metrics\n",
    "train_cm_svm = confusion_matrix(y_train, y_pred_train)\n",
    "train_accuracy_svm = accuracy(y_train, y_pred_train)\n",
    "train_precision_svm = precision(train_cm_svm)\n",
    "train_recall_svm = recall(train_cm_svm)\n",
    "train_f1_svm = f1_score(train_cm_svm)\n",
    "\n",
    "print(\"\\nSVM Training Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm_svm)\n",
    "print(f\"Accuracy: {train_accuracy_svm:.4f}\")\n",
    "print(f\"Precision: {train_precision_svm:.4f}\")\n",
    "print(f\"Recall: {train_recall_svm:.4f}\")\n",
    "print(f\"F1 Score: {train_f1_svm:.4f}\")\n",
    "\n",
    "# Test Metrics\n",
    "test_cm_svm = confusion_matrix(y_test, y_pred_test)\n",
    "test_accuracy_svm = accuracy(y_test, y_pred_test)\n",
    "test_precision_svm = precision(test_cm_svm)\n",
    "test_recall_svm = recall(test_cm_svm)\n",
    "test_f1_svm = f1_score(test_cm_svm)\n",
    "\n",
    "print(\"\\nSVM Validation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm_svm)\n",
    "print(f\"Accuracy: {test_accuracy_svm:.4f}\")\n",
    "print(f\"Precision: {test_precision_svm:.4f}\")\n",
    "print(f\"Recall: {test_recall_svm:.4f}\")\n",
    "print(f\"F1 Score: {test_f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def predictProb(self, X):\n",
    "        # Average probabilities from each model\n",
    "        probs = [model.predictProb(X) for model in self.models]\n",
    "        avg_probs = np.mean(probs, axis=0)\n",
    "        return avg_probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        avg_probs = self.predictProb(X)\n",
    "        return np.argmax(avg_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleModel(models=[lr, svm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[1533  350   19  590   26  101  118  335 1518  410]\n",
      " [ 121 2534   10  308   22  167  185  201  660  792]\n",
      " [ 331  313  346 1207  188  436  828  657  556  138]\n",
      " [ 121  422   64 2076   49  772  454  305  429  308]\n",
      " [ 201  239  160 1008  682  335  995  833  363  184]\n",
      " [  65  311  126 1515   78 1536  422  395  398  154]\n",
      " [  44  321   70 1201  155  326 2234  248  220  181]\n",
      " [  85  355   51  720  157  307  264 2277  337  447]\n",
      " [ 290  439    7  272    8  154   43  106 3217  464]\n",
      " [ 134  982   11  242   13  109  149  208  782 2370]]\n",
      "Accuracy: 0.3761\n",
      "Precision: 0.4029\n",
      "Recall: 0.3761\n",
      "F1 Score: 0.3890\n",
      "\n",
      "Ensemble Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[302  73   5 117   3  18  25  69 310  78]\n",
      " [ 25 485   3  64   4  35  35  41 154 154]\n",
      " [ 75  61  68 234  45  83 179 114 117  24]\n",
      " [ 24  81  18 412   9 163  91  62  75  65]\n",
      " [ 41  48  35 204 133  67 214 159  70  29]\n",
      " [ 14  60  26 270  21 314  83  94  94  24]\n",
      " [  4  68  21 240  22  57 467  40  41  40]\n",
      " [ 16  75   7 150  27  60  48 431  85 101]\n",
      " [ 58 101   0  61   0  30  10  20 624  96]\n",
      " [ 30 186   1  56   3  21  38  46 163 456]]\n",
      "Accuracy: 0.3692\n",
      "Precision: 0.3951\n",
      "Recall: 0.3692\n",
      "F1 Score: 0.3817\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Ensemble Model\n",
    "y_pred_train = ensemble.predict(X_train)\n",
    "y_pred_test = ensemble.predict(X_test)\n",
    "\n",
    "# Training Metrics\n",
    "train_cm_ensemble = confusion_matrix(y_train, y_pred_train)\n",
    "train_accuracy_ensemble = accuracy(y_train, y_pred_train)\n",
    "train_precision_ensemble = precision(train_cm_ensemble)\n",
    "train_recall_ensemble = recall(train_cm_ensemble)\n",
    "train_f1_ensemble = f1_score(train_cm_ensemble)\n",
    "\n",
    "print(\"\\nEnsemble Training Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm_ensemble)\n",
    "print(f\"Accuracy: {train_accuracy_ensemble:.4f}\")\n",
    "print(f\"Precision: {train_precision_ensemble:.4f}\")\n",
    "print(f\"Recall: {train_recall_ensemble:.4f}\")\n",
    "print(f\"F1 Score: {train_f1_ensemble:.4f}\")\n",
    "\n",
    "# Test Metrics\n",
    "test_cm_ensemble = confusion_matrix(y_test, y_pred_test)\n",
    "test_accuracy_ensemble = accuracy(y_test, y_pred_test)\n",
    "test_precision_ensemble = precision(test_cm_ensemble)\n",
    "test_recall_ensemble = recall(test_cm_ensemble)\n",
    "test_f1_ensemble = f1_score(test_cm_ensemble)\n",
    "\n",
    "print(\"\\nEnsemble Validation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm_ensemble)\n",
    "print(f\"Accuracy: {test_accuracy_ensemble:.4f}\")\n",
    "print(f\"Precision: {test_precision_ensemble:.4f}\")\n",
    "print(f\"Recall: {test_recall_ensemble:.4f}\")\n",
    "print(f\"F1 Score: {test_f1_ensemble:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
