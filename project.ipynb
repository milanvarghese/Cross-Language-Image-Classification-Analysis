{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS613 Final Project: Image Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='bytes')\n",
    "    \n",
    "def load_cifar10():\n",
    "    train_data, train_labels = [] , []\n",
    "    for i in range(1,6):\n",
    "        batch = unpickle(f\"data/cifar-10-python/cifar-10-batches-py/data_batch_{i}\")\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    # Load test batch\n",
    "    test_batch = unpickle(f\"data/cifar-10-python/cifar-10-batches-py/test_batch\")\n",
    "    test_data = np.array(test_batch[b'data'])\n",
    "    test_labels = np.array(test_batch[b'labels'])\n",
    "\n",
    "    # Reshape the data to (N, 32, 32, 3)\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# Save data to CSV\n",
    "def save_to_csv(data, labels, file_path):\n",
    "    # Combine labels and data\n",
    "    combined = np.column_stack((labels, data))\n",
    "    # Save as a CSV file\n",
    "    np.savetxt(file_path, combined, delimiter=\",\", fmt=\"%f\")\n",
    "    print(f\"Saved {file_path} successfully!\")\n",
    "    \n",
    "# Prepare data\n",
    "def normalize_images(data):\n",
    "    return data / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot = np.zeros((labels.size, num_classes))\n",
    "    one_hot[np.arange(labels.size),labels] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10 dataset...\n",
      "Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "x_train, y_train, x_test, y_test = load_cifar10()\n",
    "\n",
    "# Preprocess data\n",
    "print(\"Preprocessing data...\")\n",
    "x_train = normalize_images(x_train).reshape(x_train.shape[0], -1)\n",
    "x_test = normalize_images(x_test).reshape(x_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to CSV...\n",
      "Saved train.csv successfully!\n",
      "Saved test.csv successfully!\n",
      "CSV files created.\n"
     ]
    }
   ],
   "source": [
    "# Save csv files if needed\n",
    "print(\"Saving to CSV...\")\n",
    "save_to_csv(x_train, y_train, \"train.csv\")\n",
    "save_to_csv(x_test, y_test, \"test.csv\")\n",
    "print(\"CSV files created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    num_classes = y_true.shape[1]\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    y_true_indices = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    for t, p in zip(y_true_indices, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true_label = np.argmax(y_true, axis=1)  #Converting one-hot encoded back to label encoded\n",
    "    return np.sum(y_true_label == y_pred) / len(y_true_label)\n",
    "\n",
    "\n",
    "def precision(cm):\n",
    "    \n",
    "    precisions = []\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        if tp + fp > 0:\n",
    "            precisions.append(tp / (tp + fp))\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "def recall(cm):\n",
    "    \n",
    "    recalls = []\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        if tp + fn > 0:\n",
    "            recalls.append(tp / (tp + fn))\n",
    "        else:\n",
    "            recalls.append(0)\n",
    "    return np.mean(recalls)\n",
    "\n",
    "def f1_score(cm):\n",
    "    \n",
    "    precisions = precision(cm)\n",
    "    recalls = recall(cm)\n",
    "    \n",
    "    if (precisions + recalls) > 0:\n",
    "        return 2 * (precisions * recalls) / (precisions + recalls) \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        #Weight Initalization\n",
    "        self.num_features = None\n",
    "        self.num_classes = None\n",
    "        self.weights = None\n",
    "\n",
    "        # Initialize lists for tracking losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    # Logistic Regression Functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predictProb(self, X):\n",
    "        y_pred = self.softmax(np.dot(X, self.weights))\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, y_pred):\n",
    "        return np.argmax(self.predictProb(y_pred), axis=1)\n",
    "\n",
    "    def log_loss(self, y, y_pred, epsilon=1e-15):\n",
    "        return -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
    "    \n",
    "    def categorical_crossentropy(self, y, y_pred, epsilon=1e-15):\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon) \n",
    "        return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  \n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def gradient_descent(self, X, y, weights, lr):\n",
    "        y_pred = self.softmax(np.dot(X, weights))\n",
    "        error = y_pred - y\n",
    "        gradient = np.dot(X.T, error) / len(y)\n",
    "        return weights - lr * gradient\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test=None, y_test=None):\n",
    "        self.num_features = X_train.shape[1]\n",
    "        self.num_classes = y_train.shape[1]\n",
    "        self.weights = np.random.randn(self.num_features, self.num_classes)\n",
    "    \n",
    "        for epoch in range(self.epochs):\n",
    "            # Update Weights\n",
    "            self.weights = self.gradient_descent(X_train, y_train, self.weights, self.learning_rate)\n",
    "\n",
    "            train_pred = self.softmax(np.dot(X_train, self.weights))\n",
    "            train_loss = self.categorical_crossentropy(y_train, train_pred)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_pred = self.softmax(np.dot(X_test, self.weights))\n",
    "                test_loss = self.categorical_crossentropy(y_test, test_pred)\n",
    "                self.test_losses.append(test_loss)\n",
    "            else:\n",
    "                test_loss = None\n",
    "\n",
    "            # if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = one_hot_encode(y_train, num_classes)\n",
    "y_test = one_hot_encode(y_test, num_classes)\n",
    "\n",
    "# Add biases to X\n",
    "X_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
    "X_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Train Loss = 23.9671, Test Loss = 23.9026\n",
      "Epoch 2/1000: Train Loss = 20.7714, Test Loss = 20.6149\n",
      "Epoch 3/1000: Train Loss = 15.4320, Test Loss = 15.1726\n",
      "Epoch 4/1000: Train Loss = 14.4948, Test Loss = 14.2649\n",
      "Epoch 5/1000: Train Loss = 14.2964, Test Loss = 14.0537\n",
      "Epoch 6/1000: Train Loss = 14.1761, Test Loss = 13.9320\n",
      "Epoch 7/1000: Train Loss = 14.0597, Test Loss = 13.8143\n",
      "Epoch 8/1000: Train Loss = 13.9504, Test Loss = 13.7033\n",
      "Epoch 9/1000: Train Loss = 13.8431, Test Loss = 13.5954\n",
      "Epoch 10/1000: Train Loss = 13.7398, Test Loss = 13.4913\n",
      "Epoch 11/1000: Train Loss = 13.6394, Test Loss = 13.3903\n",
      "Epoch 12/1000: Train Loss = 13.5423, Test Loss = 13.2924\n",
      "Epoch 13/1000: Train Loss = 13.4479, Test Loss = 13.1975\n",
      "Epoch 14/1000: Train Loss = 13.3565, Test Loss = 13.1054\n",
      "Epoch 15/1000: Train Loss = 13.2677, Test Loss = 13.0159\n",
      "Epoch 16/1000: Train Loss = 13.1816, Test Loss = 12.9290\n",
      "Epoch 17/1000: Train Loss = 13.0979, Test Loss = 12.8446\n",
      "Epoch 18/1000: Train Loss = 13.0165, Test Loss = 12.7625\n",
      "Epoch 19/1000: Train Loss = 12.9374, Test Loss = 12.6826\n",
      "Epoch 20/1000: Train Loss = 12.8603, Test Loss = 12.6049\n",
      "Epoch 21/1000: Train Loss = 12.7851, Test Loss = 12.5292\n",
      "Epoch 22/1000: Train Loss = 12.7118, Test Loss = 12.4555\n",
      "Epoch 23/1000: Train Loss = 12.6404, Test Loss = 12.3838\n",
      "Epoch 24/1000: Train Loss = 12.5706, Test Loss = 12.3138\n",
      "Epoch 25/1000: Train Loss = 12.5025, Test Loss = 12.2457\n",
      "Epoch 26/1000: Train Loss = 12.4359, Test Loss = 12.1793\n",
      "Epoch 27/1000: Train Loss = 12.3709, Test Loss = 12.1145\n",
      "Epoch 28/1000: Train Loss = 12.3075, Test Loss = 12.0513\n",
      "Epoch 29/1000: Train Loss = 12.2454, Test Loss = 11.9895\n",
      "Epoch 30/1000: Train Loss = 12.1846, Test Loss = 11.9291\n",
      "Epoch 31/1000: Train Loss = 12.1251, Test Loss = 11.8701\n",
      "Epoch 32/1000: Train Loss = 12.0669, Test Loss = 11.8124\n",
      "Epoch 33/1000: Train Loss = 12.0099, Test Loss = 11.7559\n",
      "Epoch 34/1000: Train Loss = 11.9539, Test Loss = 11.7005\n",
      "Epoch 35/1000: Train Loss = 11.8991, Test Loss = 11.6463\n",
      "Epoch 36/1000: Train Loss = 11.8453, Test Loss = 11.5932\n",
      "Epoch 37/1000: Train Loss = 11.7926, Test Loss = 11.5412\n",
      "Epoch 38/1000: Train Loss = 11.7408, Test Loss = 11.4901\n",
      "Epoch 39/1000: Train Loss = 11.6900, Test Loss = 11.4399\n",
      "Epoch 40/1000: Train Loss = 11.6401, Test Loss = 11.3908\n",
      "Epoch 41/1000: Train Loss = 11.5911, Test Loss = 11.3425\n",
      "Epoch 42/1000: Train Loss = 11.5430, Test Loss = 11.2951\n",
      "Epoch 43/1000: Train Loss = 11.4957, Test Loss = 11.2485\n",
      "Epoch 44/1000: Train Loss = 11.4493, Test Loss = 11.2028\n",
      "Epoch 45/1000: Train Loss = 11.4036, Test Loss = 11.1578\n",
      "Epoch 46/1000: Train Loss = 11.3587, Test Loss = 11.1135\n",
      "Epoch 47/1000: Train Loss = 11.3145, Test Loss = 11.0701\n",
      "Epoch 48/1000: Train Loss = 11.2711, Test Loss = 11.0273\n",
      "Epoch 49/1000: Train Loss = 11.2284, Test Loss = 10.9853\n",
      "Epoch 50/1000: Train Loss = 11.1864, Test Loss = 10.9440\n",
      "Epoch 51/1000: Train Loss = 11.1450, Test Loss = 10.9033\n",
      "Epoch 52/1000: Train Loss = 11.1043, Test Loss = 10.8632\n",
      "Epoch 53/1000: Train Loss = 11.0642, Test Loss = 10.8239\n",
      "Epoch 54/1000: Train Loss = 11.0247, Test Loss = 10.7851\n",
      "Epoch 55/1000: Train Loss = 10.9859, Test Loss = 10.7470\n",
      "Epoch 56/1000: Train Loss = 10.9476, Test Loss = 10.7095\n",
      "Epoch 57/1000: Train Loss = 10.9099, Test Loss = 10.6726\n",
      "Epoch 58/1000: Train Loss = 10.8728, Test Loss = 10.6363\n",
      "Epoch 59/1000: Train Loss = 10.8362, Test Loss = 10.6006\n",
      "Epoch 60/1000: Train Loss = 10.8001, Test Loss = 10.5654\n",
      "Epoch 61/1000: Train Loss = 10.7646, Test Loss = 10.5308\n",
      "Epoch 62/1000: Train Loss = 10.7296, Test Loss = 10.4967\n",
      "Epoch 63/1000: Train Loss = 10.6951, Test Loss = 10.4632\n",
      "Epoch 64/1000: Train Loss = 10.6611, Test Loss = 10.4301\n",
      "Epoch 65/1000: Train Loss = 10.6277, Test Loss = 10.3976\n",
      "Epoch 66/1000: Train Loss = 10.5947, Test Loss = 10.3657\n",
      "Epoch 67/1000: Train Loss = 10.5622, Test Loss = 10.3342\n",
      "Epoch 68/1000: Train Loss = 10.5301, Test Loss = 10.3032\n",
      "Epoch 69/1000: Train Loss = 10.4985, Test Loss = 10.2726\n",
      "Epoch 70/1000: Train Loss = 10.4674, Test Loss = 10.2425\n",
      "Epoch 71/1000: Train Loss = 10.4367, Test Loss = 10.2128\n",
      "Epoch 72/1000: Train Loss = 10.4063, Test Loss = 10.1836\n",
      "Epoch 73/1000: Train Loss = 10.3765, Test Loss = 10.1547\n",
      "Epoch 74/1000: Train Loss = 10.3470, Test Loss = 10.1263\n",
      "Epoch 75/1000: Train Loss = 10.3179, Test Loss = 10.0983\n",
      "Epoch 76/1000: Train Loss = 10.2892, Test Loss = 10.0707\n",
      "Epoch 77/1000: Train Loss = 10.2608, Test Loss = 10.0435\n",
      "Epoch 78/1000: Train Loss = 10.2329, Test Loss = 10.0167\n",
      "Epoch 79/1000: Train Loss = 10.2053, Test Loss = 9.9903\n",
      "Epoch 80/1000: Train Loss = 10.1781, Test Loss = 9.9642\n",
      "Epoch 81/1000: Train Loss = 10.1512, Test Loss = 9.9384\n",
      "Epoch 82/1000: Train Loss = 10.1246, Test Loss = 9.9130\n",
      "Epoch 83/1000: Train Loss = 10.0984, Test Loss = 9.8879\n",
      "Epoch 84/1000: Train Loss = 10.0726, Test Loss = 9.8631\n",
      "Epoch 85/1000: Train Loss = 10.0470, Test Loss = 9.8387\n",
      "Epoch 86/1000: Train Loss = 10.0218, Test Loss = 9.8146\n",
      "Epoch 87/1000: Train Loss = 9.9969, Test Loss = 9.7909\n",
      "Epoch 88/1000: Train Loss = 9.9722, Test Loss = 9.7674\n",
      "Epoch 89/1000: Train Loss = 9.9479, Test Loss = 9.7442\n",
      "Epoch 90/1000: Train Loss = 9.9239, Test Loss = 9.7214\n",
      "Epoch 91/1000: Train Loss = 9.9001, Test Loss = 9.6988\n",
      "Epoch 92/1000: Train Loss = 9.8767, Test Loss = 9.6765\n",
      "Epoch 93/1000: Train Loss = 9.8535, Test Loss = 9.6544\n",
      "Epoch 94/1000: Train Loss = 9.8306, Test Loss = 9.6327\n",
      "Epoch 95/1000: Train Loss = 9.8080, Test Loss = 9.6113\n",
      "Epoch 96/1000: Train Loss = 9.7856, Test Loss = 9.5900\n",
      "Epoch 97/1000: Train Loss = 9.7635, Test Loss = 9.5690\n",
      "Epoch 98/1000: Train Loss = 9.7417, Test Loss = 9.5483\n",
      "Epoch 99/1000: Train Loss = 9.7201, Test Loss = 9.5278\n",
      "Epoch 100/1000: Train Loss = 9.6987, Test Loss = 9.5075\n",
      "Epoch 101/1000: Train Loss = 9.6776, Test Loss = 9.4875\n",
      "Epoch 102/1000: Train Loss = 9.6568, Test Loss = 9.4677\n",
      "Epoch 103/1000: Train Loss = 9.6361, Test Loss = 9.4481\n",
      "Epoch 104/1000: Train Loss = 9.6157, Test Loss = 9.4287\n",
      "Epoch 105/1000: Train Loss = 9.5955, Test Loss = 9.4096\n",
      "Epoch 106/1000: Train Loss = 9.5755, Test Loss = 9.3906\n",
      "Epoch 107/1000: Train Loss = 9.5558, Test Loss = 9.3719\n",
      "Epoch 108/1000: Train Loss = 9.5362, Test Loss = 9.3533\n",
      "Epoch 109/1000: Train Loss = 9.5169, Test Loss = 9.3350\n",
      "Epoch 110/1000: Train Loss = 9.4978, Test Loss = 9.3168\n",
      "Epoch 111/1000: Train Loss = 9.4788, Test Loss = 9.2989\n",
      "Epoch 112/1000: Train Loss = 9.4601, Test Loss = 9.2811\n",
      "Epoch 113/1000: Train Loss = 9.4415, Test Loss = 9.2635\n",
      "Epoch 114/1000: Train Loss = 9.4232, Test Loss = 9.2461\n",
      "Epoch 115/1000: Train Loss = 9.4050, Test Loss = 9.2288\n",
      "Epoch 116/1000: Train Loss = 9.3870, Test Loss = 9.2118\n",
      "Epoch 117/1000: Train Loss = 9.3692, Test Loss = 9.1948\n",
      "Epoch 118/1000: Train Loss = 9.3516, Test Loss = 9.1781\n",
      "Epoch 119/1000: Train Loss = 9.3341, Test Loss = 9.1615\n",
      "Epoch 120/1000: Train Loss = 9.3168, Test Loss = 9.1450\n",
      "Epoch 121/1000: Train Loss = 9.2997, Test Loss = 9.1288\n",
      "Epoch 122/1000: Train Loss = 9.2827, Test Loss = 9.1126\n",
      "Epoch 123/1000: Train Loss = 9.2659, Test Loss = 9.0967\n",
      "Epoch 124/1000: Train Loss = 9.2493, Test Loss = 9.0808\n",
      "Epoch 125/1000: Train Loss = 9.2328, Test Loss = 9.0652\n",
      "Epoch 126/1000: Train Loss = 9.2165, Test Loss = 9.0497\n",
      "Epoch 127/1000: Train Loss = 9.2003, Test Loss = 9.0343\n",
      "Epoch 128/1000: Train Loss = 9.1843, Test Loss = 9.0190\n",
      "Epoch 129/1000: Train Loss = 9.1684, Test Loss = 9.0039\n",
      "Epoch 130/1000: Train Loss = 9.1526, Test Loss = 8.9890\n",
      "Epoch 131/1000: Train Loss = 9.1370, Test Loss = 8.9742\n",
      "Epoch 132/1000: Train Loss = 9.1215, Test Loss = 8.9595\n",
      "Epoch 133/1000: Train Loss = 9.1062, Test Loss = 8.9449\n",
      "Epoch 134/1000: Train Loss = 9.0910, Test Loss = 8.9305\n",
      "Epoch 135/1000: Train Loss = 9.0759, Test Loss = 8.9162\n",
      "Epoch 136/1000: Train Loss = 9.0610, Test Loss = 8.9020\n",
      "Epoch 137/1000: Train Loss = 9.0461, Test Loss = 8.8879\n",
      "Epoch 138/1000: Train Loss = 9.0314, Test Loss = 8.8740\n",
      "Epoch 139/1000: Train Loss = 9.0169, Test Loss = 8.8601\n",
      "Epoch 140/1000: Train Loss = 9.0024, Test Loss = 8.8464\n",
      "Epoch 141/1000: Train Loss = 8.9881, Test Loss = 8.8328\n",
      "Epoch 142/1000: Train Loss = 8.9739, Test Loss = 8.8193\n",
      "Epoch 143/1000: Train Loss = 8.9597, Test Loss = 8.8059\n",
      "Epoch 144/1000: Train Loss = 8.9458, Test Loss = 8.7926\n",
      "Epoch 145/1000: Train Loss = 8.9319, Test Loss = 8.7794\n",
      "Epoch 146/1000: Train Loss = 8.9181, Test Loss = 8.7663\n",
      "Epoch 147/1000: Train Loss = 8.9045, Test Loss = 8.7533\n",
      "Epoch 148/1000: Train Loss = 8.8909, Test Loss = 8.7404\n",
      "Epoch 149/1000: Train Loss = 8.8775, Test Loss = 8.7275\n",
      "Epoch 150/1000: Train Loss = 8.8642, Test Loss = 8.7148\n",
      "Epoch 151/1000: Train Loss = 8.8509, Test Loss = 8.7022\n",
      "Epoch 152/1000: Train Loss = 8.8378, Test Loss = 8.6896\n",
      "Epoch 153/1000: Train Loss = 8.8248, Test Loss = 8.6771\n",
      "Epoch 154/1000: Train Loss = 8.8118, Test Loss = 8.6648\n",
      "Epoch 155/1000: Train Loss = 8.7990, Test Loss = 8.6525\n",
      "Epoch 156/1000: Train Loss = 8.7862, Test Loss = 8.6403\n",
      "Epoch 157/1000: Train Loss = 8.7736, Test Loss = 8.6282\n",
      "Epoch 158/1000: Train Loss = 8.7610, Test Loss = 8.6162\n",
      "Epoch 159/1000: Train Loss = 8.7486, Test Loss = 8.6043\n",
      "Epoch 160/1000: Train Loss = 8.7362, Test Loss = 8.5924\n",
      "Epoch 161/1000: Train Loss = 8.7239, Test Loss = 8.5807\n",
      "Epoch 162/1000: Train Loss = 8.7116, Test Loss = 8.5690\n",
      "Epoch 163/1000: Train Loss = 8.6995, Test Loss = 8.5574\n",
      "Epoch 164/1000: Train Loss = 8.6875, Test Loss = 8.5459\n",
      "Epoch 165/1000: Train Loss = 8.6755, Test Loss = 8.5344\n",
      "Epoch 166/1000: Train Loss = 8.6636, Test Loss = 8.5230\n",
      "Epoch 167/1000: Train Loss = 8.6518, Test Loss = 8.5117\n",
      "Epoch 168/1000: Train Loss = 8.6401, Test Loss = 8.5005\n",
      "Epoch 169/1000: Train Loss = 8.6285, Test Loss = 8.4894\n",
      "Epoch 170/1000: Train Loss = 8.6169, Test Loss = 8.4783\n",
      "Epoch 171/1000: Train Loss = 8.6054, Test Loss = 8.4673\n",
      "Epoch 172/1000: Train Loss = 8.5940, Test Loss = 8.4564\n",
      "Epoch 173/1000: Train Loss = 8.5827, Test Loss = 8.4455\n",
      "Epoch 174/1000: Train Loss = 8.5714, Test Loss = 8.4347\n",
      "Epoch 175/1000: Train Loss = 8.5602, Test Loss = 8.4240\n",
      "Epoch 176/1000: Train Loss = 8.5491, Test Loss = 8.4134\n",
      "Epoch 177/1000: Train Loss = 8.5380, Test Loss = 8.4028\n",
      "Epoch 178/1000: Train Loss = 8.5270, Test Loss = 8.3923\n",
      "Epoch 179/1000: Train Loss = 8.5161, Test Loss = 8.3818\n",
      "Epoch 180/1000: Train Loss = 8.5053, Test Loss = 8.3714\n",
      "Epoch 181/1000: Train Loss = 8.4945, Test Loss = 8.3611\n",
      "Epoch 182/1000: Train Loss = 8.4838, Test Loss = 8.3509\n",
      "Epoch 183/1000: Train Loss = 8.4731, Test Loss = 8.3407\n",
      "Epoch 184/1000: Train Loss = 8.4625, Test Loss = 8.3306\n",
      "Epoch 185/1000: Train Loss = 8.4520, Test Loss = 8.3205\n",
      "Epoch 186/1000: Train Loss = 8.4415, Test Loss = 8.3105\n",
      "Epoch 187/1000: Train Loss = 8.4311, Test Loss = 8.3006\n",
      "Epoch 188/1000: Train Loss = 8.4208, Test Loss = 8.2907\n",
      "Epoch 189/1000: Train Loss = 8.4105, Test Loss = 8.2808\n",
      "Epoch 190/1000: Train Loss = 8.4003, Test Loss = 8.2711\n",
      "Epoch 191/1000: Train Loss = 8.3901, Test Loss = 8.2614\n",
      "Epoch 192/1000: Train Loss = 8.3800, Test Loss = 8.2517\n",
      "Epoch 193/1000: Train Loss = 8.3700, Test Loss = 8.2421\n",
      "Epoch 194/1000: Train Loss = 8.3600, Test Loss = 8.2326\n",
      "Epoch 195/1000: Train Loss = 8.3501, Test Loss = 8.2231\n",
      "Epoch 196/1000: Train Loss = 8.3402, Test Loss = 8.2136\n",
      "Epoch 197/1000: Train Loss = 8.3304, Test Loss = 8.2043\n",
      "Epoch 198/1000: Train Loss = 8.3206, Test Loss = 8.1949\n",
      "Epoch 199/1000: Train Loss = 8.3109, Test Loss = 8.1857\n",
      "Epoch 200/1000: Train Loss = 8.3012, Test Loss = 8.1764\n",
      "Epoch 201/1000: Train Loss = 8.2916, Test Loss = 8.1673\n",
      "Epoch 202/1000: Train Loss = 8.2821, Test Loss = 8.1581\n",
      "Epoch 203/1000: Train Loss = 8.2726, Test Loss = 8.1491\n",
      "Epoch 204/1000: Train Loss = 8.2631, Test Loss = 8.1400\n",
      "Epoch 205/1000: Train Loss = 8.2537, Test Loss = 8.1311\n",
      "Epoch 206/1000: Train Loss = 8.2444, Test Loss = 8.1221\n",
      "Epoch 207/1000: Train Loss = 8.2351, Test Loss = 8.1133\n",
      "Epoch 208/1000: Train Loss = 8.2259, Test Loss = 8.1044\n",
      "Epoch 209/1000: Train Loss = 8.2167, Test Loss = 8.0957\n",
      "Epoch 210/1000: Train Loss = 8.2075, Test Loss = 8.0869\n",
      "Epoch 211/1000: Train Loss = 8.1984, Test Loss = 8.0783\n",
      "Epoch 212/1000: Train Loss = 8.1893, Test Loss = 8.0696\n",
      "Epoch 213/1000: Train Loss = 8.1803, Test Loss = 8.0610\n",
      "Epoch 214/1000: Train Loss = 8.1714, Test Loss = 8.0525\n",
      "Epoch 215/1000: Train Loss = 8.1624, Test Loss = 8.0440\n",
      "Epoch 216/1000: Train Loss = 8.1536, Test Loss = 8.0356\n",
      "Epoch 217/1000: Train Loss = 8.1447, Test Loss = 8.0271\n",
      "Epoch 218/1000: Train Loss = 8.1359, Test Loss = 8.0188\n",
      "Epoch 219/1000: Train Loss = 8.1272, Test Loss = 8.0105\n",
      "Epoch 220/1000: Train Loss = 8.1185, Test Loss = 8.0022\n",
      "Epoch 221/1000: Train Loss = 8.1099, Test Loss = 7.9939\n",
      "Epoch 222/1000: Train Loss = 8.1012, Test Loss = 7.9857\n",
      "Epoch 223/1000: Train Loss = 8.0927, Test Loss = 7.9776\n",
      "Epoch 224/1000: Train Loss = 8.0842, Test Loss = 7.9695\n",
      "Epoch 225/1000: Train Loss = 8.0757, Test Loss = 7.9614\n",
      "Epoch 226/1000: Train Loss = 8.0672, Test Loss = 7.9534\n",
      "Epoch 227/1000: Train Loss = 8.0588, Test Loss = 7.9454\n",
      "Epoch 228/1000: Train Loss = 8.0505, Test Loss = 7.9374\n",
      "Epoch 229/1000: Train Loss = 8.0421, Test Loss = 7.9295\n",
      "Epoch 230/1000: Train Loss = 8.0339, Test Loss = 7.9216\n",
      "Epoch 231/1000: Train Loss = 8.0256, Test Loss = 7.9138\n",
      "Epoch 232/1000: Train Loss = 8.0174, Test Loss = 7.9060\n",
      "Epoch 233/1000: Train Loss = 8.0093, Test Loss = 7.8982\n",
      "Epoch 234/1000: Train Loss = 8.0011, Test Loss = 7.8905\n",
      "Epoch 235/1000: Train Loss = 7.9930, Test Loss = 7.8828\n",
      "Epoch 236/1000: Train Loss = 7.9850, Test Loss = 7.8752\n",
      "Epoch 237/1000: Train Loss = 7.9770, Test Loss = 7.8676\n",
      "Epoch 238/1000: Train Loss = 7.9690, Test Loss = 7.8600\n",
      "Epoch 239/1000: Train Loss = 7.9610, Test Loss = 7.8524\n",
      "Epoch 240/1000: Train Loss = 7.9531, Test Loss = 7.8449\n",
      "Epoch 241/1000: Train Loss = 7.9453, Test Loss = 7.8374\n",
      "Epoch 242/1000: Train Loss = 7.9374, Test Loss = 7.8300\n",
      "Epoch 243/1000: Train Loss = 7.9296, Test Loss = 7.8226\n",
      "Epoch 244/1000: Train Loss = 7.9219, Test Loss = 7.8152\n",
      "Epoch 245/1000: Train Loss = 7.9141, Test Loss = 7.8079\n",
      "Epoch 246/1000: Train Loss = 7.9064, Test Loss = 7.8006\n",
      "Epoch 247/1000: Train Loss = 7.8988, Test Loss = 7.7933\n",
      "Epoch 248/1000: Train Loss = 7.8912, Test Loss = 7.7861\n",
      "Epoch 249/1000: Train Loss = 7.8836, Test Loss = 7.7789\n",
      "Epoch 250/1000: Train Loss = 7.8760, Test Loss = 7.7717\n",
      "Epoch 251/1000: Train Loss = 7.8685, Test Loss = 7.7646\n",
      "Epoch 252/1000: Train Loss = 7.8610, Test Loss = 7.7575\n",
      "Epoch 253/1000: Train Loss = 7.8535, Test Loss = 7.7504\n",
      "Epoch 254/1000: Train Loss = 7.8461, Test Loss = 7.7433\n",
      "Epoch 255/1000: Train Loss = 7.8387, Test Loss = 7.7364\n",
      "Epoch 256/1000: Train Loss = 7.8313, Test Loss = 7.7293\n",
      "Epoch 257/1000: Train Loss = 7.8240, Test Loss = 7.7225\n",
      "Epoch 258/1000: Train Loss = 7.8167, Test Loss = 7.7154\n",
      "Epoch 259/1000: Train Loss = 7.8095, Test Loss = 7.7088\n",
      "Epoch 260/1000: Train Loss = 7.8023, Test Loss = 7.7017\n",
      "Epoch 261/1000: Train Loss = 7.7952, Test Loss = 7.6953\n",
      "Epoch 262/1000: Train Loss = 7.7882, Test Loss = 7.6882\n",
      "Epoch 263/1000: Train Loss = 7.7814, Test Loss = 7.6825\n",
      "Epoch 264/1000: Train Loss = 7.7749, Test Loss = 7.6755\n",
      "Epoch 265/1000: Train Loss = 7.7692, Test Loss = 7.6715\n",
      "Epoch 266/1000: Train Loss = 7.7645, Test Loss = 7.6653\n",
      "Epoch 267/1000: Train Loss = 7.7624, Test Loss = 7.6664\n",
      "Epoch 268/1000: Train Loss = 7.7630, Test Loss = 7.6639\n",
      "Epoch 269/1000: Train Loss = 7.7733, Test Loss = 7.6800\n",
      "Epoch 270/1000: Train Loss = 7.7859, Test Loss = 7.6867\n",
      "Epoch 271/1000: Train Loss = 7.8288, Test Loss = 7.7396\n",
      "Epoch 272/1000: Train Loss = 7.8415, Test Loss = 7.7424\n",
      "Epoch 273/1000: Train Loss = 7.9227, Test Loss = 7.8375\n",
      "Epoch 274/1000: Train Loss = 7.8743, Test Loss = 7.7762\n",
      "Epoch 275/1000: Train Loss = 7.9493, Test Loss = 7.8654\n",
      "Epoch 276/1000: Train Loss = 7.8683, Test Loss = 7.7707\n",
      "Epoch 277/1000: Train Loss = 7.9334, Test Loss = 7.8496\n",
      "Epoch 278/1000: Train Loss = 7.8589, Test Loss = 7.7616\n",
      "Epoch 279/1000: Train Loss = 7.9256, Test Loss = 7.8423\n",
      "Epoch 280/1000: Train Loss = 7.8482, Test Loss = 7.7512\n",
      "Epoch 281/1000: Train Loss = 7.9143, Test Loss = 7.8314\n",
      "Epoch 282/1000: Train Loss = 7.8371, Test Loss = 7.7404\n",
      "Epoch 283/1000: Train Loss = 7.9034, Test Loss = 7.8210\n",
      "Epoch 284/1000: Train Loss = 7.8257, Test Loss = 7.7294\n",
      "Epoch 285/1000: Train Loss = 7.8920, Test Loss = 7.8101\n",
      "Epoch 286/1000: Train Loss = 7.8142, Test Loss = 7.7184\n",
      "Epoch 287/1000: Train Loss = 7.8806, Test Loss = 7.7993\n",
      "Epoch 288/1000: Train Loss = 7.8028, Test Loss = 7.7073\n",
      "Epoch 289/1000: Train Loss = 7.8692, Test Loss = 7.7885\n",
      "Epoch 290/1000: Train Loss = 7.7914, Test Loss = 7.6963\n",
      "Epoch 291/1000: Train Loss = 7.8579, Test Loss = 7.7777\n",
      "Epoch 292/1000: Train Loss = 7.7800, Test Loss = 7.6854\n",
      "Epoch 293/1000: Train Loss = 7.8466, Test Loss = 7.7670\n",
      "Epoch 294/1000: Train Loss = 7.7688, Test Loss = 7.6746\n",
      "Epoch 295/1000: Train Loss = 7.8354, Test Loss = 7.7565\n",
      "Epoch 296/1000: Train Loss = 7.7577, Test Loss = 7.6639\n",
      "Epoch 297/1000: Train Loss = 7.8244, Test Loss = 7.7461\n",
      "Epoch 298/1000: Train Loss = 7.7467, Test Loss = 7.6533\n",
      "Epoch 299/1000: Train Loss = 7.8134, Test Loss = 7.7358\n",
      "Epoch 300/1000: Train Loss = 7.7358, Test Loss = 7.6429\n",
      "Epoch 301/1000: Train Loss = 7.8027, Test Loss = 7.7256\n",
      "Epoch 302/1000: Train Loss = 7.7251, Test Loss = 7.6326\n",
      "Epoch 303/1000: Train Loss = 7.7920, Test Loss = 7.7156\n",
      "Epoch 304/1000: Train Loss = 7.7145, Test Loss = 7.6225\n",
      "Epoch 305/1000: Train Loss = 7.7815, Test Loss = 7.7058\n",
      "Epoch 306/1000: Train Loss = 7.7041, Test Loss = 7.6124\n",
      "Epoch 307/1000: Train Loss = 7.7711, Test Loss = 7.6961\n",
      "Epoch 308/1000: Train Loss = 7.6938, Test Loss = 7.6026\n",
      "Epoch 309/1000: Train Loss = 7.7609, Test Loss = 7.6865\n",
      "Epoch 310/1000: Train Loss = 7.6836, Test Loss = 7.5928\n",
      "Epoch 311/1000: Train Loss = 7.7508, Test Loss = 7.6771\n",
      "Epoch 312/1000: Train Loss = 7.6736, Test Loss = 7.5833\n",
      "Epoch 313/1000: Train Loss = 7.7408, Test Loss = 7.6678\n",
      "Epoch 314/1000: Train Loss = 7.6637, Test Loss = 7.5738\n",
      "Epoch 315/1000: Train Loss = 7.7310, Test Loss = 7.6586\n",
      "Epoch 316/1000: Train Loss = 7.6539, Test Loss = 7.5645\n",
      "Epoch 317/1000: Train Loss = 7.7213, Test Loss = 7.6496\n",
      "Epoch 318/1000: Train Loss = 7.6443, Test Loss = 7.5554\n",
      "Epoch 319/1000: Train Loss = 7.7117, Test Loss = 7.6407\n",
      "Epoch 320/1000: Train Loss = 7.6349, Test Loss = 7.5463\n",
      "Epoch 321/1000: Train Loss = 7.7023, Test Loss = 7.6319\n",
      "Epoch 322/1000: Train Loss = 7.6256, Test Loss = 7.5374\n",
      "Epoch 323/1000: Train Loss = 7.6930, Test Loss = 7.6233\n",
      "Epoch 324/1000: Train Loss = 7.6164, Test Loss = 7.5287\n",
      "Epoch 325/1000: Train Loss = 7.6838, Test Loss = 7.6148\n",
      "Epoch 326/1000: Train Loss = 7.6073, Test Loss = 7.5201\n",
      "Epoch 327/1000: Train Loss = 7.6748, Test Loss = 7.6064\n",
      "Epoch 328/1000: Train Loss = 7.5984, Test Loss = 7.5116\n",
      "Epoch 329/1000: Train Loss = 7.6658, Test Loss = 7.5981\n",
      "Epoch 330/1000: Train Loss = 7.5896, Test Loss = 7.5032\n",
      "Epoch 331/1000: Train Loss = 7.6570, Test Loss = 7.5900\n",
      "Epoch 332/1000: Train Loss = 7.5809, Test Loss = 7.4950\n",
      "Epoch 333/1000: Train Loss = 7.6483, Test Loss = 7.5819\n",
      "Epoch 334/1000: Train Loss = 7.5724, Test Loss = 7.4869\n",
      "Epoch 335/1000: Train Loss = 7.6396, Test Loss = 7.5739\n",
      "Epoch 336/1000: Train Loss = 7.5639, Test Loss = 7.4788\n",
      "Epoch 337/1000: Train Loss = 7.6311, Test Loss = 7.5661\n",
      "Epoch 338/1000: Train Loss = 7.5556, Test Loss = 7.4709\n",
      "Epoch 339/1000: Train Loss = 7.6227, Test Loss = 7.5583\n",
      "Epoch 340/1000: Train Loss = 7.5473, Test Loss = 7.4631\n",
      "Epoch 341/1000: Train Loss = 7.6143, Test Loss = 7.5506\n",
      "Epoch 342/1000: Train Loss = 7.5392, Test Loss = 7.4554\n",
      "Epoch 343/1000: Train Loss = 7.6061, Test Loss = 7.5430\n",
      "Epoch 344/1000: Train Loss = 7.5311, Test Loss = 7.4477\n",
      "Epoch 345/1000: Train Loss = 7.5979, Test Loss = 7.5354\n",
      "Epoch 346/1000: Train Loss = 7.5231, Test Loss = 7.4401\n",
      "Epoch 347/1000: Train Loss = 7.5898, Test Loss = 7.5280\n",
      "Epoch 348/1000: Train Loss = 7.5152, Test Loss = 7.4326\n",
      "Epoch 349/1000: Train Loss = 7.5817, Test Loss = 7.5206\n",
      "Epoch 350/1000: Train Loss = 7.5074, Test Loss = 7.4252\n",
      "Epoch 351/1000: Train Loss = 7.5738, Test Loss = 7.5132\n",
      "Epoch 352/1000: Train Loss = 7.4996, Test Loss = 7.4179\n",
      "Epoch 353/1000: Train Loss = 7.5658, Test Loss = 7.5059\n",
      "Epoch 354/1000: Train Loss = 7.4919, Test Loss = 7.4106\n",
      "Epoch 355/1000: Train Loss = 7.5580, Test Loss = 7.4987\n",
      "Epoch 356/1000: Train Loss = 7.4842, Test Loss = 7.4033\n",
      "Epoch 357/1000: Train Loss = 7.5502, Test Loss = 7.4915\n",
      "Epoch 358/1000: Train Loss = 7.4766, Test Loss = 7.3961\n",
      "Epoch 359/1000: Train Loss = 7.5424, Test Loss = 7.4843\n",
      "Epoch 360/1000: Train Loss = 7.4690, Test Loss = 7.3890\n",
      "Epoch 361/1000: Train Loss = 7.5347, Test Loss = 7.4772\n",
      "Epoch 362/1000: Train Loss = 7.4615, Test Loss = 7.3819\n",
      "Epoch 363/1000: Train Loss = 7.5271, Test Loss = 7.4702\n",
      "Epoch 364/1000: Train Loss = 7.4541, Test Loss = 7.3749\n",
      "Epoch 365/1000: Train Loss = 7.5195, Test Loss = 7.4632\n",
      "Epoch 366/1000: Train Loss = 7.4467, Test Loss = 7.3679\n",
      "Epoch 367/1000: Train Loss = 7.5119, Test Loss = 7.4562\n",
      "Epoch 368/1000: Train Loss = 7.4393, Test Loss = 7.3610\n",
      "Epoch 369/1000: Train Loss = 7.5044, Test Loss = 7.4493\n",
      "Epoch 370/1000: Train Loss = 7.4320, Test Loss = 7.3541\n",
      "Epoch 371/1000: Train Loss = 7.4970, Test Loss = 7.4424\n",
      "Epoch 372/1000: Train Loss = 7.4248, Test Loss = 7.3472\n",
      "Epoch 373/1000: Train Loss = 7.4896, Test Loss = 7.4355\n",
      "Epoch 374/1000: Train Loss = 7.4176, Test Loss = 7.3404\n",
      "Epoch 375/1000: Train Loss = 7.4822, Test Loss = 7.4288\n",
      "Epoch 376/1000: Train Loss = 7.4104, Test Loss = 7.3337\n",
      "Epoch 377/1000: Train Loss = 7.4749, Test Loss = 7.4220\n",
      "Epoch 378/1000: Train Loss = 7.4033, Test Loss = 7.3270\n",
      "Epoch 379/1000: Train Loss = 7.4677, Test Loss = 7.4153\n",
      "Epoch 380/1000: Train Loss = 7.3962, Test Loss = 7.3203\n",
      "Epoch 381/1000: Train Loss = 7.4605, Test Loss = 7.4087\n",
      "Epoch 382/1000: Train Loss = 7.3892, Test Loss = 7.3137\n",
      "Epoch 383/1000: Train Loss = 7.4533, Test Loss = 7.4020\n",
      "Epoch 384/1000: Train Loss = 7.3822, Test Loss = 7.3071\n",
      "Epoch 385/1000: Train Loss = 7.4462, Test Loss = 7.3955\n",
      "Epoch 386/1000: Train Loss = 7.3753, Test Loss = 7.3006\n",
      "Epoch 387/1000: Train Loss = 7.4391, Test Loss = 7.3889\n",
      "Epoch 388/1000: Train Loss = 7.3684, Test Loss = 7.2941\n",
      "Epoch 389/1000: Train Loss = 7.4321, Test Loss = 7.3824\n",
      "Epoch 390/1000: Train Loss = 7.3616, Test Loss = 7.2876\n",
      "Epoch 391/1000: Train Loss = 7.4251, Test Loss = 7.3760\n",
      "Epoch 392/1000: Train Loss = 7.3548, Test Loss = 7.2812\n",
      "Epoch 393/1000: Train Loss = 7.4181, Test Loss = 7.3696\n",
      "Epoch 394/1000: Train Loss = 7.3480, Test Loss = 7.2749\n",
      "Epoch 395/1000: Train Loss = 7.4112, Test Loss = 7.3632\n",
      "Epoch 396/1000: Train Loss = 7.3413, Test Loss = 7.2685\n",
      "Epoch 397/1000: Train Loss = 7.4044, Test Loss = 7.3569\n",
      "Epoch 398/1000: Train Loss = 7.3347, Test Loss = 7.2622\n",
      "Epoch 399/1000: Train Loss = 7.3976, Test Loss = 7.3506\n",
      "Epoch 400/1000: Train Loss = 7.3280, Test Loss = 7.2560\n",
      "Epoch 401/1000: Train Loss = 7.3908, Test Loss = 7.3443\n",
      "Epoch 402/1000: Train Loss = 7.3215, Test Loss = 7.2498\n",
      "Epoch 403/1000: Train Loss = 7.3841, Test Loss = 7.3381\n",
      "Epoch 404/1000: Train Loss = 7.3149, Test Loss = 7.2436\n",
      "Epoch 405/1000: Train Loss = 7.3774, Test Loss = 7.3320\n",
      "Epoch 406/1000: Train Loss = 7.3084, Test Loss = 7.2375\n",
      "Epoch 407/1000: Train Loss = 7.3708, Test Loss = 7.3258\n",
      "Epoch 408/1000: Train Loss = 7.3020, Test Loss = 7.2314\n",
      "Epoch 409/1000: Train Loss = 7.3641, Test Loss = 7.3197\n",
      "Epoch 410/1000: Train Loss = 7.2955, Test Loss = 7.2254\n",
      "Epoch 411/1000: Train Loss = 7.3576, Test Loss = 7.3137\n",
      "Epoch 412/1000: Train Loss = 7.2892, Test Loss = 7.2193\n",
      "Epoch 413/1000: Train Loss = 7.3511, Test Loss = 7.3076\n",
      "Epoch 414/1000: Train Loss = 7.2828, Test Loss = 7.2134\n",
      "Epoch 415/1000: Train Loss = 7.3446, Test Loss = 7.3017\n",
      "Epoch 416/1000: Train Loss = 7.2765, Test Loss = 7.2075\n",
      "Epoch 417/1000: Train Loss = 7.3382, Test Loss = 7.2957\n",
      "Epoch 418/1000: Train Loss = 7.2703, Test Loss = 7.2016\n",
      "Epoch 419/1000: Train Loss = 7.3318, Test Loss = 7.2898\n",
      "Epoch 420/1000: Train Loss = 7.2641, Test Loss = 7.1957\n",
      "Epoch 421/1000: Train Loss = 7.3254, Test Loss = 7.2839\n",
      "Epoch 422/1000: Train Loss = 7.2579, Test Loss = 7.1899\n",
      "Epoch 423/1000: Train Loss = 7.3191, Test Loss = 7.2781\n",
      "Epoch 424/1000: Train Loss = 7.2518, Test Loss = 7.1841\n",
      "Epoch 425/1000: Train Loss = 7.3128, Test Loss = 7.2723\n",
      "Epoch 426/1000: Train Loss = 7.2457, Test Loss = 7.1784\n",
      "Epoch 427/1000: Train Loss = 7.3066, Test Loss = 7.2665\n",
      "Epoch 428/1000: Train Loss = 7.2396, Test Loss = 7.1727\n",
      "Epoch 429/1000: Train Loss = 7.3004, Test Loss = 7.2608\n",
      "Epoch 430/1000: Train Loss = 7.2336, Test Loss = 7.1670\n",
      "Epoch 431/1000: Train Loss = 7.2942, Test Loss = 7.2551\n",
      "Epoch 432/1000: Train Loss = 7.2276, Test Loss = 7.1614\n",
      "Epoch 433/1000: Train Loss = 7.2881, Test Loss = 7.2494\n",
      "Epoch 434/1000: Train Loss = 7.2216, Test Loss = 7.1558\n",
      "Epoch 435/1000: Train Loss = 7.2820, Test Loss = 7.2438\n",
      "Epoch 436/1000: Train Loss = 7.2157, Test Loss = 7.1503\n",
      "Epoch 437/1000: Train Loss = 7.2759, Test Loss = 7.2382\n",
      "Epoch 438/1000: Train Loss = 7.2098, Test Loss = 7.1447\n",
      "Epoch 439/1000: Train Loss = 7.2699, Test Loss = 7.2326\n",
      "Epoch 440/1000: Train Loss = 7.2040, Test Loss = 7.1393\n",
      "Epoch 441/1000: Train Loss = 7.2640, Test Loss = 7.2271\n",
      "Epoch 442/1000: Train Loss = 7.1982, Test Loss = 7.1338\n",
      "Epoch 443/1000: Train Loss = 7.2580, Test Loss = 7.2216\n",
      "Epoch 444/1000: Train Loss = 7.1924, Test Loss = 7.1284\n",
      "Epoch 445/1000: Train Loss = 7.2521, Test Loss = 7.2161\n",
      "Epoch 446/1000: Train Loss = 7.1866, Test Loss = 7.1230\n",
      "Epoch 447/1000: Train Loss = 7.2463, Test Loss = 7.2107\n",
      "Epoch 448/1000: Train Loss = 7.1809, Test Loss = 7.1177\n",
      "Epoch 449/1000: Train Loss = 7.2404, Test Loss = 7.2053\n",
      "Epoch 450/1000: Train Loss = 7.1753, Test Loss = 7.1123\n",
      "Epoch 451/1000: Train Loss = 7.2347, Test Loss = 7.2000\n",
      "Epoch 452/1000: Train Loss = 7.1696, Test Loss = 7.1071\n",
      "Epoch 453/1000: Train Loss = 7.2289, Test Loss = 7.1946\n",
      "Epoch 454/1000: Train Loss = 7.1640, Test Loss = 7.1018\n",
      "Epoch 455/1000: Train Loss = 7.2232, Test Loss = 7.1893\n",
      "Epoch 456/1000: Train Loss = 7.1584, Test Loss = 7.0966\n",
      "Epoch 457/1000: Train Loss = 7.2175, Test Loss = 7.1841\n",
      "Epoch 458/1000: Train Loss = 7.1529, Test Loss = 7.0914\n",
      "Epoch 459/1000: Train Loss = 7.2119, Test Loss = 7.1788\n",
      "Epoch 460/1000: Train Loss = 7.1474, Test Loss = 7.0862\n",
      "Epoch 461/1000: Train Loss = 7.2063, Test Loss = 7.1736\n",
      "Epoch 462/1000: Train Loss = 7.1419, Test Loss = 7.0811\n",
      "Epoch 463/1000: Train Loss = 7.2007, Test Loss = 7.1685\n",
      "Epoch 464/1000: Train Loss = 7.1365, Test Loss = 7.0760\n",
      "Epoch 465/1000: Train Loss = 7.1952, Test Loss = 7.1633\n",
      "Epoch 466/1000: Train Loss = 7.1311, Test Loss = 7.0710\n",
      "Epoch 467/1000: Train Loss = 7.1897, Test Loss = 7.1582\n",
      "Epoch 468/1000: Train Loss = 7.1257, Test Loss = 7.0659\n",
      "Epoch 469/1000: Train Loss = 7.1842, Test Loss = 7.1531\n",
      "Epoch 470/1000: Train Loss = 7.1203, Test Loss = 7.0609\n",
      "Epoch 471/1000: Train Loss = 7.1788, Test Loss = 7.1481\n",
      "Epoch 472/1000: Train Loss = 7.1150, Test Loss = 7.0559\n",
      "Epoch 473/1000: Train Loss = 7.1734, Test Loss = 7.1431\n",
      "Epoch 474/1000: Train Loss = 7.1097, Test Loss = 7.0510\n",
      "Epoch 475/1000: Train Loss = 7.1680, Test Loss = 7.1381\n",
      "Epoch 476/1000: Train Loss = 7.1045, Test Loss = 7.0461\n",
      "Epoch 477/1000: Train Loss = 7.1627, Test Loss = 7.1332\n",
      "Epoch 478/1000: Train Loss = 7.0992, Test Loss = 7.0412\n",
      "Epoch 479/1000: Train Loss = 7.1574, Test Loss = 7.1283\n",
      "Epoch 480/1000: Train Loss = 7.0940, Test Loss = 7.0363\n",
      "Epoch 481/1000: Train Loss = 7.1522, Test Loss = 7.1234\n",
      "Epoch 482/1000: Train Loss = 7.0889, Test Loss = 7.0315\n",
      "Epoch 483/1000: Train Loss = 7.1470, Test Loss = 7.1185\n",
      "Epoch 484/1000: Train Loss = 7.0837, Test Loss = 7.0267\n",
      "Epoch 485/1000: Train Loss = 7.1418, Test Loss = 7.1137\n",
      "Epoch 486/1000: Train Loss = 7.0786, Test Loss = 7.0219\n",
      "Epoch 487/1000: Train Loss = 7.1367, Test Loss = 7.1089\n",
      "Epoch 488/1000: Train Loss = 7.0736, Test Loss = 7.0172\n",
      "Epoch 489/1000: Train Loss = 7.1316, Test Loss = 7.1042\n",
      "Epoch 490/1000: Train Loss = 7.0685, Test Loss = 7.0125\n",
      "Epoch 491/1000: Train Loss = 7.1265, Test Loss = 7.0995\n",
      "Epoch 492/1000: Train Loss = 7.0635, Test Loss = 7.0078\n",
      "Epoch 493/1000: Train Loss = 7.1215, Test Loss = 7.0948\n",
      "Epoch 494/1000: Train Loss = 7.0585, Test Loss = 7.0031\n",
      "Epoch 495/1000: Train Loss = 7.1165, Test Loss = 7.0901\n",
      "Epoch 496/1000: Train Loss = 7.0535, Test Loss = 6.9985\n",
      "Epoch 497/1000: Train Loss = 7.1115, Test Loss = 7.0855\n",
      "Epoch 498/1000: Train Loss = 7.0486, Test Loss = 6.9939\n",
      "Epoch 499/1000: Train Loss = 7.1066, Test Loss = 7.0809\n",
      "Epoch 500/1000: Train Loss = 7.0437, Test Loss = 6.9893\n",
      "Epoch 501/1000: Train Loss = 7.1017, Test Loss = 7.0764\n",
      "Epoch 502/1000: Train Loss = 7.0388, Test Loss = 6.9847\n",
      "Epoch 503/1000: Train Loss = 7.0969, Test Loss = 7.0719\n",
      "Epoch 504/1000: Train Loss = 7.0339, Test Loss = 6.9802\n",
      "Epoch 505/1000: Train Loss = 7.0921, Test Loss = 7.0674\n",
      "Epoch 506/1000: Train Loss = 7.0291, Test Loss = 6.9757\n",
      "Epoch 507/1000: Train Loss = 7.0873, Test Loss = 7.0629\n",
      "Epoch 508/1000: Train Loss = 7.0243, Test Loss = 6.9712\n",
      "Epoch 509/1000: Train Loss = 7.0826, Test Loss = 7.0585\n",
      "Epoch 510/1000: Train Loss = 7.0195, Test Loss = 6.9668\n",
      "Epoch 511/1000: Train Loss = 7.0779, Test Loss = 7.0541\n",
      "Epoch 512/1000: Train Loss = 7.0148, Test Loss = 6.9623\n",
      "Epoch 513/1000: Train Loss = 7.0733, Test Loss = 7.0498\n",
      "Epoch 514/1000: Train Loss = 7.0101, Test Loss = 6.9579\n",
      "Epoch 515/1000: Train Loss = 7.0687, Test Loss = 7.0455\n",
      "Epoch 516/1000: Train Loss = 7.0054, Test Loss = 6.9536\n",
      "Epoch 517/1000: Train Loss = 7.0641, Test Loss = 7.0412\n",
      "Epoch 518/1000: Train Loss = 7.0007, Test Loss = 6.9492\n",
      "Epoch 519/1000: Train Loss = 7.0595, Test Loss = 7.0369\n",
      "Epoch 520/1000: Train Loss = 6.9960, Test Loss = 6.9449\n",
      "Epoch 521/1000: Train Loss = 7.0550, Test Loss = 7.0327\n",
      "Epoch 522/1000: Train Loss = 6.9914, Test Loss = 6.9406\n",
      "Epoch 523/1000: Train Loss = 7.0506, Test Loss = 7.0285\n",
      "Epoch 524/1000: Train Loss = 6.9868, Test Loss = 6.9363\n",
      "Epoch 525/1000: Train Loss = 7.0461, Test Loss = 7.0244\n",
      "Epoch 526/1000: Train Loss = 6.9822, Test Loss = 6.9321\n",
      "Epoch 527/1000: Train Loss = 7.0417, Test Loss = 7.0202\n",
      "Epoch 528/1000: Train Loss = 6.9777, Test Loss = 6.9278\n",
      "Epoch 529/1000: Train Loss = 7.0373, Test Loss = 7.0162\n",
      "Epoch 530/1000: Train Loss = 6.9731, Test Loss = 6.9236\n",
      "Epoch 531/1000: Train Loss = 7.0330, Test Loss = 7.0121\n",
      "Epoch 532/1000: Train Loss = 6.9686, Test Loss = 6.9194\n",
      "Epoch 533/1000: Train Loss = 7.0287, Test Loss = 7.0081\n",
      "Epoch 534/1000: Train Loss = 6.9641, Test Loss = 6.9153\n",
      "Epoch 535/1000: Train Loss = 7.0244, Test Loss = 7.0041\n",
      "Epoch 536/1000: Train Loss = 6.9597, Test Loss = 6.9111\n",
      "Epoch 537/1000: Train Loss = 7.0202, Test Loss = 7.0001\n",
      "Epoch 538/1000: Train Loss = 6.9552, Test Loss = 6.9070\n",
      "Epoch 539/1000: Train Loss = 7.0160, Test Loss = 6.9961\n",
      "Epoch 540/1000: Train Loss = 6.9508, Test Loss = 6.9029\n",
      "Epoch 541/1000: Train Loss = 7.0118, Test Loss = 6.9922\n",
      "Epoch 542/1000: Train Loss = 6.9464, Test Loss = 6.8988\n",
      "Epoch 543/1000: Train Loss = 7.0076, Test Loss = 6.9883\n",
      "Epoch 544/1000: Train Loss = 6.9420, Test Loss = 6.8947\n",
      "Epoch 545/1000: Train Loss = 7.0035, Test Loss = 6.9845\n",
      "Epoch 546/1000: Train Loss = 6.9377, Test Loss = 6.8907\n",
      "Epoch 547/1000: Train Loss = 6.9994, Test Loss = 6.9806\n",
      "Epoch 548/1000: Train Loss = 6.9333, Test Loss = 6.8867\n",
      "Epoch 549/1000: Train Loss = 6.9953, Test Loss = 6.9768\n",
      "Epoch 550/1000: Train Loss = 6.9290, Test Loss = 6.8827\n",
      "Epoch 551/1000: Train Loss = 6.9913, Test Loss = 6.9730\n",
      "Epoch 552/1000: Train Loss = 6.9247, Test Loss = 6.8787\n",
      "Epoch 553/1000: Train Loss = 6.9873, Test Loss = 6.9692\n",
      "Epoch 554/1000: Train Loss = 6.9204, Test Loss = 6.8747\n",
      "Epoch 555/1000: Train Loss = 6.9832, Test Loss = 6.9655\n",
      "Epoch 556/1000: Train Loss = 6.9162, Test Loss = 6.8707\n",
      "Epoch 557/1000: Train Loss = 6.9793, Test Loss = 6.9618\n",
      "Epoch 558/1000: Train Loss = 6.9119, Test Loss = 6.8668\n",
      "Epoch 559/1000: Train Loss = 6.9753, Test Loss = 6.9580\n",
      "Epoch 560/1000: Train Loss = 6.9077, Test Loss = 6.8629\n",
      "Epoch 561/1000: Train Loss = 6.9713, Test Loss = 6.9544\n",
      "Epoch 562/1000: Train Loss = 6.9035, Test Loss = 6.8590\n",
      "Epoch 563/1000: Train Loss = 6.9674, Test Loss = 6.9507\n",
      "Epoch 564/1000: Train Loss = 6.8993, Test Loss = 6.8551\n",
      "Epoch 565/1000: Train Loss = 6.9635, Test Loss = 6.9470\n",
      "Epoch 566/1000: Train Loss = 6.8951, Test Loss = 6.8512\n",
      "Epoch 567/1000: Train Loss = 6.9596, Test Loss = 6.9434\n",
      "Epoch 568/1000: Train Loss = 6.8910, Test Loss = 6.8474\n",
      "Epoch 569/1000: Train Loss = 6.9557, Test Loss = 6.9398\n",
      "Epoch 570/1000: Train Loss = 6.8868, Test Loss = 6.8435\n",
      "Epoch 571/1000: Train Loss = 6.9519, Test Loss = 6.9361\n",
      "Epoch 572/1000: Train Loss = 6.8827, Test Loss = 6.8397\n",
      "Epoch 573/1000: Train Loss = 6.9480, Test Loss = 6.9326\n",
      "Epoch 574/1000: Train Loss = 6.8786, Test Loss = 6.8359\n",
      "Epoch 575/1000: Train Loss = 6.9442, Test Loss = 6.9290\n",
      "Epoch 576/1000: Train Loss = 6.8745, Test Loss = 6.8321\n",
      "Epoch 577/1000: Train Loss = 6.9404, Test Loss = 6.9254\n",
      "Epoch 578/1000: Train Loss = 6.8704, Test Loss = 6.8283\n",
      "Epoch 579/1000: Train Loss = 6.9366, Test Loss = 6.9219\n",
      "Epoch 580/1000: Train Loss = 6.8664, Test Loss = 6.8245\n",
      "Epoch 581/1000: Train Loss = 6.9328, Test Loss = 6.9183\n",
      "Epoch 582/1000: Train Loss = 6.8623, Test Loss = 6.8208\n",
      "Epoch 583/1000: Train Loss = 6.9290, Test Loss = 6.9148\n",
      "Epoch 584/1000: Train Loss = 6.8583, Test Loss = 6.8170\n",
      "Epoch 585/1000: Train Loss = 6.9252, Test Loss = 6.9113\n",
      "Epoch 586/1000: Train Loss = 6.8543, Test Loss = 6.8133\n",
      "Epoch 587/1000: Train Loss = 6.9215, Test Loss = 6.9078\n",
      "Epoch 588/1000: Train Loss = 6.8503, Test Loss = 6.8096\n",
      "Epoch 589/1000: Train Loss = 6.9177, Test Loss = 6.9043\n",
      "Epoch 590/1000: Train Loss = 6.8463, Test Loss = 6.8059\n",
      "Epoch 591/1000: Train Loss = 6.9140, Test Loss = 6.9008\n",
      "Epoch 592/1000: Train Loss = 6.8424, Test Loss = 6.8022\n",
      "Epoch 593/1000: Train Loss = 6.9103, Test Loss = 6.8974\n",
      "Epoch 594/1000: Train Loss = 6.8384, Test Loss = 6.7986\n",
      "Epoch 595/1000: Train Loss = 6.9066, Test Loss = 6.8939\n",
      "Epoch 596/1000: Train Loss = 6.8345, Test Loss = 6.7949\n",
      "Epoch 597/1000: Train Loss = 6.9029, Test Loss = 6.8905\n",
      "Epoch 598/1000: Train Loss = 6.8306, Test Loss = 6.7913\n",
      "Epoch 599/1000: Train Loss = 6.8992, Test Loss = 6.8870\n",
      "Epoch 600/1000: Train Loss = 6.8267, Test Loss = 6.7876\n",
      "Epoch 601/1000: Train Loss = 6.8956, Test Loss = 6.8836\n",
      "Epoch 602/1000: Train Loss = 6.8228, Test Loss = 6.7840\n",
      "Epoch 603/1000: Train Loss = 6.8919, Test Loss = 6.8802\n",
      "Epoch 604/1000: Train Loss = 6.8189, Test Loss = 6.7804\n",
      "Epoch 605/1000: Train Loss = 6.8883, Test Loss = 6.8768\n",
      "Epoch 606/1000: Train Loss = 6.8151, Test Loss = 6.7768\n",
      "Epoch 607/1000: Train Loss = 6.8846, Test Loss = 6.8735\n",
      "Epoch 608/1000: Train Loss = 6.8112, Test Loss = 6.7732\n",
      "Epoch 609/1000: Train Loss = 6.8810, Test Loss = 6.8701\n",
      "Epoch 610/1000: Train Loss = 6.8074, Test Loss = 6.7697\n",
      "Epoch 611/1000: Train Loss = 6.8774, Test Loss = 6.8668\n",
      "Epoch 612/1000: Train Loss = 6.8036, Test Loss = 6.7661\n",
      "Epoch 613/1000: Train Loss = 6.8738, Test Loss = 6.8634\n",
      "Epoch 614/1000: Train Loss = 6.7998, Test Loss = 6.7626\n",
      "Epoch 615/1000: Train Loss = 6.8703, Test Loss = 6.8601\n",
      "Epoch 616/1000: Train Loss = 6.7960, Test Loss = 6.7591\n",
      "Epoch 617/1000: Train Loss = 6.8667, Test Loss = 6.8568\n",
      "Epoch 618/1000: Train Loss = 6.7922, Test Loss = 6.7556\n",
      "Epoch 619/1000: Train Loss = 6.8631, Test Loss = 6.8535\n",
      "Epoch 620/1000: Train Loss = 6.7885, Test Loss = 6.7521\n",
      "Epoch 621/1000: Train Loss = 6.8596, Test Loss = 6.8502\n",
      "Epoch 622/1000: Train Loss = 6.7847, Test Loss = 6.7486\n",
      "Epoch 623/1000: Train Loss = 6.8561, Test Loss = 6.8469\n",
      "Epoch 624/1000: Train Loss = 6.7810, Test Loss = 6.7451\n",
      "Epoch 625/1000: Train Loss = 6.8526, Test Loss = 6.8437\n",
      "Epoch 626/1000: Train Loss = 6.7773, Test Loss = 6.7416\n",
      "Epoch 627/1000: Train Loss = 6.8491, Test Loss = 6.8404\n",
      "Epoch 628/1000: Train Loss = 6.7736, Test Loss = 6.7382\n",
      "Epoch 629/1000: Train Loss = 6.8456, Test Loss = 6.8372\n",
      "Epoch 630/1000: Train Loss = 6.7699, Test Loss = 6.7348\n",
      "Epoch 631/1000: Train Loss = 6.8421, Test Loss = 6.8339\n",
      "Epoch 632/1000: Train Loss = 6.7662, Test Loss = 6.7313\n",
      "Epoch 633/1000: Train Loss = 6.8386, Test Loss = 6.8307\n",
      "Epoch 634/1000: Train Loss = 6.7626, Test Loss = 6.7279\n",
      "Epoch 635/1000: Train Loss = 6.8352, Test Loss = 6.8275\n",
      "Epoch 636/1000: Train Loss = 6.7589, Test Loss = 6.7245\n",
      "Epoch 637/1000: Train Loss = 6.8318, Test Loss = 6.8243\n",
      "Epoch 638/1000: Train Loss = 6.7553, Test Loss = 6.7212\n",
      "Epoch 639/1000: Train Loss = 6.8283, Test Loss = 6.8212\n",
      "Epoch 640/1000: Train Loss = 6.7517, Test Loss = 6.7178\n",
      "Epoch 641/1000: Train Loss = 6.8249, Test Loss = 6.8180\n",
      "Epoch 642/1000: Train Loss = 6.7481, Test Loss = 6.7144\n",
      "Epoch 643/1000: Train Loss = 6.8215, Test Loss = 6.8148\n",
      "Epoch 644/1000: Train Loss = 6.7445, Test Loss = 6.7111\n",
      "Epoch 645/1000: Train Loss = 6.8181, Test Loss = 6.8117\n",
      "Epoch 646/1000: Train Loss = 6.7409, Test Loss = 6.7077\n",
      "Epoch 647/1000: Train Loss = 6.8148, Test Loss = 6.8086\n",
      "Epoch 648/1000: Train Loss = 6.7373, Test Loss = 6.7044\n",
      "Epoch 649/1000: Train Loss = 6.8114, Test Loss = 6.8054\n",
      "Epoch 650/1000: Train Loss = 6.7338, Test Loss = 6.7011\n",
      "Epoch 651/1000: Train Loss = 6.8080, Test Loss = 6.8023\n",
      "Epoch 652/1000: Train Loss = 6.7302, Test Loss = 6.6978\n",
      "Epoch 653/1000: Train Loss = 6.8047, Test Loss = 6.7992\n",
      "Epoch 654/1000: Train Loss = 6.7267, Test Loss = 6.6945\n",
      "Epoch 655/1000: Train Loss = 6.8014, Test Loss = 6.7961\n",
      "Epoch 656/1000: Train Loss = 6.7232, Test Loss = 6.6912\n",
      "Epoch 657/1000: Train Loss = 6.7980, Test Loss = 6.7931\n",
      "Epoch 658/1000: Train Loss = 6.7196, Test Loss = 6.6880\n",
      "Epoch 659/1000: Train Loss = 6.7947, Test Loss = 6.7900\n",
      "Epoch 660/1000: Train Loss = 6.7162, Test Loss = 6.6847\n",
      "Epoch 661/1000: Train Loss = 6.7914, Test Loss = 6.7869\n",
      "Epoch 662/1000: Train Loss = 6.7127, Test Loss = 6.6815\n",
      "Epoch 663/1000: Train Loss = 6.7882, Test Loss = 6.7839\n",
      "Epoch 664/1000: Train Loss = 6.7092, Test Loss = 6.6783\n",
      "Epoch 665/1000: Train Loss = 6.7849, Test Loss = 6.7808\n",
      "Epoch 666/1000: Train Loss = 6.7057, Test Loss = 6.6750\n",
      "Epoch 667/1000: Train Loss = 6.7816, Test Loss = 6.7778\n",
      "Epoch 668/1000: Train Loss = 6.7023, Test Loss = 6.6718\n",
      "Epoch 669/1000: Train Loss = 6.7784, Test Loss = 6.7748\n",
      "Epoch 670/1000: Train Loss = 6.6989, Test Loss = 6.6686\n",
      "Epoch 671/1000: Train Loss = 6.7751, Test Loss = 6.7718\n",
      "Epoch 672/1000: Train Loss = 6.6954, Test Loss = 6.6654\n",
      "Epoch 673/1000: Train Loss = 6.7719, Test Loss = 6.7688\n",
      "Epoch 674/1000: Train Loss = 6.6920, Test Loss = 6.6623\n",
      "Epoch 675/1000: Train Loss = 6.7687, Test Loss = 6.7658\n",
      "Epoch 676/1000: Train Loss = 6.6886, Test Loss = 6.6591\n",
      "Epoch 677/1000: Train Loss = 6.7655, Test Loss = 6.7629\n",
      "Epoch 678/1000: Train Loss = 6.6852, Test Loss = 6.6559\n",
      "Epoch 679/1000: Train Loss = 6.7623, Test Loss = 6.7599\n",
      "Epoch 680/1000: Train Loss = 6.6819, Test Loss = 6.6528\n",
      "Epoch 681/1000: Train Loss = 6.7591, Test Loss = 6.7569\n",
      "Epoch 682/1000: Train Loss = 6.6785, Test Loss = 6.6497\n",
      "Epoch 683/1000: Train Loss = 6.7559, Test Loss = 6.7540\n",
      "Epoch 684/1000: Train Loss = 6.6752, Test Loss = 6.6466\n",
      "Epoch 685/1000: Train Loss = 6.7527, Test Loss = 6.7511\n",
      "Epoch 686/1000: Train Loss = 6.6718, Test Loss = 6.6434\n",
      "Epoch 687/1000: Train Loss = 6.7496, Test Loss = 6.7481\n",
      "Epoch 688/1000: Train Loss = 6.6685, Test Loss = 6.6403\n",
      "Epoch 689/1000: Train Loss = 6.7464, Test Loss = 6.7452\n",
      "Epoch 690/1000: Train Loss = 6.6652, Test Loss = 6.6373\n",
      "Epoch 691/1000: Train Loss = 6.7433, Test Loss = 6.7423\n",
      "Epoch 692/1000: Train Loss = 6.6619, Test Loss = 6.6342\n",
      "Epoch 693/1000: Train Loss = 6.7402, Test Loss = 6.7394\n",
      "Epoch 694/1000: Train Loss = 6.6586, Test Loss = 6.6311\n",
      "Epoch 695/1000: Train Loss = 6.7370, Test Loss = 6.7365\n",
      "Epoch 696/1000: Train Loss = 6.6553, Test Loss = 6.6280\n",
      "Epoch 697/1000: Train Loss = 6.7339, Test Loss = 6.7337\n",
      "Epoch 698/1000: Train Loss = 6.6520, Test Loss = 6.6250\n",
      "Epoch 699/1000: Train Loss = 6.7308, Test Loss = 6.7308\n",
      "Epoch 700/1000: Train Loss = 6.6487, Test Loss = 6.6220\n",
      "Epoch 701/1000: Train Loss = 6.7277, Test Loss = 6.7279\n",
      "Epoch 702/1000: Train Loss = 6.6455, Test Loss = 6.6189\n",
      "Epoch 703/1000: Train Loss = 6.7247, Test Loss = 6.7251\n",
      "Epoch 704/1000: Train Loss = 6.6422, Test Loss = 6.6159\n",
      "Epoch 705/1000: Train Loss = 6.7216, Test Loss = 6.7222\n",
      "Epoch 706/1000: Train Loss = 6.6390, Test Loss = 6.6129\n",
      "Epoch 707/1000: Train Loss = 6.7185, Test Loss = 6.7194\n",
      "Epoch 708/1000: Train Loss = 6.6358, Test Loss = 6.6099\n",
      "Epoch 709/1000: Train Loss = 6.7155, Test Loss = 6.7166\n",
      "Epoch 710/1000: Train Loss = 6.6326, Test Loss = 6.6069\n",
      "Epoch 711/1000: Train Loss = 6.7124, Test Loss = 6.7138\n",
      "Epoch 712/1000: Train Loss = 6.6294, Test Loss = 6.6039\n",
      "Epoch 713/1000: Train Loss = 6.7094, Test Loss = 6.7110\n",
      "Epoch 714/1000: Train Loss = 6.6262, Test Loss = 6.6010\n",
      "Epoch 715/1000: Train Loss = 6.7064, Test Loss = 6.7082\n",
      "Epoch 716/1000: Train Loss = 6.6230, Test Loss = 6.5980\n",
      "Epoch 717/1000: Train Loss = 6.7034, Test Loss = 6.7054\n",
      "Epoch 718/1000: Train Loss = 6.6198, Test Loss = 6.5951\n",
      "Epoch 719/1000: Train Loss = 6.7004, Test Loss = 6.7026\n",
      "Epoch 720/1000: Train Loss = 6.6167, Test Loss = 6.5921\n",
      "Epoch 721/1000: Train Loss = 6.6974, Test Loss = 6.6998\n",
      "Epoch 722/1000: Train Loss = 6.6135, Test Loss = 6.5892\n",
      "Epoch 723/1000: Train Loss = 6.6944, Test Loss = 6.6971\n",
      "Epoch 724/1000: Train Loss = 6.6104, Test Loss = 6.5863\n",
      "Epoch 725/1000: Train Loss = 6.6914, Test Loss = 6.6943\n",
      "Epoch 726/1000: Train Loss = 6.6073, Test Loss = 6.5834\n",
      "Epoch 727/1000: Train Loss = 6.6884, Test Loss = 6.6916\n",
      "Epoch 728/1000: Train Loss = 6.6042, Test Loss = 6.5805\n",
      "Epoch 729/1000: Train Loss = 6.6855, Test Loss = 6.6888\n",
      "Epoch 730/1000: Train Loss = 6.6010, Test Loss = 6.5776\n",
      "Epoch 731/1000: Train Loss = 6.6825, Test Loss = 6.6861\n",
      "Epoch 732/1000: Train Loss = 6.5979, Test Loss = 6.5747\n",
      "Epoch 733/1000: Train Loss = 6.6796, Test Loss = 6.6834\n",
      "Epoch 734/1000: Train Loss = 6.5949, Test Loss = 6.5718\n",
      "Epoch 735/1000: Train Loss = 6.6766, Test Loss = 6.6806\n",
      "Epoch 736/1000: Train Loss = 6.5918, Test Loss = 6.5690\n",
      "Epoch 737/1000: Train Loss = 6.6737, Test Loss = 6.6779\n",
      "Epoch 738/1000: Train Loss = 6.5887, Test Loss = 6.5661\n",
      "Epoch 739/1000: Train Loss = 6.6708, Test Loss = 6.6752\n",
      "Epoch 740/1000: Train Loss = 6.5856, Test Loss = 6.5633\n",
      "Epoch 741/1000: Train Loss = 6.6679, Test Loss = 6.6725\n",
      "Epoch 742/1000: Train Loss = 6.5826, Test Loss = 6.5604\n",
      "Epoch 743/1000: Train Loss = 6.6650, Test Loss = 6.6698\n",
      "Epoch 744/1000: Train Loss = 6.5796, Test Loss = 6.5576\n",
      "Epoch 745/1000: Train Loss = 6.6621, Test Loss = 6.6672\n",
      "Epoch 746/1000: Train Loss = 6.5765, Test Loss = 6.5548\n",
      "Epoch 747/1000: Train Loss = 6.6592, Test Loss = 6.6645\n",
      "Epoch 748/1000: Train Loss = 6.5735, Test Loss = 6.5520\n",
      "Epoch 749/1000: Train Loss = 6.6563, Test Loss = 6.6618\n",
      "Epoch 750/1000: Train Loss = 6.5705, Test Loss = 6.5492\n",
      "Epoch 751/1000: Train Loss = 6.6534, Test Loss = 6.6592\n",
      "Epoch 752/1000: Train Loss = 6.5675, Test Loss = 6.5464\n",
      "Epoch 753/1000: Train Loss = 6.6506, Test Loss = 6.6565\n",
      "Epoch 754/1000: Train Loss = 6.5645, Test Loss = 6.5436\n",
      "Epoch 755/1000: Train Loss = 6.6477, Test Loss = 6.6539\n",
      "Epoch 756/1000: Train Loss = 6.5615, Test Loss = 6.5408\n",
      "Epoch 757/1000: Train Loss = 6.6449, Test Loss = 6.6513\n",
      "Epoch 758/1000: Train Loss = 6.5585, Test Loss = 6.5381\n",
      "Epoch 759/1000: Train Loss = 6.6420, Test Loss = 6.6486\n",
      "Epoch 760/1000: Train Loss = 6.5556, Test Loss = 6.5353\n",
      "Epoch 761/1000: Train Loss = 6.6392, Test Loss = 6.6460\n",
      "Epoch 762/1000: Train Loss = 6.5526, Test Loss = 6.5325\n",
      "Epoch 763/1000: Train Loss = 6.6364, Test Loss = 6.6434\n",
      "Epoch 764/1000: Train Loss = 6.5497, Test Loss = 6.5298\n",
      "Epoch 765/1000: Train Loss = 6.6335, Test Loss = 6.6408\n",
      "Epoch 766/1000: Train Loss = 6.5467, Test Loss = 6.5271\n",
      "Epoch 767/1000: Train Loss = 6.6307, Test Loss = 6.6382\n",
      "Epoch 768/1000: Train Loss = 6.5438, Test Loss = 6.5243\n",
      "Epoch 769/1000: Train Loss = 6.6279, Test Loss = 6.6356\n",
      "Epoch 770/1000: Train Loss = 6.5409, Test Loss = 6.5216\n",
      "Epoch 771/1000: Train Loss = 6.6251, Test Loss = 6.6330\n",
      "Epoch 772/1000: Train Loss = 6.5380, Test Loss = 6.5189\n",
      "Epoch 773/1000: Train Loss = 6.6224, Test Loss = 6.6304\n",
      "Epoch 774/1000: Train Loss = 6.5351, Test Loss = 6.5162\n",
      "Epoch 775/1000: Train Loss = 6.6196, Test Loss = 6.6279\n",
      "Epoch 776/1000: Train Loss = 6.5322, Test Loss = 6.5135\n",
      "Epoch 777/1000: Train Loss = 6.6168, Test Loss = 6.6253\n",
      "Epoch 778/1000: Train Loss = 6.5293, Test Loss = 6.5108\n",
      "Epoch 779/1000: Train Loss = 6.6140, Test Loss = 6.6227\n",
      "Epoch 780/1000: Train Loss = 6.5264, Test Loss = 6.5081\n",
      "Epoch 781/1000: Train Loss = 6.6113, Test Loss = 6.6202\n",
      "Epoch 782/1000: Train Loss = 6.5235, Test Loss = 6.5055\n",
      "Epoch 783/1000: Train Loss = 6.6085, Test Loss = 6.6176\n",
      "Epoch 784/1000: Train Loss = 6.5207, Test Loss = 6.5028\n",
      "Epoch 785/1000: Train Loss = 6.6058, Test Loss = 6.6151\n",
      "Epoch 786/1000: Train Loss = 6.5178, Test Loss = 6.5001\n",
      "Epoch 787/1000: Train Loss = 6.6031, Test Loss = 6.6126\n",
      "Epoch 788/1000: Train Loss = 6.5150, Test Loss = 6.4975\n",
      "Epoch 789/1000: Train Loss = 6.6003, Test Loss = 6.6101\n",
      "Epoch 790/1000: Train Loss = 6.5121, Test Loss = 6.4948\n",
      "Epoch 791/1000: Train Loss = 6.5976, Test Loss = 6.6075\n",
      "Epoch 792/1000: Train Loss = 6.5093, Test Loss = 6.4922\n",
      "Epoch 793/1000: Train Loss = 6.5949, Test Loss = 6.6050\n",
      "Epoch 794/1000: Train Loss = 6.5065, Test Loss = 6.4896\n",
      "Epoch 795/1000: Train Loss = 6.5922, Test Loss = 6.6025\n",
      "Epoch 796/1000: Train Loss = 6.5037, Test Loss = 6.4870\n",
      "Epoch 797/1000: Train Loss = 6.5895, Test Loss = 6.6000\n",
      "Epoch 798/1000: Train Loss = 6.5009, Test Loss = 6.4844\n",
      "Epoch 799/1000: Train Loss = 6.5868, Test Loss = 6.5975\n",
      "Epoch 800/1000: Train Loss = 6.4981, Test Loss = 6.4818\n",
      "Epoch 801/1000: Train Loss = 6.5841, Test Loss = 6.5950\n",
      "Epoch 802/1000: Train Loss = 6.4953, Test Loss = 6.4792\n",
      "Epoch 803/1000: Train Loss = 6.5814, Test Loss = 6.5925\n",
      "Epoch 804/1000: Train Loss = 6.4925, Test Loss = 6.4766\n",
      "Epoch 805/1000: Train Loss = 6.5788, Test Loss = 6.5901\n",
      "Epoch 806/1000: Train Loss = 6.4897, Test Loss = 6.4740\n",
      "Epoch 807/1000: Train Loss = 6.5761, Test Loss = 6.5876\n",
      "Epoch 808/1000: Train Loss = 6.4870, Test Loss = 6.4714\n",
      "Epoch 809/1000: Train Loss = 6.5734, Test Loss = 6.5851\n",
      "Epoch 810/1000: Train Loss = 6.4842, Test Loss = 6.4688\n",
      "Epoch 811/1000: Train Loss = 6.5708, Test Loss = 6.5827\n",
      "Epoch 812/1000: Train Loss = 6.4815, Test Loss = 6.4663\n",
      "Epoch 813/1000: Train Loss = 6.5682, Test Loss = 6.5802\n",
      "Epoch 814/1000: Train Loss = 6.4787, Test Loss = 6.4637\n",
      "Epoch 815/1000: Train Loss = 6.5655, Test Loss = 6.5778\n",
      "Epoch 816/1000: Train Loss = 6.4760, Test Loss = 6.4612\n",
      "Epoch 817/1000: Train Loss = 6.5629, Test Loss = 6.5753\n",
      "Epoch 818/1000: Train Loss = 6.4733, Test Loss = 6.4587\n",
      "Epoch 819/1000: Train Loss = 6.5603, Test Loss = 6.5729\n",
      "Epoch 820/1000: Train Loss = 6.4705, Test Loss = 6.4561\n",
      "Epoch 821/1000: Train Loss = 6.5576, Test Loss = 6.5705\n",
      "Epoch 822/1000: Train Loss = 6.4678, Test Loss = 6.4536\n",
      "Epoch 823/1000: Train Loss = 6.5550, Test Loss = 6.5680\n",
      "Epoch 824/1000: Train Loss = 6.4651, Test Loss = 6.4511\n",
      "Epoch 825/1000: Train Loss = 6.5524, Test Loss = 6.5656\n",
      "Epoch 826/1000: Train Loss = 6.4624, Test Loss = 6.4486\n",
      "Epoch 827/1000: Train Loss = 6.5498, Test Loss = 6.5632\n",
      "Epoch 828/1000: Train Loss = 6.4597, Test Loss = 6.4461\n",
      "Epoch 829/1000: Train Loss = 6.5472, Test Loss = 6.5608\n",
      "Epoch 830/1000: Train Loss = 6.4570, Test Loss = 6.4436\n",
      "Epoch 831/1000: Train Loss = 6.5446, Test Loss = 6.5584\n",
      "Epoch 832/1000: Train Loss = 6.4544, Test Loss = 6.4411\n",
      "Epoch 833/1000: Train Loss = 6.5421, Test Loss = 6.5560\n",
      "Epoch 834/1000: Train Loss = 6.4517, Test Loss = 6.4386\n",
      "Epoch 835/1000: Train Loss = 6.5395, Test Loss = 6.5536\n",
      "Epoch 836/1000: Train Loss = 6.4490, Test Loss = 6.4361\n",
      "Epoch 837/1000: Train Loss = 6.5369, Test Loss = 6.5512\n",
      "Epoch 838/1000: Train Loss = 6.4464, Test Loss = 6.4337\n",
      "Epoch 839/1000: Train Loss = 6.5343, Test Loss = 6.5488\n",
      "Epoch 840/1000: Train Loss = 6.4437, Test Loss = 6.4312\n",
      "Epoch 841/1000: Train Loss = 6.5318, Test Loss = 6.5465\n",
      "Epoch 842/1000: Train Loss = 6.4411, Test Loss = 6.4287\n",
      "Epoch 843/1000: Train Loss = 6.5292, Test Loss = 6.5441\n",
      "Epoch 844/1000: Train Loss = 6.4385, Test Loss = 6.4263\n",
      "Epoch 845/1000: Train Loss = 6.5267, Test Loss = 6.5417\n",
      "Epoch 846/1000: Train Loss = 6.4358, Test Loss = 6.4239\n",
      "Epoch 847/1000: Train Loss = 6.5242, Test Loss = 6.5394\n",
      "Epoch 848/1000: Train Loss = 6.4332, Test Loss = 6.4214\n",
      "Epoch 849/1000: Train Loss = 6.5216, Test Loss = 6.5370\n",
      "Epoch 850/1000: Train Loss = 6.4306, Test Loss = 6.4190\n",
      "Epoch 851/1000: Train Loss = 6.5191, Test Loss = 6.5347\n",
      "Epoch 852/1000: Train Loss = 6.4280, Test Loss = 6.4166\n",
      "Epoch 853/1000: Train Loss = 6.5166, Test Loss = 6.5324\n",
      "Epoch 854/1000: Train Loss = 6.4254, Test Loss = 6.4142\n",
      "Epoch 855/1000: Train Loss = 6.5141, Test Loss = 6.5300\n",
      "Epoch 856/1000: Train Loss = 6.4228, Test Loss = 6.4117\n",
      "Epoch 857/1000: Train Loss = 6.5115, Test Loss = 6.5277\n",
      "Epoch 858/1000: Train Loss = 6.4202, Test Loss = 6.4093\n",
      "Epoch 859/1000: Train Loss = 6.5090, Test Loss = 6.5254\n",
      "Epoch 860/1000: Train Loss = 6.4176, Test Loss = 6.4069\n",
      "Epoch 861/1000: Train Loss = 6.5065, Test Loss = 6.5230\n",
      "Epoch 862/1000: Train Loss = 6.4151, Test Loss = 6.4046\n",
      "Epoch 863/1000: Train Loss = 6.5041, Test Loss = 6.5207\n",
      "Epoch 864/1000: Train Loss = 6.4125, Test Loss = 6.4022\n",
      "Epoch 865/1000: Train Loss = 6.5016, Test Loss = 6.5184\n",
      "Epoch 866/1000: Train Loss = 6.4100, Test Loss = 6.3998\n",
      "Epoch 867/1000: Train Loss = 6.4991, Test Loss = 6.5161\n",
      "Epoch 868/1000: Train Loss = 6.4074, Test Loss = 6.3974\n",
      "Epoch 869/1000: Train Loss = 6.4966, Test Loss = 6.5138\n",
      "Epoch 870/1000: Train Loss = 6.4049, Test Loss = 6.3951\n",
      "Epoch 871/1000: Train Loss = 6.4941, Test Loss = 6.5115\n",
      "Epoch 872/1000: Train Loss = 6.4023, Test Loss = 6.3927\n",
      "Epoch 873/1000: Train Loss = 6.4917, Test Loss = 6.5092\n",
      "Epoch 874/1000: Train Loss = 6.3998, Test Loss = 6.3903\n",
      "Epoch 875/1000: Train Loss = 6.4892, Test Loss = 6.5070\n",
      "Epoch 876/1000: Train Loss = 6.3973, Test Loss = 6.3880\n",
      "Epoch 877/1000: Train Loss = 6.4868, Test Loss = 6.5047\n",
      "Epoch 878/1000: Train Loss = 6.3947, Test Loss = 6.3857\n",
      "Epoch 879/1000: Train Loss = 6.4843, Test Loss = 6.5024\n",
      "Epoch 880/1000: Train Loss = 6.3922, Test Loss = 6.3833\n",
      "Epoch 881/1000: Train Loss = 6.4819, Test Loss = 6.5001\n",
      "Epoch 882/1000: Train Loss = 6.3897, Test Loss = 6.3810\n",
      "Epoch 883/1000: Train Loss = 6.4794, Test Loss = 6.4979\n",
      "Epoch 884/1000: Train Loss = 6.3872, Test Loss = 6.3787\n",
      "Epoch 885/1000: Train Loss = 6.4770, Test Loss = 6.4956\n",
      "Epoch 886/1000: Train Loss = 6.3847, Test Loss = 6.3764\n",
      "Epoch 887/1000: Train Loss = 6.4746, Test Loss = 6.4934\n",
      "Epoch 888/1000: Train Loss = 6.3822, Test Loss = 6.3740\n",
      "Epoch 889/1000: Train Loss = 6.4722, Test Loss = 6.4911\n",
      "Epoch 890/1000: Train Loss = 6.3798, Test Loss = 6.3717\n",
      "Epoch 891/1000: Train Loss = 6.4698, Test Loss = 6.4889\n",
      "Epoch 892/1000: Train Loss = 6.3773, Test Loss = 6.3694\n",
      "Epoch 893/1000: Train Loss = 6.4674, Test Loss = 6.4866\n",
      "Epoch 894/1000: Train Loss = 6.3748, Test Loss = 6.3671\n",
      "Epoch 895/1000: Train Loss = 6.4649, Test Loss = 6.4844\n",
      "Epoch 896/1000: Train Loss = 6.3724, Test Loss = 6.3649\n",
      "Epoch 897/1000: Train Loss = 6.4625, Test Loss = 6.4822\n",
      "Epoch 898/1000: Train Loss = 6.3699, Test Loss = 6.3626\n",
      "Epoch 899/1000: Train Loss = 6.4602, Test Loss = 6.4799\n",
      "Epoch 900/1000: Train Loss = 6.3674, Test Loss = 6.3603\n",
      "Epoch 901/1000: Train Loss = 6.4578, Test Loss = 6.4777\n",
      "Epoch 902/1000: Train Loss = 6.3650, Test Loss = 6.3580\n",
      "Epoch 903/1000: Train Loss = 6.4554, Test Loss = 6.4755\n",
      "Epoch 904/1000: Train Loss = 6.3626, Test Loss = 6.3558\n",
      "Epoch 905/1000: Train Loss = 6.4530, Test Loss = 6.4733\n",
      "Epoch 906/1000: Train Loss = 6.3601, Test Loss = 6.3535\n",
      "Epoch 907/1000: Train Loss = 6.4506, Test Loss = 6.4711\n",
      "Epoch 908/1000: Train Loss = 6.3577, Test Loss = 6.3512\n",
      "Epoch 909/1000: Train Loss = 6.4483, Test Loss = 6.4689\n",
      "Epoch 910/1000: Train Loss = 6.3553, Test Loss = 6.3490\n",
      "Epoch 911/1000: Train Loss = 6.4459, Test Loss = 6.4667\n",
      "Epoch 912/1000: Train Loss = 6.3529, Test Loss = 6.3468\n",
      "Epoch 913/1000: Train Loss = 6.4436, Test Loss = 6.4645\n",
      "Epoch 914/1000: Train Loss = 6.3505, Test Loss = 6.3445\n",
      "Epoch 915/1000: Train Loss = 6.4412, Test Loss = 6.4623\n",
      "Epoch 916/1000: Train Loss = 6.3481, Test Loss = 6.3423\n",
      "Epoch 917/1000: Train Loss = 6.4389, Test Loss = 6.4601\n",
      "Epoch 918/1000: Train Loss = 6.3457, Test Loss = 6.3401\n",
      "Epoch 919/1000: Train Loss = 6.4365, Test Loss = 6.4579\n",
      "Epoch 920/1000: Train Loss = 6.3433, Test Loss = 6.3378\n",
      "Epoch 921/1000: Train Loss = 6.4342, Test Loss = 6.4557\n",
      "Epoch 922/1000: Train Loss = 6.3409, Test Loss = 6.3356\n",
      "Epoch 923/1000: Train Loss = 6.4318, Test Loss = 6.4536\n",
      "Epoch 924/1000: Train Loss = 6.3385, Test Loss = 6.3334\n",
      "Epoch 925/1000: Train Loss = 6.4295, Test Loss = 6.4514\n",
      "Epoch 926/1000: Train Loss = 6.3361, Test Loss = 6.3312\n",
      "Epoch 927/1000: Train Loss = 6.4272, Test Loss = 6.4492\n",
      "Epoch 928/1000: Train Loss = 6.3337, Test Loss = 6.3290\n",
      "Epoch 929/1000: Train Loss = 6.4249, Test Loss = 6.4471\n",
      "Epoch 930/1000: Train Loss = 6.3314, Test Loss = 6.3268\n",
      "Epoch 931/1000: Train Loss = 6.4226, Test Loss = 6.4449\n",
      "Epoch 932/1000: Train Loss = 6.3290, Test Loss = 6.3246\n",
      "Epoch 933/1000: Train Loss = 6.4203, Test Loss = 6.4428\n",
      "Epoch 934/1000: Train Loss = 6.3267, Test Loss = 6.3224\n",
      "Epoch 935/1000: Train Loss = 6.4180, Test Loss = 6.4406\n",
      "Epoch 936/1000: Train Loss = 6.3243, Test Loss = 6.3203\n",
      "Epoch 937/1000: Train Loss = 6.4157, Test Loss = 6.4385\n",
      "Epoch 938/1000: Train Loss = 6.3220, Test Loss = 6.3181\n",
      "Epoch 939/1000: Train Loss = 6.4134, Test Loss = 6.4364\n",
      "Epoch 940/1000: Train Loss = 6.3196, Test Loss = 6.3159\n",
      "Epoch 941/1000: Train Loss = 6.4111, Test Loss = 6.4342\n",
      "Epoch 942/1000: Train Loss = 6.3173, Test Loss = 6.3138\n",
      "Epoch 943/1000: Train Loss = 6.4088, Test Loss = 6.4321\n",
      "Epoch 944/1000: Train Loss = 6.3150, Test Loss = 6.3116\n",
      "Epoch 945/1000: Train Loss = 6.4065, Test Loss = 6.4300\n",
      "Epoch 946/1000: Train Loss = 6.3127, Test Loss = 6.3094\n",
      "Epoch 947/1000: Train Loss = 6.4043, Test Loss = 6.4279\n",
      "Epoch 948/1000: Train Loss = 6.3104, Test Loss = 6.3073\n",
      "Epoch 949/1000: Train Loss = 6.4020, Test Loss = 6.4258\n",
      "Epoch 950/1000: Train Loss = 6.3081, Test Loss = 6.3052\n",
      "Epoch 951/1000: Train Loss = 6.3997, Test Loss = 6.4237\n",
      "Epoch 952/1000: Train Loss = 6.3057, Test Loss = 6.3030\n",
      "Epoch 953/1000: Train Loss = 6.3975, Test Loss = 6.4215\n",
      "Epoch 954/1000: Train Loss = 6.3034, Test Loss = 6.3009\n",
      "Epoch 955/1000: Train Loss = 6.3952, Test Loss = 6.4195\n",
      "Epoch 956/1000: Train Loss = 6.3012, Test Loss = 6.2988\n",
      "Epoch 957/1000: Train Loss = 6.3930, Test Loss = 6.4174\n",
      "Epoch 958/1000: Train Loss = 6.2989, Test Loss = 6.2966\n",
      "Epoch 959/1000: Train Loss = 6.3907, Test Loss = 6.4153\n",
      "Epoch 960/1000: Train Loss = 6.2966, Test Loss = 6.2945\n",
      "Epoch 961/1000: Train Loss = 6.3885, Test Loss = 6.4132\n",
      "Epoch 962/1000: Train Loss = 6.2943, Test Loss = 6.2924\n",
      "Epoch 963/1000: Train Loss = 6.3862, Test Loss = 6.4111\n",
      "Epoch 964/1000: Train Loss = 6.2920, Test Loss = 6.2903\n",
      "Epoch 965/1000: Train Loss = 6.3840, Test Loss = 6.4090\n",
      "Epoch 966/1000: Train Loss = 6.2898, Test Loss = 6.2882\n",
      "Epoch 967/1000: Train Loss = 6.3818, Test Loss = 6.4069\n",
      "Epoch 968/1000: Train Loss = 6.2875, Test Loss = 6.2861\n",
      "Epoch 969/1000: Train Loss = 6.3796, Test Loss = 6.4049\n",
      "Epoch 970/1000: Train Loss = 6.2852, Test Loss = 6.2840\n",
      "Epoch 971/1000: Train Loss = 6.3773, Test Loss = 6.4028\n",
      "Epoch 972/1000: Train Loss = 6.2830, Test Loss = 6.2819\n",
      "Epoch 973/1000: Train Loss = 6.3751, Test Loss = 6.4007\n",
      "Epoch 974/1000: Train Loss = 6.2807, Test Loss = 6.2798\n",
      "Epoch 975/1000: Train Loss = 6.3729, Test Loss = 6.3987\n",
      "Epoch 976/1000: Train Loss = 6.2785, Test Loss = 6.2778\n",
      "Epoch 977/1000: Train Loss = 6.3707, Test Loss = 6.3966\n",
      "Epoch 978/1000: Train Loss = 6.2763, Test Loss = 6.2757\n",
      "Epoch 979/1000: Train Loss = 6.3685, Test Loss = 6.3946\n",
      "Epoch 980/1000: Train Loss = 6.2740, Test Loss = 6.2736\n",
      "Epoch 981/1000: Train Loss = 6.3663, Test Loss = 6.3925\n",
      "Epoch 982/1000: Train Loss = 6.2718, Test Loss = 6.2715\n",
      "Epoch 983/1000: Train Loss = 6.3641, Test Loss = 6.3905\n",
      "Epoch 984/1000: Train Loss = 6.2696, Test Loss = 6.2695\n",
      "Epoch 985/1000: Train Loss = 6.3620, Test Loss = 6.3885\n",
      "Epoch 986/1000: Train Loss = 6.2674, Test Loss = 6.2674\n",
      "Epoch 987/1000: Train Loss = 6.3598, Test Loss = 6.3864\n",
      "Epoch 988/1000: Train Loss = 6.2652, Test Loss = 6.2654\n",
      "Epoch 989/1000: Train Loss = 6.3576, Test Loss = 6.3844\n",
      "Epoch 990/1000: Train Loss = 6.2630, Test Loss = 6.2633\n",
      "Epoch 991/1000: Train Loss = 6.3554, Test Loss = 6.3824\n",
      "Epoch 992/1000: Train Loss = 6.2608, Test Loss = 6.2613\n",
      "Epoch 993/1000: Train Loss = 6.3533, Test Loss = 6.3803\n",
      "Epoch 994/1000: Train Loss = 6.2586, Test Loss = 6.2592\n",
      "Epoch 995/1000: Train Loss = 6.3511, Test Loss = 6.3783\n",
      "Epoch 996/1000: Train Loss = 6.2564, Test Loss = 6.2572\n",
      "Epoch 997/1000: Train Loss = 6.3489, Test Loss = 6.3763\n",
      "Epoch 998/1000: Train Loss = 6.2542, Test Loss = 6.2552\n",
      "Epoch 999/1000: Train Loss = 6.3468, Test Loss = 6.3743\n",
      "Epoch 1000/1000: Train Loss = 6.2520, Test Loss = 6.2532\n",
      "Total runtime: 350.68 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = logistic_regression(learning_rate=0.1, epochs=1000)\n",
    "\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the LR model\n",
    "\n",
    "lr.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# End timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total runtime: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[2601  548   69   55  418  429  217  163  216  284]\n",
      " [ 762 1835  116  104  352  511  347  143  212  618]\n",
      " [ 917  359  238  111 1221  934  772  221   96  131]\n",
      " [ 621  581  134  273  714 1380  789  182  101  225]\n",
      " [ 591  332  217   96 1538  880  834  321   50  141]\n",
      " [ 533  487  186  234  800 1630  655  246   77  152]\n",
      " [ 292  433  202  184  932  914 1584  224   79  156]\n",
      " [ 700  434  166  137 1012  741  609  773   96  332]\n",
      " [2025  985   72   63  261  427  164   66  556  381]\n",
      " [ 885 1236   88   86  347  400  329  195  216 1218]]\n",
      "Accuracy: 0.2449\n",
      "Precision: 0.2498\n",
      "Recall: 0.2449\n",
      "F1 Score: 0.2473\n",
      "\n",
      "Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[525 109  12  10  94  84  33  28  36  69]\n",
      " [166 380  23  24  59  96  80  26  40 106]\n",
      " [168  71  47  29 256 189 163  34  17  26]\n",
      " [117 128  25  56 149 272 132  42  27  52]\n",
      " [125  62  49  27 299 162 182  49  14  31]\n",
      " [109  80  28  42 178 343 127  47  25  21]\n",
      " [ 50  88  42  34 189 191 309  48  15  34]\n",
      " [164  94  29  24 188 145  93 182  11  70]\n",
      " [398 193  19  12  54  84  42  17 101  80]\n",
      " [167 254  13  28  67  75  72  37  42 245]]\n",
      "Accuracy: 0.2487\n",
      "Precision: 0.2537\n",
      "Recall: 0.2487\n",
      "F1 Score: 0.2512\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for train and test sets\n",
    "train_pred = lr.predict(X_train)\n",
    "\n",
    "# Calculate training metrics\n",
    "train_cm = confusion_matrix(y_train, train_pred)\n",
    "train_accuracy = accuracy(y_train, train_pred)\n",
    "train_precision = precision(train_cm)\n",
    "train_recall = recall(train_cm)\n",
    "train_f1 = f1_score(train_cm)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"\\nTraining Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm)\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1 Score: {train_f1:.4f}\")\n",
    "\n",
    "\n",
    "test_pred = lr.predict(X_test)\n",
    "\n",
    "test_cm = confusion_matrix(y_test, test_pred)\n",
    "test_accuracy = accuracy(y_test, test_pred)\n",
    "test_precision = precision(test_cm)\n",
    "test_recall = recall(test_cm)\n",
    "test_f1 = f1_score(test_cm)\n",
    "\n",
    "print(\"\\nValidation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm)\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch vs log-loss\n",
    "epochs = list(range(1, len(lr.train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, lr.train_losses, label='Training Loss')\n",
    "plt.plot(epochs, lr.test_losses, label='Validation Loss')\n",
    "\n",
    "plt.title('Epoch vs Mean Log-Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Log-Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.01, lambda_param=0.01, n_epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weights = None\n",
    "        self.accuracy_per_epoch = []\n",
    "\n",
    "    def fit(self, X, y, num_classes, x_test = None, y_test = None):\n",
    "        _, n_features = X.shape\n",
    "        self.weights = np.zeros((num_classes, n_features))\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for c in range(num_classes):\n",
    "                y_binary = np.where(y == c, 1, -1)\n",
    "                w = self.weights[c,:]\n",
    "                \n",
    "                for idx, x_i in enumerate(X):\n",
    "                    condition = y_binary[idx] * (np.dot(x_i, w)) # Check if current sample passes margin condition\n",
    "                    if condition >= 1:\n",
    "                        w -= self.learning_rate * (2 * self.lambda_param * w)\n",
    "                    else:\n",
    "                        w -= self.learning_rate * (2 * self.lambda_param * w - x_i * y_binary[idx])\n",
    "                            \n",
    "                self.weights[c, :] = w\n",
    "            \n",
    "            # Evaluate after each epoch\n",
    "            if x_test is not None and y_test is not None:\n",
    "                y_pred = self.predict(x_test)\n",
    "                accuracy = np.mean(y_pred == y_test)\n",
    "                self.accuracy_per_epoch.append(accuracy)\n",
    "                print(f\"Epoch {epoch + 1}/{self.n_epochs} - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    def predictProb(self, X):\n",
    "        linear_output = np.dot(X, self.weights.T)\n",
    "        return linear_output\n",
    "    \n",
    "    def predict(self, linear_output):\n",
    "        return np.argmax(self.predictProb(linear_output), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n",
      "Epoch 1/200 - Accuracy: 25.88%\n",
      "Epoch 2/200 - Accuracy: 29.39%\n",
      "Epoch 3/200 - Accuracy: 31.21%\n",
      "Epoch 4/200 - Accuracy: 32.05%\n",
      "Epoch 5/200 - Accuracy: 32.89%\n",
      "Epoch 6/200 - Accuracy: 33.39%\n",
      "Epoch 7/200 - Accuracy: 33.89%\n",
      "Epoch 8/200 - Accuracy: 34.29%\n",
      "Epoch 9/200 - Accuracy: 34.48%\n",
      "Epoch 10/200 - Accuracy: 34.83%\n",
      "Epoch 11/200 - Accuracy: 35.01%\n",
      "Epoch 12/200 - Accuracy: 35.09%\n",
      "Epoch 13/200 - Accuracy: 35.17%\n",
      "Epoch 14/200 - Accuracy: 35.45%\n",
      "Epoch 15/200 - Accuracy: 35.59%\n",
      "Epoch 16/200 - Accuracy: 35.72%\n",
      "Epoch 17/200 - Accuracy: 35.83%\n",
      "Epoch 18/200 - Accuracy: 35.99%\n",
      "Epoch 19/200 - Accuracy: 36.15%\n",
      "Epoch 20/200 - Accuracy: 36.40%\n",
      "Epoch 21/200 - Accuracy: 36.46%\n",
      "Epoch 22/200 - Accuracy: 36.45%\n",
      "Epoch 23/200 - Accuracy: 36.47%\n",
      "Epoch 24/200 - Accuracy: 36.52%\n",
      "Epoch 25/200 - Accuracy: 36.70%\n",
      "Epoch 26/200 - Accuracy: 36.71%\n",
      "Epoch 27/200 - Accuracy: 36.76%\n",
      "Epoch 28/200 - Accuracy: 36.83%\n",
      "Epoch 29/200 - Accuracy: 36.87%\n",
      "Epoch 30/200 - Accuracy: 37.01%\n",
      "Epoch 31/200 - Accuracy: 37.03%\n",
      "Epoch 32/200 - Accuracy: 37.07%\n",
      "Epoch 33/200 - Accuracy: 37.20%\n",
      "Epoch 34/200 - Accuracy: 37.20%\n",
      "Epoch 35/200 - Accuracy: 37.32%\n",
      "Epoch 36/200 - Accuracy: 37.35%\n",
      "Epoch 37/200 - Accuracy: 37.49%\n",
      "Epoch 38/200 - Accuracy: 37.54%\n",
      "Epoch 39/200 - Accuracy: 37.62%\n",
      "Epoch 40/200 - Accuracy: 37.61%\n",
      "Epoch 41/200 - Accuracy: 37.66%\n",
      "Epoch 42/200 - Accuracy: 37.74%\n",
      "Epoch 43/200 - Accuracy: 37.79%\n",
      "Epoch 44/200 - Accuracy: 37.85%\n",
      "Epoch 45/200 - Accuracy: 37.86%\n",
      "Epoch 46/200 - Accuracy: 37.92%\n",
      "Epoch 47/200 - Accuracy: 37.94%\n",
      "Epoch 48/200 - Accuracy: 37.99%\n",
      "Epoch 49/200 - Accuracy: 38.03%\n",
      "Epoch 50/200 - Accuracy: 37.95%\n",
      "Epoch 51/200 - Accuracy: 37.94%\n",
      "Epoch 52/200 - Accuracy: 38.00%\n",
      "Epoch 53/200 - Accuracy: 38.00%\n",
      "Epoch 54/200 - Accuracy: 38.01%\n",
      "Epoch 55/200 - Accuracy: 38.01%\n",
      "Epoch 56/200 - Accuracy: 37.99%\n",
      "Epoch 57/200 - Accuracy: 38.08%\n",
      "Epoch 58/200 - Accuracy: 38.08%\n",
      "Epoch 59/200 - Accuracy: 38.08%\n",
      "Epoch 60/200 - Accuracy: 38.13%\n",
      "Epoch 61/200 - Accuracy: 38.16%\n",
      "Epoch 62/200 - Accuracy: 38.22%\n",
      "Epoch 63/200 - Accuracy: 38.19%\n",
      "Epoch 64/200 - Accuracy: 38.26%\n",
      "Epoch 65/200 - Accuracy: 38.25%\n",
      "Epoch 66/200 - Accuracy: 38.29%\n",
      "Epoch 67/200 - Accuracy: 38.32%\n",
      "Epoch 68/200 - Accuracy: 38.30%\n",
      "Epoch 69/200 - Accuracy: 38.38%\n",
      "Epoch 70/200 - Accuracy: 38.36%\n",
      "Epoch 71/200 - Accuracy: 38.40%\n",
      "Epoch 72/200 - Accuracy: 38.38%\n",
      "Epoch 73/200 - Accuracy: 38.33%\n",
      "Epoch 74/200 - Accuracy: 38.36%\n",
      "Epoch 75/200 - Accuracy: 38.37%\n",
      "Epoch 76/200 - Accuracy: 38.35%\n",
      "Epoch 77/200 - Accuracy: 38.40%\n",
      "Epoch 78/200 - Accuracy: 38.40%\n",
      "Epoch 79/200 - Accuracy: 38.38%\n",
      "Epoch 80/200 - Accuracy: 38.42%\n",
      "Epoch 81/200 - Accuracy: 38.43%\n",
      "Epoch 82/200 - Accuracy: 38.43%\n",
      "Epoch 83/200 - Accuracy: 38.48%\n",
      "Epoch 84/200 - Accuracy: 38.49%\n",
      "Epoch 85/200 - Accuracy: 38.45%\n",
      "Epoch 86/200 - Accuracy: 38.49%\n",
      "Epoch 87/200 - Accuracy: 38.46%\n",
      "Epoch 88/200 - Accuracy: 38.47%\n",
      "Epoch 89/200 - Accuracy: 38.51%\n",
      "Epoch 90/200 - Accuracy: 38.54%\n",
      "Epoch 91/200 - Accuracy: 38.56%\n",
      "Epoch 92/200 - Accuracy: 38.56%\n",
      "Epoch 93/200 - Accuracy: 38.61%\n",
      "Epoch 94/200 - Accuracy: 38.61%\n",
      "Epoch 95/200 - Accuracy: 38.56%\n",
      "Epoch 96/200 - Accuracy: 38.57%\n",
      "Epoch 97/200 - Accuracy: 38.57%\n",
      "Epoch 98/200 - Accuracy: 38.56%\n",
      "Epoch 99/200 - Accuracy: 38.59%\n",
      "Epoch 100/200 - Accuracy: 38.56%\n",
      "Epoch 101/200 - Accuracy: 38.59%\n",
      "Epoch 102/200 - Accuracy: 38.57%\n",
      "Epoch 103/200 - Accuracy: 38.60%\n",
      "Epoch 104/200 - Accuracy: 38.69%\n",
      "Epoch 105/200 - Accuracy: 38.69%\n",
      "Epoch 106/200 - Accuracy: 38.64%\n",
      "Epoch 107/200 - Accuracy: 38.65%\n",
      "Epoch 108/200 - Accuracy: 38.68%\n",
      "Epoch 109/200 - Accuracy: 38.69%\n",
      "Epoch 110/200 - Accuracy: 38.68%\n",
      "Epoch 111/200 - Accuracy: 38.70%\n",
      "Epoch 112/200 - Accuracy: 38.73%\n",
      "Epoch 113/200 - Accuracy: 38.76%\n",
      "Epoch 114/200 - Accuracy: 38.73%\n",
      "Epoch 115/200 - Accuracy: 38.74%\n",
      "Epoch 116/200 - Accuracy: 38.75%\n",
      "Epoch 117/200 - Accuracy: 38.80%\n",
      "Epoch 118/200 - Accuracy: 38.79%\n",
      "Epoch 119/200 - Accuracy: 38.80%\n",
      "Epoch 120/200 - Accuracy: 38.80%\n",
      "Epoch 121/200 - Accuracy: 38.78%\n",
      "Epoch 122/200 - Accuracy: 38.81%\n",
      "Epoch 123/200 - Accuracy: 38.82%\n",
      "Epoch 124/200 - Accuracy: 38.81%\n",
      "Epoch 125/200 - Accuracy: 38.83%\n",
      "Epoch 126/200 - Accuracy: 38.86%\n",
      "Epoch 127/200 - Accuracy: 38.87%\n",
      "Epoch 128/200 - Accuracy: 38.83%\n",
      "Epoch 129/200 - Accuracy: 38.86%\n",
      "Epoch 130/200 - Accuracy: 38.81%\n",
      "Epoch 131/200 - Accuracy: 38.82%\n",
      "Epoch 132/200 - Accuracy: 38.80%\n",
      "Epoch 133/200 - Accuracy: 38.82%\n",
      "Epoch 134/200 - Accuracy: 38.86%\n",
      "Epoch 135/200 - Accuracy: 38.83%\n",
      "Epoch 136/200 - Accuracy: 38.78%\n",
      "Epoch 137/200 - Accuracy: 38.92%\n",
      "Epoch 138/200 - Accuracy: 38.89%\n",
      "Epoch 139/200 - Accuracy: 38.87%\n",
      "Epoch 140/200 - Accuracy: 38.91%\n",
      "Epoch 141/200 - Accuracy: 38.93%\n",
      "Epoch 142/200 - Accuracy: 38.92%\n",
      "Epoch 143/200 - Accuracy: 38.93%\n",
      "Epoch 144/200 - Accuracy: 38.85%\n",
      "Epoch 145/200 - Accuracy: 38.89%\n",
      "Epoch 146/200 - Accuracy: 38.94%\n",
      "Epoch 147/200 - Accuracy: 38.91%\n",
      "Epoch 148/200 - Accuracy: 38.91%\n",
      "Epoch 149/200 - Accuracy: 38.92%\n",
      "Epoch 150/200 - Accuracy: 38.91%\n",
      "Epoch 151/200 - Accuracy: 38.89%\n",
      "Epoch 152/200 - Accuracy: 38.89%\n",
      "Epoch 153/200 - Accuracy: 38.91%\n",
      "Epoch 154/200 - Accuracy: 38.90%\n",
      "Epoch 155/200 - Accuracy: 38.87%\n",
      "Epoch 156/200 - Accuracy: 38.87%\n",
      "Epoch 157/200 - Accuracy: 38.88%\n",
      "Epoch 158/200 - Accuracy: 38.90%\n",
      "Epoch 159/200 - Accuracy: 38.83%\n",
      "Epoch 160/200 - Accuracy: 38.86%\n",
      "Epoch 161/200 - Accuracy: 38.87%\n",
      "Epoch 162/200 - Accuracy: 38.84%\n",
      "Epoch 163/200 - Accuracy: 38.86%\n",
      "Epoch 164/200 - Accuracy: 38.88%\n",
      "Epoch 165/200 - Accuracy: 38.88%\n",
      "Epoch 166/200 - Accuracy: 38.89%\n",
      "Epoch 167/200 - Accuracy: 38.90%\n",
      "Epoch 168/200 - Accuracy: 38.90%\n",
      "Epoch 169/200 - Accuracy: 38.89%\n",
      "Epoch 170/200 - Accuracy: 38.88%\n",
      "Epoch 171/200 - Accuracy: 38.89%\n",
      "Epoch 172/200 - Accuracy: 38.91%\n",
      "Epoch 173/200 - Accuracy: 38.89%\n",
      "Epoch 174/200 - Accuracy: 38.93%\n",
      "Epoch 175/200 - Accuracy: 38.94%\n",
      "Epoch 176/200 - Accuracy: 38.90%\n",
      "Epoch 177/200 - Accuracy: 38.92%\n",
      "Epoch 178/200 - Accuracy: 38.90%\n",
      "Epoch 179/200 - Accuracy: 38.93%\n",
      "Epoch 180/200 - Accuracy: 38.94%\n",
      "Epoch 181/200 - Accuracy: 38.91%\n",
      "Epoch 182/200 - Accuracy: 38.98%\n",
      "Epoch 183/200 - Accuracy: 38.95%\n",
      "Epoch 184/200 - Accuracy: 38.95%\n",
      "Epoch 185/200 - Accuracy: 38.97%\n",
      "Epoch 186/200 - Accuracy: 39.00%\n",
      "Epoch 187/200 - Accuracy: 38.98%\n",
      "Epoch 188/200 - Accuracy: 38.96%\n",
      "Epoch 189/200 - Accuracy: 39.04%\n",
      "Epoch 190/200 - Accuracy: 39.00%\n",
      "Epoch 191/200 - Accuracy: 39.01%\n",
      "Epoch 192/200 - Accuracy: 39.01%\n",
      "Epoch 193/200 - Accuracy: 39.02%\n",
      "Epoch 194/200 - Accuracy: 39.01%\n",
      "Epoch 195/200 - Accuracy: 39.07%\n",
      "Epoch 196/200 - Accuracy: 38.98%\n",
      "Epoch 197/200 - Accuracy: 39.01%\n",
      "Epoch 198/200 - Accuracy: 39.00%\n",
      "Epoch 199/200 - Accuracy: 39.03%\n",
      "Epoch 200/200 - Accuracy: 39.08%\n",
      "Total runtime: 1229.79 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the SVM\n",
    "print(\"Training SVM...\")\n",
    "svm = SVM(learning_rate=1e-6, lambda_param=0.1, n_epochs=200)\n",
    "\n",
    "# Decode labels from one-hot encoding to integers\n",
    "y_train_decoded = np.argmax(y_train, axis=1)\n",
    "y_test_decoded = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the SVM model\n",
    "svm.fit(X_train, y_train_decoded, num_classes, x_test=X_test, y_test=y_test_decoded)\n",
    "\n",
    "# End timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total runtime: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVM...\n",
      "\n",
      "SVM Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[2410  320   46  192   52  176  130  253  971  450]\n",
      " [ 310 2494   23  134   51  199  215  180  495  899]\n",
      " [ 760  287  636  521  420  590  812  473  327  174]\n",
      " [ 342  386  135 1353  114 1108  595  273  333  361]\n",
      " [ 454  197  297  364 1191  535  936  589  222  215]\n",
      " [ 231  286  222  830  181 1956  493  333  279  189]\n",
      " [ 140  291  122  659  304  489 2419  202  162  212]\n",
      " [ 258  326  107  359  352  451  279 2152  235  481]\n",
      " [ 797  410   21   97   19  226   57   87 2749  537]\n",
      " [ 293  884   24  120   34  152  156  174  567 2596]]\n",
      "Accuracy: 0.3991\n",
      "Precision: 0.3990\n",
      "Recall: 0.3991\n",
      "F1 Score: 0.3991\n",
      "\n",
      "SVM Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[503  61  11  40  11  28  27  42 185  92]\n",
      " [ 63 486   5  30   7  44  42  35 118 170]\n",
      " [158  55 117 126  84 111 171  87  61  30]\n",
      " [ 65  76  38 270  26 229 104  53  57  82]\n",
      " [ 78  35  72  81 210 111 205 126  49  33]\n",
      " [ 49  50  50 166  38 376  87  83  71  30]\n",
      " [ 26  53  31 133  52  84 511  34  29  47]\n",
      " [ 56  64  29  64  62  84  52 421  57 111]\n",
      " [154  95   2  23   4  54  12  17 523 116]\n",
      " [ 69 168   4  34   9  27  38  37 123 491]]\n",
      "Accuracy: 0.3908\n",
      "Precision: 0.3866\n",
      "Recall: 0.3908\n",
      "F1 Score: 0.3887\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating SVM...\")\n",
    "y_pred_train = svm.predict(X_train)\n",
    "y_pred_test = svm.predict(X_test)\n",
    "\n",
    "# Training Metrics\n",
    "train_cm_svm = confusion_matrix(y_train, y_pred_train)\n",
    "train_accuracy_svm = accuracy(y_train, y_pred_train)\n",
    "train_precision_svm = precision(train_cm_svm)\n",
    "train_recall_svm = recall(train_cm_svm)\n",
    "train_f1_svm = f1_score(train_cm_svm)\n",
    "\n",
    "print(\"\\nSVM Training Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm_svm)\n",
    "print(f\"Accuracy: {train_accuracy_svm:.4f}\")\n",
    "print(f\"Precision: {train_precision_svm:.4f}\")\n",
    "print(f\"Recall: {train_recall_svm:.4f}\")\n",
    "print(f\"F1 Score: {train_f1_svm:.4f}\")\n",
    "\n",
    "# Test Metrics\n",
    "test_cm_svm = confusion_matrix(y_test, y_pred_test)\n",
    "test_accuracy_svm = accuracy(y_test, y_pred_test)\n",
    "test_precision_svm = precision(test_cm_svm)\n",
    "test_recall_svm = recall(test_cm_svm)\n",
    "test_f1_svm = f1_score(test_cm_svm)\n",
    "\n",
    "print(\"\\nSVM Validation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm_svm)\n",
    "print(f\"Accuracy: {test_accuracy_svm:.4f}\")\n",
    "print(f\"Precision: {test_precision_svm:.4f}\")\n",
    "print(f\"Recall: {test_recall_svm:.4f}\")\n",
    "print(f\"F1 Score: {test_f1_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def predictProb(self, X):\n",
    "        # Average probabilities from each model\n",
    "        probs = [model.predictProb(X) for model in self.models]\n",
    "        avg_probs = np.mean(probs, axis=0)\n",
    "        return avg_probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        avg_probs = self.predictProb(X)\n",
    "        return np.argmax(avg_probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleModel(models=[lr, svm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Training Evaluation:\n",
      "Confusion Matrix:\n",
      " [[2643  511   67   59  382  419  208  174  240  297]\n",
      " [ 737 1919  106  101  322  496  328  138  221  632]\n",
      " [ 939  344  244  113 1211  927  772  223   95  132]\n",
      " [ 623  571  125  286  681 1417  786  177  112  222]\n",
      " [ 583  318  215   98 1539  887  828  333   53  146]\n",
      " [ 519  465  182  241  762 1710  650  247   80  144]\n",
      " [ 282  414  197  184  904  915 1651  215   79  159]\n",
      " [ 680  408  160  140  997  740  576  865   95  339]\n",
      " [2019  970   60   63  228  414  144   65  635  402]\n",
      " [ 852 1233   77   81  308  383  310  180  222 1354]]\n",
      "Accuracy: 0.2569\n",
      "Precision: 0.2626\n",
      "Recall: 0.2569\n",
      "F1 Score: 0.2597\n",
      "\n",
      "Ensemble Validation Evaluation:\n",
      "Confusion Matrix:\n",
      " [[541 102  13   8  81  81  31  30  45  68]\n",
      " [161 387  21  23  56  96  76  25  43 112]\n",
      " [174  71  47  29 254 188 162  36  16  23]\n",
      " [116 127  25  60 143 281 130  42  27  49]\n",
      " [127  60  48  28 299 162 181  53  13  29]\n",
      " [107  78  25  44 172 353 126  48  26  21]\n",
      " [ 47  83  44  38 187 191 319  43  13  35]\n",
      " [158  92  30  24 182 145  89 201  11  68]\n",
      " [391 178  19  12  48  86  36  18 122  90]\n",
      " [167 257  12  26  57  71  65  36  43 266]]\n",
      "Accuracy: 0.2595\n",
      "Precision: 0.2658\n",
      "Recall: 0.2595\n",
      "F1 Score: 0.2626\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Ensemble Model\n",
    "y_pred_train = ensemble.predict(X_train)\n",
    "y_pred_test = ensemble.predict(X_test)\n",
    "\n",
    "# Training Metrics\n",
    "train_cm_ensemble = confusion_matrix(y_train, y_pred_train)\n",
    "train_accuracy_ensemble = accuracy(y_train, y_pred_train)\n",
    "train_precision_ensemble = precision(train_cm_ensemble)\n",
    "train_recall_ensemble = recall(train_cm_ensemble)\n",
    "train_f1_ensemble = f1_score(train_cm_ensemble)\n",
    "\n",
    "print(\"\\nEnsemble Training Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", train_cm_ensemble)\n",
    "print(f\"Accuracy: {train_accuracy_ensemble:.4f}\")\n",
    "print(f\"Precision: {train_precision_ensemble:.4f}\")\n",
    "print(f\"Recall: {train_recall_ensemble:.4f}\")\n",
    "print(f\"F1 Score: {train_f1_ensemble:.4f}\")\n",
    "\n",
    "# Test Metrics\n",
    "test_cm_ensemble = confusion_matrix(y_test, y_pred_test)\n",
    "test_accuracy_ensemble = accuracy(y_test, y_pred_test)\n",
    "test_precision_ensemble = precision(test_cm_ensemble)\n",
    "test_recall_ensemble = recall(test_cm_ensemble)\n",
    "test_f1_ensemble = f1_score(test_cm_ensemble)\n",
    "\n",
    "print(\"\\nEnsemble Validation Evaluation:\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm_ensemble)\n",
    "print(f\"Accuracy: {test_accuracy_ensemble:.4f}\")\n",
    "print(f\"Precision: {test_precision_ensemble:.4f}\")\n",
    "print(f\"Recall: {test_recall_ensemble:.4f}\")\n",
    "print(f\"F1 Score: {test_f1_ensemble:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
